{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 25 August - DDP"
      ],
      "metadata": {
        "id": "7usHWjFmZzbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of Batch Normalization Methods\n",
        "\n",
        "1. Function Breakdown:\n",
        "\n",
        "```python\n",
        "def forward(self, input, pad_mask=None):\n",
        "    # ... (existing preprocessing) ...\n",
        "    \n",
        "    need_sync = self.training and torch.distributed.is_initialized()\n",
        "    if need_sync:\n",
        "        process_group = torch.distributed.group.WORLD\n",
        "        if self.process_group:\n",
        "            process_group = self.process_group\n",
        "        world_size = torch.distributed.get_world_size(process_group)\n",
        "        need_sync = world_size > 1\n",
        "\n",
        "    if need_sync:\n",
        "        # Implement synchronization logic here, similar to SyncBatchNorm\n",
        "        # This would involve gathering statistics from all GPUs and computing global statistics\n",
        "        pass\n",
        "    else:\n",
        "        # Existing PowerNorm logic\n",
        "        pass\n",
        "```\n",
        "\n",
        "Purpose:\n",
        "This function determines whether synchronization across GPUs is necessary and prepares for it if needed.\n",
        "\n",
        "Key steps:\n",
        "a. Check if synchronization is needed:\n",
        "   - The model is in training mode (`self.training`)\n",
        "   - Distributed training is initialized (`torch.distributed.is_initialized()`)\n",
        "\n",
        "b. Set up the process group:\n",
        "   - Use the default world group or a custom group if specified\n",
        "\n",
        "c. Get the world size (number of processes/GPUs)\n",
        "\n",
        "d. Determine if synchronization is actually needed (more than one GPU)\n",
        "\n",
        "e. If synchronization is needed:\n",
        "   - Implement logic to gather statistics from all GPUs\n",
        "   - Compute global statistics\n",
        "   - Apply these global statistics in the normalization process\n",
        "\n",
        "f. If synchronization is not needed:\n",
        "   - Proceed with the standard PowerNorm logic\n",
        "\n",
        "2. Comparison: F.batch_norm vs sync_batch_norm.apply()\n",
        "\n",
        "a. F.batch_norm:\n",
        "   - Standard batch normalization function\n",
        "   - Operates independently on each GPU in a multi-GPU setup\n",
        "   - Computes mean and variance using only the local batch on each GPU\n",
        "   - Faster for single-GPU or small-scale multi-GPU setups\n",
        "   - May lead to inconsistent statistics across GPUs in large-scale distributed training\n",
        "\n",
        "Example:\n",
        "```python\n",
        "output = F.batch_norm(input, running_mean, running_var, weight, bias,\n",
        "                      training, momentum, eps)\n",
        "```\n",
        "\n",
        "b. sync_batch_norm.apply():\n",
        "   - Synchronized version of batch normalization\n",
        "   - Coordinates computation across all GPUs in a distributed setup\n",
        "   - Computes global mean and variance by aggregating statistics from all GPUs\n",
        "   - Ensures consistent normalization across the entire model, regardless of data distribution across GPUs\n",
        "   - More computationally expensive due to inter-GPU communication\n",
        "   - Crucial for maintaining model consistency in large-scale distributed training\n",
        "\n",
        "Example:\n",
        "```python\n",
        "output = sync_batch_norm.apply(input, weight, bias, running_mean, running_var,\n",
        "                               eps, momentum, process_group, world_size)\n",
        "```\n",
        "\n",
        "Key Differences:\n",
        "1. Consistency: sync_batch_norm ensures consistent statistics across all GPUs, while F.batch_norm does not.\n",
        "2. Communication: sync_batch_norm involves inter-GPU communication, F.batch_norm does not.\n",
        "3. Computational cost: sync_batch_norm is more expensive due to synchronization overhead.\n",
        "4. Scale of distribution: sync_batch_norm is more suitable for large-scale distributed training.\n",
        "\n",
        "When to use which:\n",
        "- Use F.batch_norm for single-GPU training or when batch statistics on each GPU are representative of the whole dataset.\n",
        "- Use sync_batch_norm.apply() for large-scale distributed training where maintaining consistent statistics across GPUs is crucial for model stability and performance."
      ],
      "metadata": {
        "id": "qWiJ54I2ZzZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules._functions import SyncBatchNorm as sync_batch_norm\n",
        "\n",
        "class GroupScaling1D(nn.Module):\n",
        "    def __init__(self, eps=1e-5, group_num=4):\n",
        "        super(GroupScaling1D, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.group_num = group_num\n",
        "\n",
        "    def forward(self, input):\n",
        "        T, B, C = input.shape\n",
        "        Cg = C // self.group_num\n",
        "        gn_input = input.contiguous().reshape(T, B, self.group_num, Cg)\n",
        "        moment2 = torch.repeat_interleave(torch.mean(gn_input * gn_input, dim=3, keepdim=True),\n",
        "            repeats=Cg, dim=-1).contiguous().reshape(T, B, C)\n",
        "        return input / torch.sqrt(moment2 + self.eps)\n",
        "\n",
        "class SyncPowerNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True, process_group=None,\n",
        "                 alpha_fwd=0.9, alpha_bkw=0.9, warmup_iters=10000, group_num=1):\n",
        "        super(SyncPowerNorm, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "        self.process_group = process_group\n",
        "\n",
        "        self.alpha_fwd = alpha_fwd\n",
        "        self.alpha_bkw = alpha_bkw\n",
        "        self.warmup_iters = warmup_iters\n",
        "\n",
        "        if self.affine:\n",
        "            self.weight = nn.Parameter(torch.ones(num_features))\n",
        "            self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_phi', torch.ones(1, num_features, 1, 1))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "        else:\n",
        "            self.register_parameter('running_phi', None)\n",
        "            self.register_parameter('num_batches_tracked', None)\n",
        "\n",
        "        self.register_buffer('ema_gz', torch.zeros(1, num_features, 1, 1))\n",
        "        self.gp = GroupScaling1D(group_num=group_num)\n",
        "\n",
        "    def forward(self, input, pad_mask=None):\n",
        "        if input.dim() < 2:\n",
        "            raise ValueError(f\"expected at least 2D input (got {input.dim()}D input)\")\n",
        "\n",
        "        # Handle the case where input is (N, C) instead of (N, C, *)\n",
        "        shaped_input = (len(input.shape) == 2)\n",
        "        if shaped_input:\n",
        "            input = input.unsqueeze(0)\n",
        "\n",
        "        T, B, C = input.shape\n",
        "        input = self.gp(input)\n",
        "        input = input.permute(1, 2, 0).contiguous()  # B x C x T\n",
        "        input_shape = input.size()\n",
        "        input = input.reshape(input.size(0), self.num_features, -1)\n",
        "        input = input.unsqueeze(-1)  # B x C x T x 1\n",
        "\n",
        "        if self.momentum is None:\n",
        "            exponential_average_factor = 0.0\n",
        "        else:\n",
        "            exponential_average_factor = self.momentum\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked += 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum\n",
        "\n",
        "        need_sync = self.training and torch.distributed.is_available() and torch.distributed.is_initialized()\n",
        "        if need_sync:\n",
        "            process_group = torch.distributed.group.WORLD\n",
        "            if self.process_group:\n",
        "                process_group = self.process_group\n",
        "            world_size = torch.distributed.get_world_size(process_group)\n",
        "            need_sync = world_size > 1\n",
        "\n",
        "        if not need_sync:\n",
        "            x2 = (input * input).mean(dim=(0, 2, 3), keepdim=True)\n",
        "            if self.training:\n",
        "                if self.num_batches_tracked <= self.warmup_iters:\n",
        "                    z = input / (x2 + self.eps).sqrt()\n",
        "                else:\n",
        "                    z = input / (self.running_phi + self.eps).sqrt()\n",
        "                self.running_phi = self.alpha_fwd * self.running_phi + (1 - self.alpha_fwd) * x2\n",
        "            else:\n",
        "                z = input / (self.running_phi + self.eps).sqrt()\n",
        "        else:\n",
        "            x2 = (input * input).mean(dim=(2, 3), keepdim=True)  # B x C x 1 x 1\n",
        "            x2_all = torch.empty(world_size, B, C, 1, 1, dtype=x2.dtype, device=x2.device)\n",
        "            torch.distributed.all_gather_into_tensor(x2_all, x2, group=process_group)\n",
        "            x2 = x2_all.mean(dim=0)  # Average across all processes\n",
        "\n",
        "            if self.training:\n",
        "                if self.num_batches_tracked <= self.warmup_iters:\n",
        "                    z = input / (x2 + self.eps).sqrt()\n",
        "                else:\n",
        "                    z = input / (self.running_phi + self.eps).sqrt()\n",
        "                self.running_phi = self.alpha_fwd * self.running_phi + (1 - self.alpha_fwd) * x2\n",
        "            else:\n",
        "                z = input / (self.running_phi + self.eps).sqrt()\n",
        "\n",
        "        if self.affine:\n",
        "            z = self.weight.view(1, C, 1, 1) * z + self.bias.view(1, C, 1, 1)\n",
        "\n",
        "        output = z.reshape(input_shape)\n",
        "        output = output.permute(2, 0, 1).contiguous()  # T x B x C\n",
        "\n",
        "        if shaped_input:\n",
        "            output = output.squeeze(0)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_sync_powernorm(module, process_group=None):\n",
        "        module_output = module\n",
        "        if isinstance(module, MaskPowerNorm):\n",
        "            module_output = SyncPowerNorm(module.num_features, module.eps, module.afwd,\n",
        "                                          module.affine, module.track_running_stats,\n",
        "                                          process_group, module.alpha_fwd, module.alpha_bkw,\n",
        "                                          module.warmup_iters, module.group_num)\n",
        "            if module.affine:\n",
        "                module_output.weight = module.weight\n",
        "                module_output.bias = module.bias\n",
        "            module_output.running_phi = module.running_phi\n",
        "            module_output.num_batches_tracked = module.num_batches_tracked\n",
        "        for name, child in module.named_children():\n",
        "            module_output.add_module(name, SyncPowerNorm.convert_sync_powernorm(child, process_group))\n",
        "        del module\n",
        "        return module_output"
      ],
      "metadata": {
        "id": "9QGBervhjOki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sync batch norm implementation"
      ],
      "metadata": {
        "id": "o2t4XMFpZzW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.autograd.function import Function\n",
        "\n",
        "\n",
        "class SyncBatchNorm(Function):\n",
        "    @staticmethod\n",
        "    def forward(\n",
        "        self,\n",
        "        input,\n",
        "        weight,\n",
        "        bias,\n",
        "        running_mean,\n",
        "        running_var,\n",
        "        eps,\n",
        "        momentum,\n",
        "        process_group,\n",
        "        world_size,\n",
        "    ):\n",
        "        # contiguity check <-- ensures input and weight tensors are in a contiguous format\n",
        "        if not (\n",
        "            input.is_contiguous(memory_format=torch.channels_last)\n",
        "            or input.is_contiguous(memory_format=torch.channels_last_3d)\n",
        "        ):\n",
        "            input = input.contiguous()\n",
        "        if weight is not None:\n",
        "            weight = weight.contiguous()\n",
        "\n",
        "        # ensures sufficient data points per channel for meaningful normalization. Prevent statistical anomalies due to insufficient data.\n",
        "        size = int(input.numel() // input.size(1))\n",
        "        if size == 1 and world_size < 2:\n",
        "            raise ValueError(\n",
        "                f\"Expected more than 1 value per channel when training, got input size {size}\"\n",
        "            )\n",
        "\n",
        "        num_channels = input.shape[1]\n",
        "        if input.numel() > 0:\n",
        "            # calculate mean/invstd for input. <-- COMPUTES THE MEAN AND INVERSE STD FOR INPUT TENSOR, these stats are concatenated along with count of elements per channel, forming a combined tensor which is synchronised across all processesÃŸ\n",
        "            mean, invstd = torch.batch_norm_stats(input, eps) # <-- BATCH_NORM_STATS, reverse engineering required\n",
        "\n",
        "            count = torch.full(\n",
        "                (1,),\n",
        "                input.numel() // input.size(1),\n",
        "                dtype=mean.dtype,\n",
        "                device=mean.device,\n",
        "            )\n",
        "\n",
        "            # C, C, 1 -> (2C + 1)\n",
        "            combined = torch.cat([mean, invstd, count], dim=0)\n",
        "        else:\n",
        "            # for empty input, set stats and the count to zero. The stats with\n",
        "            # zero count will be filtered out later when computing global mean\n",
        "            # & invstd, but they still needs to participate the all_gather\n",
        "            # collective communication to unblock other peer processes.\n",
        "            combined = torch.zeros(\n",
        "                2 * num_channels + 1, dtype=input.dtype, device=input.device\n",
        "            )\n",
        "\n",
        "        # Use allgather instead of allreduce because count could be different across\n",
        "        # ranks, simple all reduce op can not give correct results.\n",
        "        # batch_norm_gather_stats_with_counts calculates global mean & invstd based on\n",
        "        # all gathered mean, invstd and count.\n",
        "        # for nccl backend, use the optimized version of all gather.\n",
        "        # The Gloo backend does not support `all_gather_into_tensor`.\n",
        "        if process_group._get_backend_name() != \"gloo\":\n",
        "            # world_size * (2C + 1)\n",
        "            combined_size = combined.numel()\n",
        "            combined_flat = torch.empty(\n",
        "                1,\n",
        "                combined_size * world_size,\n",
        "                dtype=combined.dtype,\n",
        "                device=combined.device,\n",
        "            )\n",
        "            dist.all_gather_into_tensor(\n",
        "                combined_flat, combined, process_group, async_op=False\n",
        "            )\n",
        "            combined = torch.reshape(combined_flat, (world_size, combined_size))\n",
        "            # world_size * (2C + 1) -> world_size * C, world_size * C, world_size * 1\n",
        "            mean_all, invstd_all, count_all = torch.split(combined, num_channels, dim=1)\n",
        "        else:\n",
        "            # world_size * (2C + 1)\n",
        "            combined_list = [torch.empty_like(combined) for _ in range(world_size)]\n",
        "            dist.all_gather(combined_list, combined, process_group, async_op=False)\n",
        "            combined = torch.stack(combined_list, dim=0)\n",
        "            # world_size * (2C + 1) -> world_size * C, world_size * C, world_size * 1\n",
        "            mean_all, invstd_all, count_all = torch.split(combined, num_channels, dim=1)\n",
        "\n",
        "        if not (torch.cuda.is_available() and torch.cuda.is_current_stream_capturing()):\n",
        "            # The lines below force a synchronization between CUDA and CPU, because\n",
        "            # the shape of the result count_all depends on the values in mask tensor.\n",
        "            # Such synchronizations break CUDA Graph capturing.\n",
        "            # See https://github.com/pytorch/pytorch/issues/78549\n",
        "            # FIXME: https://github.com/pytorch/pytorch/issues/78656 describes\n",
        "            # a better longer-term solution.\n",
        "\n",
        "            # remove stats from empty inputs\n",
        "            mask = count_all.squeeze(-1) >= 1\n",
        "            count_all = count_all[mask]\n",
        "            mean_all = mean_all[mask]\n",
        "            invstd_all = invstd_all[mask]\n",
        "\n",
        "        # calculate global mean & invstd\n",
        "        counts = count_all.view(-1)\n",
        "        if running_mean is not None and counts.dtype != running_mean.dtype:\n",
        "            counts = counts.to(running_mean.dtype)\n",
        "        mean, invstd = torch.batch_norm_gather_stats_with_counts(\n",
        "            input,\n",
        "            mean_all,\n",
        "            invstd_all,\n",
        "            running_mean,\n",
        "            running_var,\n",
        "            momentum,\n",
        "            eps,\n",
        "            counts,\n",
        "        )\n",
        "\n",
        "        self.save_for_backward(input, weight, mean, invstd, count_all.to(torch.int32))\n",
        "        self.process_group = process_group\n",
        "\n",
        "        # apply element-wise normalization\n",
        "        if input.numel() > 0:\n",
        "            return torch.batch_norm_elemt(input, weight, bias, mean, invstd, eps)\n",
        "        else:\n",
        "            return torch.empty_like(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        if not (\n",
        "            grad_output.is_contiguous(memory_format=torch.channels_last)\n",
        "            or grad_output.is_contiguous(memory_format=torch.channels_last_3d)\n",
        "        ):\n",
        "            grad_output = grad_output.contiguous()\n",
        "        saved_input, weight, mean, invstd, count_tensor = self.saved_tensors\n",
        "        grad_input = grad_weight = grad_bias = None\n",
        "        process_group = self.process_group\n",
        "\n",
        "        if saved_input.numel() > 0:\n",
        "            # calculate local stats as well as grad_weight / grad_bias\n",
        "            (\n",
        "                sum_dy,\n",
        "                sum_dy_xmu,\n",
        "                grad_weight,\n",
        "                grad_bias,\n",
        "            ) = torch.batch_norm_backward_reduce(\n",
        "                grad_output,\n",
        "                saved_input,\n",
        "                mean,\n",
        "                invstd,\n",
        "                weight,\n",
        "                self.needs_input_grad[0],\n",
        "                self.needs_input_grad[1],\n",
        "                self.needs_input_grad[2],\n",
        "            )\n",
        "\n",
        "            if self.needs_input_grad[0]:\n",
        "                # synchronizing stats used to calculate input gradient.\n",
        "                num_channels = sum_dy.shape[0]\n",
        "                combined = torch.cat([sum_dy, sum_dy_xmu], dim=0)\n",
        "                torch.distributed.all_reduce(\n",
        "                    combined,\n",
        "                    torch.distributed.ReduceOp.SUM,\n",
        "                    process_group,\n",
        "                    async_op=False,\n",
        "                )\n",
        "                sum_dy, sum_dy_xmu = torch.split(combined, num_channels)\n",
        "\n",
        "                # backward pass for gradient calculation\n",
        "                if weight is not None and weight.dtype != mean.dtype:\n",
        "                    weight = weight.to(mean.dtype)\n",
        "                grad_input = torch.batch_norm_backward_elemt(\n",
        "                    grad_output,\n",
        "                    saved_input,\n",
        "                    mean,\n",
        "                    invstd,\n",
        "                    weight,\n",
        "                    sum_dy,\n",
        "                    sum_dy_xmu,\n",
        "                    count_tensor,\n",
        "                )\n",
        "            # synchronizing of grad_weight / grad_bias is not needed as distributed\n",
        "            # training would handle all reduce.\n",
        "            if weight is None or not self.needs_input_grad[1]:\n",
        "                grad_weight = None\n",
        "\n",
        "            if weight is None or not self.needs_input_grad[2]:\n",
        "                grad_bias = None\n",
        "        else:\n",
        "            # This process got an empty input tensor in the forward pass.\n",
        "            # Although this process can directly set grad_input as an empty\n",
        "            # tensor of zeros, it still needs to participate in the collective\n",
        "            # communication to unblock its peers, as other peer processes might\n",
        "            # have received non-empty inputs.\n",
        "            num_channels = saved_input.shape[1]\n",
        "            if self.needs_input_grad[0]:\n",
        "                # launch all_reduce to unblock other peer processes\n",
        "                combined = torch.zeros(\n",
        "                    2 * num_channels, dtype=saved_input.dtype, device=saved_input.device\n",
        "                )\n",
        "                torch.distributed.all_reduce(\n",
        "                    combined,\n",
        "                    torch.distributed.ReduceOp.SUM,\n",
        "                    process_group,\n",
        "                    async_op=False,\n",
        "                )\n",
        "\n",
        "            # Leave grad_input, grad_weight and grad_bias as None, which will be\n",
        "            # interpreted by the autograd engine as Tensors full of zeros.\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None\n"
      ],
      "metadata": {
        "id": "5B1F8JUwmd9_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SguG7ovmkD1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}