{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To install git-lfs if you don't have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary prompt \n",
    "\n",
    "---\n",
    "\n",
    "i want to ensure that, my code train_gpt2.py below is able to train from checkpoint entirely. how do i test this? cuz i wanna be able to yk, maybe insert somewhere of what the branch_name in HF repo that i want to be traiing from. can you maybe write out the full code, and i wanna just put it inside config, a resume_from_checkpoint=True and branch_name=XXX which I will input manually. also, do it so that, all of the codes are centralised. as in uhh, i want to have it so that, all the numbers like 80 for save_checkpoint is all moved, and centralised inside config = {}, and that i am basically only just calling from config = {} only instead.\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from hellaswag import render_example, iterate_examples\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import wandb\n",
    "\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import Tensor, Size\n",
    "from typing import Union, List, Tuple, Optional\n",
    "from torch.nn import init\n",
    "import numbers\n",
    "\n",
    "\n",
    "_shape_t = Union[int, List[int], Size]\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']\n",
    "    normalized_shape: Tuple[int, ...]\n",
    "    eps: float\n",
    "    elementwise_affine: bool\n",
    "\n",
    "    def __init__(self, normalized_shape: _shape_t, eps: float = 1e-5, elementwise_affine: bool = True, bias: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            # mypy error: incompatible types in assignment\n",
    "            normalized_shape = (normalized_shape,)  # type: ignore[assignment]\n",
    "        self.normalized_shape = tuple(normalized_shape)  # type: ignore[arg-type]\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.gain = Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n",
    "            if bias:\n",
    "                self.bias = Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n",
    "            else:\n",
    "                self.register_parameter('bias', None)\n",
    "        else:\n",
    "            self.register_parameter('gain', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        if self.elementwise_affine:\n",
    "            init.ones_(self.gain)\n",
    "            if self.bias is not None:\n",
    "                init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass of LayerNorm.\"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False) # mean_x2 = torch.square(x).mean(dim=-1, keepdim=True) // alternate code for varience calculation, faster torch.compile\n",
    "                                                                 # var = mean_x2 - torch.square(mean)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            x_norm = self.gain * x_norm + self.bias # changed name of weight parameter, to gain parameter to concur with naming convention\n",
    "\n",
    "        return x_norm\n",
    "        \n",
    "    def extra_repr(self) -> str:\n",
    "        return '{normalized_shape}, eps={eps}, elementwise_affine={elementwise_affine}'.format(**self.__dict__)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filenames\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s] # listing out shards file in the data_root dir\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        if master_process:\n",
    "            print(f\"found {len(shards)} shards for split {split}\")\n",
    "\n",
    "        # NEW IMPL\n",
    "        self.current_shard = 0\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.current_shard = state['current_shard']\n",
    "        self.current_position = state['current_position']\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'current_shard': self.current_shard,\n",
    "            'current_position': self.current_position\n",
    "        }\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def setup_logging(project_name):\n",
    "    if master_process:\n",
    "        wandb.init(project=project_name, config=args, dir=\"./../\")\n",
    "        run_name = wandb.run.name\n",
    "        wandb_id = wandb.run.id\n",
    "    else:\n",
    "        run_name = \"\"\n",
    "        wandb_id = 0 \n",
    "\n",
    "    return run_name, wandb_id\n",
    "\n",
    "def log_metrics(metrics):\n",
    "    if master_process:\n",
    "        wandb.log(metrics)\n",
    "\n",
    "def save_checkpoint(model, optimizer, lr_scheduler, step, val_loss, run_name, train_loader_state, val_loader_state):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'step': step,\n",
    "        'val_loss': val_loss,\n",
    "        'run_name': run_name,\n",
    "        'train_loader_state': train_loader_state,\n",
    "        'val_loader_state': val_loader_state\n",
    "    }\n",
    "    checkpoint_path = os.path.join(log_dir, f\"checkpoint_{step}.pt\")\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, lr_scheduler, train_loader, val_loader):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    train_loader.set_state(checkpoint['train_loader_state'])\n",
    "    val_loader.set_state(checkpoint['val_loader_state'])\n",
    "    return checkpoint['step'], checkpoint['val_loss'], checkpoint['run_name']\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# helper function for HellaSwag eval\n",
    "# takes tokens, mask, and logits, returns the index of the completion with the lowest loss\n",
    "\n",
    "def get_most_likely_row(tokens, mask, logits):\n",
    "    # evaluate the autoregressive loss at all positions\n",
    "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    flat_shift_tokens = shift_tokens.view(-1)\n",
    "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "    # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
    "    masked_shift_losses = shift_losses * shift_mask\n",
    "    # sum and divide by the number of 1s in the mask\n",
    "    sum_loss = masked_shift_losses.sum(dim=1)\n",
    "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "    # now we have a loss for each of the 4 completions\n",
    "    # the one with the lowest loss should be the most likely\n",
    "    pred_norm = avg_loss.argmin().item()\n",
    "    return pred_norm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# simple launch:\n",
    "# python train_gpt2.py\n",
    "# DDP launch for e.g. 8 GPUs:\n",
    "# torchrun --standalone --nproc_per_node=8 train_gpt2.py\n",
    "\n",
    "# run the training loop\n",
    "\n",
    "# set up DDP (distributed data parallel).\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
    "    assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    # attempt to autodetect device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "project_name = \"shng2025/GPT-Valkyrie_LN-124m\"\n",
    "\n",
    "# GPTesla - 111M param setup in comment. Modification to make lighter training requirement needed\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"weight_decay\": 0.1,\n",
    "    # \"shuffle_buffer\": 1000,\n",
    "    \"learning_rate\": 6e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_warmup_steps\": 715,  # 2000\n",
    "    \"gradient_accumulation_steps\": 2**19 // (64 * 1024 * ddp_world_size),  # total_batch_size // (B * T * ddp_world_size\n",
    "    \"max_train_steps\": 19073,  # 150000\n",
    "    \"max_eval_steps\": 20,\n",
    "    \"seq_length\": 1024,\n",
    "    \"seed\": 1,\n",
    "    # \"save_checkpoint_steps\": 10000,\n",
    "}\n",
    "\n",
    "args = Namespace(**config)\n",
    "samples_per_step = torch.cuda.device_count() * args.batch_size\n",
    "\n",
    "# Logging\n",
    "if master_process:\n",
    "    run_name, wandb_id = setup_logging(project_name.split(\"/\")[1])\n",
    "    print(f\"Weights and Biases run name: {run_name}\")\n",
    "    print(f\"Weights and Biases run id  : {wandb_id}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
    "B = 64 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "if master_process:\n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
    "val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# create model\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
    "model.to(device)\n",
    "use_compile = True # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
    "if use_compile:\n",
    "    model = torch.compile(model)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
    "\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "# optimize!\n",
    "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
    "\n",
    "# create the log directory we will write checkpoints to and log to\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "\n",
    "# Initialize HuggingFace repository\n",
    "repo_name = \"shng2025/GPT-Valkyrie_LN-124m\"\n",
    "api = HfApi()\n",
    "create_repo(repo_name, exist_ok=True)\n",
    "repo = Repository(\"./\", clone_from=repo_name, use_auth_token=True)\n",
    "\n",
    "# Training loop\n",
    "starting_step = 0\n",
    "if args.resume_from_checkpoint:\n",
    "    latest_checkpoint = max([f for f in os.listdir(log_dir) if f.startswith(\"checkpoint\")], key=os.path.getctime)\n",
    "    checkpoint_path = os.path.join(log_dir, latest_checkpoint)\n",
    "    starting_step, val_loss, run_name = load_checkpoint(checkpoint_path, raw_model, optimizer, lr_scheduler, train_loader, val_loader)\n",
    "    print(f\"Resuming from checkpoint at step {starting_step}\")\n",
    "else:\n",
    "    run_name, wandb_id = setup_logging(project_name.split(\"/\")[1])\n",
    "    print(f\"Starting new run: {run_name}\")\n",
    "    repo.git_checkout(run_name, create_branch_ok=True)\n",
    "\n",
    "# Training Loop\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation loss\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            log_metrics({\"loss/validation\": val_loss_accum})\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "            if step > 0 and (step % 5000 == 0 or last_step):\n",
    "                # optionally write model checkpoints\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'config': raw_model.config,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "                # you might also want to add optimizer.state_dict() and\n",
    "                # rng seeds etc., if you wanted to more exactly resume training\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # once in a while evaluate hellaswag\n",
    "    if (step % 250 == 0 or last_step) and (not use_compile):\n",
    "        num_correct_norm = 0\n",
    "        num_total = 0\n",
    "        for i, example in enumerate(iterate_examples(\"val\")):\n",
    "            # only process examples where i % ddp_world_size == ddp_rank\n",
    "            if i % ddp_world_size != ddp_rank:\n",
    "                continue\n",
    "            # render the example into tokens and labels\n",
    "            _, tokens, mask, label = render_example(example)\n",
    "            tokens = tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "            # get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(tokens)\n",
    "                pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "            num_total += 1\n",
    "            num_correct_norm += int(pred_norm == label)\n",
    "        # reduce the stats across all processes\n",
    "        if ddp:\n",
    "            num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
    "            num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n",
    "            dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
    "            dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n",
    "            num_total = num_total.item()\n",
    "            num_correct_norm = num_correct_norm.item()\n",
    "        acc_norm = num_correct_norm / num_total\n",
    "        if master_process:\n",
    "            log_metrics({\"hella/swag\": acc_norm, \"hella/correct norm\": num_correct_norm, \"hella/num total\": num_total})\n",
    "            print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n",
    "\n",
    "    # once in a while generate from the model (except step 0, which is noise)\n",
    "    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
    "        model.eval()\n",
    "        num_return_sequences = 4\n",
    "        max_length = 32\n",
    "        tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(42 + ddp_rank)\n",
    "        while xgen.size(1) < max_length:\n",
    "            # forward the model to get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                # take the logits at the last position\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                # get the probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # do top-k sampling of 50 (huggingface pipeline default)\n",
    "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                # select a token from the top-k probabilities\n",
    "                # note: multinomial does not demand the input to sum to 1\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                # gather the corresponding indices\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                # append to the sequence\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        # print the generated text\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = xgen[i, :max_length].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
    "\n",
    "    if step % 80 == 0 or last_step:\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss_accum = evaluate(model, val_loader)\n",
    "        model.train()\n",
    "\n",
    "        # Save checkpoint and push to HuggingFace\n",
    "        checkpoint_path = save_checkpoint(\n",
    "            raw_model, optimizer, lr_scheduler, step, val_loss_accum.item(), run_name,\n",
    "            train_loader.get_state(), val_loader.get_state()\n",
    "        )\n",
    "        repo.push_to_hub(commit_message=f\"Checkpoint at step {step}\")\n",
    "        \n",
    "        if master_process:\n",
    "            print(f\"Saved checkpoint and pushed to HuggingFace at step {step}\")\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # added after video, this field is also used by the forward pass.\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        # we have to scale the loss to account for gradient accumulation,\n",
    "        # because the gradients just add on each successive backward().\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        log_metrics({\n",
    "            \"lr\": lr, # get_lr()\n",
    "            \"samples\": step * samples_per_step,\n",
    "            \"steps\": step,\n",
    "            \"loss/train\": loss_accum.item(),\n",
    "            # file specific addition\n",
    "            \"global gradient norm\": norm,\n",
    "            \"dt\": dt,\n",
    "            \"tok per sec\": tokens_per_sec\n",
    "        })\n",
    "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()\n",
    "\n",
    "# Final save and push\n",
    "if master_process:\n",
    "    final_checkpoint_path = save_checkpoint(raw_model, optimizer, lr_scheduler, step, val_loss_accum.item(), run_name)\n",
    "    repo.push_to_hub(commit_message=\"Final model\")\n",
    "    print(\"Training completed. Final model pushed to HuggingFace.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
