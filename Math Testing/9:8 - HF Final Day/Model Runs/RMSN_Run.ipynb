{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XocROD5P6fwD",
        "outputId": "b28c48b1-c877-4929-df56-fd26b5b7f137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there would be a mass of people who would not be able to leave the place to go to the city. Now there are people that would stay and there is no way of making them go back to their place.\n",
            "Now there was a man who was going to live in the town, and he thought that he was the only one who could go and go. But he came out and said to him, “Look, there's a woman who's going and she's got a new dress that's better.” And he said, \"You know I'm going, I'll see you next time. I want you to make a list of the people in your town.\" And the man said ‘Look what I can do to help you. So, you're going for the woman.\" So he went and went away. When the day came that day, he got up and looked at him. He said that this was not a good time for him and the other men. Then he looked and saw that they were women and they talked about going back. And they said: ’Well, that woman's not going. She can't go home. It's time to have dinner. If you go with me, then I will\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the model\n",
        "model_path = \"shng2025/GPT-Valkyrie_RMSN-124m__baseModel__\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Load the tokenizer (standard GPT-2 tokenizer)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the pad token ID to the EOS token ID\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Ensure the model and tokenizer are on the same device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "input_ids = inputs.input_ids.to(device)\n",
        "attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "# Generate\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=256,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=model.config.eos_token_id,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
        "\n",
        "# Initialize the pipeline with your model and the GPT-2 tokenizer\n",
        "pipe = pipeline('text-generation',\n",
        "                model=\"shng2025/GPT-Valkyrie_RMSN-124m__baseModel__\",\n",
        "                tokenizer=tokenizer)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Once upon a time\"\n",
        "result = pipe(prompt,\n",
        "              max_length=200,  # Increased max_length\n",
        "              num_return_sequences=1,\n",
        "              do_sample=True,  # Enable sampling\n",
        "              temperature=0.7,  # Add temperature\n",
        "              top_k=50,  # Add top_k\n",
        "              top_p=0.95,  # Add top_p\n",
        "              no_repeat_ngram_size=3,  # Prevent repetition of 3-grams\n",
        "              truncation=True,\n",
        "              padding=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlK1tNpT6jCT",
        "outputId": "337ae321-b620-4591-e1fa-9868ffeb05c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, a new, more beautiful and more beautiful, it was time to make an attempt to make a new woman. The best way to do this was to make it very easy for women to understand that they are not equal to men. If we were to make them equal, then it would be very hard for us to do it, and we would not be able to do anything about it.\n",
            "Some people would say that it was so easy for people to make this mistake, that it would have been too hard, and it would not have been so easy to do. But the truth is that the mistake is very easy and it is very hard to do because we all make mistakes. For example, when I was growing up, I made it very hard because I never saw a woman who was just as beautiful as I was. But when I went to college, I just couldn’t get enough of it. When I went out to the movies, I would make\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xjlFIEfK6pPa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}