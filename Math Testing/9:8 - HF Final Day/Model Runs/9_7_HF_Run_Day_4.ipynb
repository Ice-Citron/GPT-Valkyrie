{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "torchrun --standalone --nproc_per_node=4 train_gpt2-final.py"
      ],
      "metadata": {
        "id": "IuiV14Lgbp1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# task for tommorow\n",
        "\n",
        "- get model converted and usable by HF\n",
        "- fix and clear training files\n",
        "- get RMSN trained\n",
        "- check PN\n",
        "\n",
        "---\n",
        "\n",
        "- close GPU when leave"
      ],
      "metadata": {
        "id": "8FUIdZMcbpzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the model\n",
        "model_path = \"shng2025/GPT-Valkyrie_LN-124m__baseModel__\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Load the tokenizer (standard GPT-2 tokenizer)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the pad token ID to the EOS token ID\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Ensure the model and tokenizer are on the same device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "input_ids = inputs.input_ids.to(device)\n",
        "attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "# Generate\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=256,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    pad_token_id=model.config.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI9TK2243CkV",
        "outputId": "419c7079-4747-456f-dcf7-03b82dbcd0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the first thing that came to mind was the idea of a “new” way of life. The idea was that the world was a place where people could live in peace and harmony.\n",
            "The idea that people were living in harmony with one another was not new. In the Middle Ages, people lived in a society where there was no hierarchy. People lived together in the same house, in one place, and in another. This was called a feudal society. It was based on the belief that there were no rules or regulations that could be enforced by the ruler. There were rules that were not followed by everyone. These rules were called ‘rules of the land’. They were the rules of society, which were based upon the beliefs of people. For example, if a person was born in England, he would have to follow the laws of England. If he was in France, then he had to obey the law of France. He would also have the right to marry someone who was of French descent. However, there would be no such thing as a king. So, it was only the king who had the power to make laws. And the people who were born into a country were considered to be the ’new men\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
        "\n",
        "# Initialize the pipeline with your model and the GPT-2 tokenizer\n",
        "pipe = pipeline('text-generation',\n",
        "                model=\"shng2025/GPT-Valkyrie_LN-124m__baseModel__\",\n",
        "                tokenizer=tokenizer)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Once upon a time\"\n",
        "result = pipe(prompt,\n",
        "              max_length=100,\n",
        "              num_return_sequences=1,\n",
        "              truncation=True,  # Explicitly activate truncation\n",
        "              padding=True)  # Explicitly activate padding\n",
        "\n",
        "# Print the generated text\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVUNpAvX3rHn",
        "outputId": "15359091-7308-4279-9238-7918da6019f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the first thing that came to mind was the idea of a “new” way of life. The idea of a new way of life was a new way of life. The idea of a new way of life was a new way of life.\n",
            "The idea of a new way of life was a new way of life. The idea of a new way of life was a new way of life. The idea of a new way of life was a new way of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QRfX1SK4RbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}