{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization"
      ],
      "metadata": {
        "id": "zKFpZ0GTGHNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVD0HZTKApZf",
        "outputId": "b7a25732-d549-43ac-91bb-cdc6b16f1afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 2, 2])\n",
            "\n",
            "Input:\n",
            "tensor([[[[ 1.9269,  1.4873],\n",
            "          [ 0.9007, -2.1055]],\n",
            "\n",
            "         [[ 0.6784, -1.2345],\n",
            "          [-0.0431, -1.6047]],\n",
            "\n",
            "         [[-0.7521,  1.6487],\n",
            "          [-0.3925, -1.4036]]],\n",
            "\n",
            "\n",
            "        [[[-0.7279, -0.5594],\n",
            "          [-0.7688,  0.7624]],\n",
            "\n",
            "         [[ 1.6423, -0.1596],\n",
            "          [-0.4974,  0.4396]],\n",
            "\n",
            "         [[-0.7581,  1.0783],\n",
            "          [ 0.8008,  1.6806]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2791,  1.2964],\n",
            "          [ 0.6105,  1.3347]],\n",
            "\n",
            "         [[-0.2316,  0.0418],\n",
            "          [-0.2516,  0.8599]],\n",
            "\n",
            "         [[-1.3847, -0.8712],\n",
            "          [-0.2234,  1.7174]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3189, -0.4245],\n",
            "          [ 0.3057, -0.7746]],\n",
            "\n",
            "         [[-1.5576,  0.9956],\n",
            "          [-0.8798, -0.6011]],\n",
            "\n",
            "         [[-1.2742,  2.1228],\n",
            "          [-1.2347, -0.4879]]]])\n",
            "\n",
            "Custom BatchNorm output:\n",
            "tensor([[[[ 1.5237e+00,  1.1110e+00],\n",
            "          [ 5.6032e-01, -2.2619e+00]],\n",
            "\n",
            "         [[ 9.2737e-01, -1.2135e+00],\n",
            "          [ 1.1991e-01, -1.6278e+00]],\n",
            "\n",
            "         [[-6.2543e-01,  1.3278e+00],\n",
            "          [-3.3283e-01, -1.1554e+00]]],\n",
            "\n",
            "\n",
            "        [[[-9.6861e-01, -8.1047e-01],\n",
            "          [-1.0071e+00,  4.3051e-01]],\n",
            "\n",
            "         [[ 2.0061e+00, -1.0504e-02],\n",
            "          [-3.8855e-01,  6.6008e-01]],\n",
            "\n",
            "         [[-6.3031e-01,  8.6371e-01],\n",
            "          [ 6.3794e-01,  1.3537e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 9.1557e-01,  9.3181e-01],\n",
            "          [ 2.8783e-01,  9.6778e-01]],\n",
            "\n",
            "         [[-9.1113e-02,  2.1485e-01],\n",
            "          [-1.1344e-01,  1.1304e+00]],\n",
            "\n",
            "         [[-1.1400e+00, -7.2232e-01],\n",
            "          [-1.9526e-01,  1.3836e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4091e-02, -6.8381e-01],\n",
            "          [ 1.7369e-03, -1.0125e+00]],\n",
            "\n",
            "         [[-1.5751e+00,  1.2824e+00],\n",
            "          [-8.1651e-01, -5.0466e-01]],\n",
            "\n",
            "         [[-1.0501e+00,  1.7134e+00],\n",
            "          [-1.0180e+00, -4.1047e-01]]]], grad_fn=<AddBackward0>)\n",
            "\n",
            "PyTorch BatchNorm output:\n",
            "tensor([[[[ 1.5237e+00,  1.1110e+00],\n",
            "          [ 5.6032e-01, -2.2619e+00]],\n",
            "\n",
            "         [[ 9.2737e-01, -1.2135e+00],\n",
            "          [ 1.1991e-01, -1.6278e+00]],\n",
            "\n",
            "         [[-6.2543e-01,  1.3278e+00],\n",
            "          [-3.3283e-01, -1.1554e+00]]],\n",
            "\n",
            "\n",
            "        [[[-9.6861e-01, -8.1047e-01],\n",
            "          [-1.0071e+00,  4.3051e-01]],\n",
            "\n",
            "         [[ 2.0061e+00, -1.0504e-02],\n",
            "          [-3.8855e-01,  6.6008e-01]],\n",
            "\n",
            "         [[-6.3031e-01,  8.6371e-01],\n",
            "          [ 6.3794e-01,  1.3537e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 9.1557e-01,  9.3181e-01],\n",
            "          [ 2.8783e-01,  9.6778e-01]],\n",
            "\n",
            "         [[-9.1113e-02,  2.1485e-01],\n",
            "          [-1.1344e-01,  1.1304e+00]],\n",
            "\n",
            "         [[-1.1400e+00, -7.2232e-01],\n",
            "          [-1.9526e-01,  1.3836e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4091e-02, -6.8381e-01],\n",
            "          [ 1.7370e-03, -1.0125e+00]],\n",
            "\n",
            "         [[-1.5751e+00,  1.2824e+00],\n",
            "          [-8.1651e-01, -5.0466e-01]],\n",
            "\n",
            "         [[-1.0501e+00,  1.7134e+00],\n",
            "          [-1.0180e+00, -4.1047e-01]]]], grad_fn=<NativeBatchNormBackward0>)\n",
            "\n",
            "Difference between custom and PyTorch implementations:\n",
            "2.384185791015625e-07\n",
            "\n",
            "Custom BatchNorm running mean:\n",
            "tensor([ 0.0304, -0.0150,  0.0017])\n",
            "\n",
            "PyTorch BatchNorm running mean:\n",
            "tensor([ 0.0304, -0.0150,  0.0017])\n",
            "\n",
            "Custom BatchNorm running variance:\n",
            "tensor([1.0135, 0.9798, 1.0511])\n",
            "\n",
            "PyTorch BatchNorm running variance:\n",
            "tensor([1.0210, 0.9852, 1.0612])\n",
            "\n",
            "Difference between custom and PyTorch implementations (eval mode):\n",
            "0.009842395782470703\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class CustomBatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.weight = nn.Parameter(torch.ones(num_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            mean = input.mean([0, 2, 3])\n",
        "            var = input.var([0, 2, 3], unbiased=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        input_normalized = (input - mean[None, :, None, None]) / torch.sqrt(var[None, :, None, None] + self.eps)\n",
        "        return self.weight[None, :, None, None] * input_normalized + self.bias[None, :, None, None]\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a random input tensor\n",
        "batch_size, channels, height, width = 4, 3, 2, 2\n",
        "x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "# Create custom BatchNorm2d layer\n",
        "custom_bn = CustomBatchNorm2d(channels, eps=1e-5, momentum=0.1)\n",
        "\n",
        "# Create PyTorch's BatchNorm2d layer\n",
        "torch_bn = nn.BatchNorm2d(channels, eps=1e-5, momentum=0.1)\n",
        "\n",
        "# Ensure both are in training mode\n",
        "custom_bn.train()\n",
        "torch_bn.train()\n",
        "\n",
        "# Forward pass\n",
        "custom_output = custom_bn(x)\n",
        "torch_output = torch_bn(x)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"\\nInput:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nCustom BatchNorm output:\")\n",
        "print(custom_output)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm output:\")\n",
        "print(torch_output)\n",
        "\n",
        "print(\"\\nDifference between custom and PyTorch implementations:\")\n",
        "print(torch.abs(custom_output - torch_output).max().item())\n",
        "\n",
        "print(\"\\nCustom BatchNorm running mean:\")\n",
        "print(custom_bn.running_mean)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm running mean:\")\n",
        "print(torch_bn.running_mean)\n",
        "\n",
        "print(\"\\nCustom BatchNorm running variance:\")\n",
        "print(custom_bn.running_var)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm running variance:\")\n",
        "print(torch_bn.running_var)\n",
        "\n",
        "# Test in eval mode\n",
        "custom_bn.eval()\n",
        "torch_bn.eval()\n",
        "\n",
        "custom_eval_output = custom_bn(x)\n",
        "torch_eval_output = torch_bn(x)\n",
        "\n",
        "print(\"\\nDifference between custom and PyTorch implementations (eval mode):\")\n",
        "print(torch.abs(custom_eval_output - torch_eval_output).max().item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create a random tensor\n",
        "batch_size = 4\n",
        "channels = 8\n",
        "height = 4\n",
        "width = 4\n",
        "x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "print(\"Input tensor shape:\", x.shape)\n",
        "\n",
        "# Define normalization layers\n",
        "num_groups = 4  # for Group Normalization\n",
        "batch_norm = nn.BatchNorm2d(channels)\n",
        "group_norm = nn.GroupNorm(num_groups, channels)\n",
        "\n",
        "# Apply normalizations\n",
        "batch_norm_output = batch_norm(x)\n",
        "group_norm_output = group_norm(x)\n",
        "\n",
        "print(\"\\nBatch Normalization output shape:\", batch_norm_output.shape)\n",
        "print(\"Group Normalization output shape:\", group_norm_output.shape)\n",
        "\n",
        "# Manual calculation for Group Normalization\n",
        "def manual_group_norm(x, num_groups, eps=1e-5):\n",
        "    batch_size, channels, height, width = x.shape\n",
        "    x = x.view(batch_size, num_groups, -1)\n",
        "\n",
        "    mean = x.mean(dim=2, keepdim=True)\n",
        "    var = x.var(dim=2, keepdim=True)\n",
        "    x = (x - mean) / torch.sqrt(var + eps)\n",
        "\n",
        "    return x.view(batch_size, channels, height, width)\n",
        "\n",
        "manual_group_norm_output = manual_group_norm(x, num_groups)\n",
        "\n",
        "print(\"\\nManual Group Normalization output shape:\", manual_group_norm_output.shape)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nMax difference between PyTorch and manual Group Norm:\")\n",
        "print(torch.max(torch.abs(group_norm_output - manual_group_norm_output)))\n",
        "\n",
        "# Visualize a slice of the data\n",
        "slice_idx = 0\n",
        "print(f\"\\nOriginal data (first channel, first sample):\\n{x[slice_idx, 0]}\")\n",
        "print(f\"\\nBatch Normalized data (first channel, first sample):\\n{batch_norm_output[slice_idx, 0]}\")\n",
        "print(f\"\\nGroup Normalized data (first channel, first sample):\\n{group_norm_output[slice_idx, 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0c6qoJqAwmh",
        "outputId": "fd83753a-db09-4bbe-f488-37ea70ee91ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([4, 8, 4, 4])\n",
            "\n",
            "Batch Normalization output shape: torch.Size([4, 8, 4, 4])\n",
            "Group Normalization output shape: torch.Size([4, 8, 4, 4])\n",
            "\n",
            "Manual Group Normalization output shape: torch.Size([4, 8, 4, 4])\n",
            "\n",
            "Max difference between PyTorch and manual Group Norm:\n",
            "tensor(0.0471, grad_fn=<MaxBackward1>)\n",
            "\n",
            "Original data (first channel, first sample):\n",
            "tensor([[-0.9138, -0.6581,  0.0780,  0.5258],\n",
            "        [-0.4880,  1.1914, -0.8140, -0.7360],\n",
            "        [-1.4032,  0.0360, -0.0635,  0.6756],\n",
            "        [-0.0978,  1.8446, -1.1845,  1.3835]])\n",
            "\n",
            "Batch Normalized data (first channel, first sample):\n",
            "tensor([[-0.7922, -0.5238,  0.2491,  0.7192],\n",
            "        [-0.3452,  1.4179, -0.6874, -0.6055],\n",
            "        [-1.3061,  0.2050,  0.1005,  0.8765],\n",
            "        [ 0.0645,  2.1037, -1.0764,  1.6197]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Group Normalized data (first channel, first sample):\n",
            "tensor([[-1.0120, -0.7520, -0.0033,  0.4520],\n",
            "        [-0.5789,  1.1288, -0.9105, -0.8311],\n",
            "        [-1.5097, -0.0461, -0.1472,  0.6043],\n",
            "        [-0.1821,  1.7931, -1.2873,  1.3243]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIq0zLu9H3EA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}