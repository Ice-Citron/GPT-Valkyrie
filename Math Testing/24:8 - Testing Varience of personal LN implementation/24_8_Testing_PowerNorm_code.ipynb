{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization"
      ],
      "metadata": {
        "id": "zKFpZ0GTGHNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "syncbatchnorm on the other hand is just batchnorm code, but is made so that it works with distributed training, with the running varience and running mean being synchronised"
      ],
      "metadata": {
        "id": "2i1AM9MFNPVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVD0HZTKApZf",
        "outputId": "b7a25732-d549-43ac-91bb-cdc6b16f1afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 2, 2])\n",
            "\n",
            "Input:\n",
            "tensor([[[[ 1.9269,  1.4873],\n",
            "          [ 0.9007, -2.1055]],\n",
            "\n",
            "         [[ 0.6784, -1.2345],\n",
            "          [-0.0431, -1.6047]],\n",
            "\n",
            "         [[-0.7521,  1.6487],\n",
            "          [-0.3925, -1.4036]]],\n",
            "\n",
            "\n",
            "        [[[-0.7279, -0.5594],\n",
            "          [-0.7688,  0.7624]],\n",
            "\n",
            "         [[ 1.6423, -0.1596],\n",
            "          [-0.4974,  0.4396]],\n",
            "\n",
            "         [[-0.7581,  1.0783],\n",
            "          [ 0.8008,  1.6806]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2791,  1.2964],\n",
            "          [ 0.6105,  1.3347]],\n",
            "\n",
            "         [[-0.2316,  0.0418],\n",
            "          [-0.2516,  0.8599]],\n",
            "\n",
            "         [[-1.3847, -0.8712],\n",
            "          [-0.2234,  1.7174]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3189, -0.4245],\n",
            "          [ 0.3057, -0.7746]],\n",
            "\n",
            "         [[-1.5576,  0.9956],\n",
            "          [-0.8798, -0.6011]],\n",
            "\n",
            "         [[-1.2742,  2.1228],\n",
            "          [-1.2347, -0.4879]]]])\n",
            "\n",
            "Custom BatchNorm output:\n",
            "tensor([[[[ 1.5237e+00,  1.1110e+00],\n",
            "          [ 5.6032e-01, -2.2619e+00]],\n",
            "\n",
            "         [[ 9.2737e-01, -1.2135e+00],\n",
            "          [ 1.1991e-01, -1.6278e+00]],\n",
            "\n",
            "         [[-6.2543e-01,  1.3278e+00],\n",
            "          [-3.3283e-01, -1.1554e+00]]],\n",
            "\n",
            "\n",
            "        [[[-9.6861e-01, -8.1047e-01],\n",
            "          [-1.0071e+00,  4.3051e-01]],\n",
            "\n",
            "         [[ 2.0061e+00, -1.0504e-02],\n",
            "          [-3.8855e-01,  6.6008e-01]],\n",
            "\n",
            "         [[-6.3031e-01,  8.6371e-01],\n",
            "          [ 6.3794e-01,  1.3537e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 9.1557e-01,  9.3181e-01],\n",
            "          [ 2.8783e-01,  9.6778e-01]],\n",
            "\n",
            "         [[-9.1113e-02,  2.1485e-01],\n",
            "          [-1.1344e-01,  1.1304e+00]],\n",
            "\n",
            "         [[-1.1400e+00, -7.2232e-01],\n",
            "          [-1.9526e-01,  1.3836e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4091e-02, -6.8381e-01],\n",
            "          [ 1.7369e-03, -1.0125e+00]],\n",
            "\n",
            "         [[-1.5751e+00,  1.2824e+00],\n",
            "          [-8.1651e-01, -5.0466e-01]],\n",
            "\n",
            "         [[-1.0501e+00,  1.7134e+00],\n",
            "          [-1.0180e+00, -4.1047e-01]]]], grad_fn=<AddBackward0>)\n",
            "\n",
            "PyTorch BatchNorm output:\n",
            "tensor([[[[ 1.5237e+00,  1.1110e+00],\n",
            "          [ 5.6032e-01, -2.2619e+00]],\n",
            "\n",
            "         [[ 9.2737e-01, -1.2135e+00],\n",
            "          [ 1.1991e-01, -1.6278e+00]],\n",
            "\n",
            "         [[-6.2543e-01,  1.3278e+00],\n",
            "          [-3.3283e-01, -1.1554e+00]]],\n",
            "\n",
            "\n",
            "        [[[-9.6861e-01, -8.1047e-01],\n",
            "          [-1.0071e+00,  4.3051e-01]],\n",
            "\n",
            "         [[ 2.0061e+00, -1.0504e-02],\n",
            "          [-3.8855e-01,  6.6008e-01]],\n",
            "\n",
            "         [[-6.3031e-01,  8.6371e-01],\n",
            "          [ 6.3794e-01,  1.3537e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 9.1557e-01,  9.3181e-01],\n",
            "          [ 2.8783e-01,  9.6778e-01]],\n",
            "\n",
            "         [[-9.1113e-02,  2.1485e-01],\n",
            "          [-1.1344e-01,  1.1304e+00]],\n",
            "\n",
            "         [[-1.1400e+00, -7.2232e-01],\n",
            "          [-1.9526e-01,  1.3836e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4091e-02, -6.8381e-01],\n",
            "          [ 1.7370e-03, -1.0125e+00]],\n",
            "\n",
            "         [[-1.5751e+00,  1.2824e+00],\n",
            "          [-8.1651e-01, -5.0466e-01]],\n",
            "\n",
            "         [[-1.0501e+00,  1.7134e+00],\n",
            "          [-1.0180e+00, -4.1047e-01]]]], grad_fn=<NativeBatchNormBackward0>)\n",
            "\n",
            "Difference between custom and PyTorch implementations:\n",
            "2.384185791015625e-07\n",
            "\n",
            "Custom BatchNorm running mean:\n",
            "tensor([ 0.0304, -0.0150,  0.0017])\n",
            "\n",
            "PyTorch BatchNorm running mean:\n",
            "tensor([ 0.0304, -0.0150,  0.0017])\n",
            "\n",
            "Custom BatchNorm running variance:\n",
            "tensor([1.0135, 0.9798, 1.0511])\n",
            "\n",
            "PyTorch BatchNorm running variance:\n",
            "tensor([1.0210, 0.9852, 1.0612])\n",
            "\n",
            "Difference between custom and PyTorch implementations (eval mode):\n",
            "0.009842395782470703\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class CustomBatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momenßtum=0.1):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.weight = nn.Parameter(torch.ones(num_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.training:\n",
        "            mean = input.mean([0, 2, 3])\n",
        "            var = input.var([0, 2, 3], unbiased=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        input_normalized = (input - mean[None, :, None, None]) / torch.sqrt(var[None, :, None, None] + self.eps)\n",
        "        return self.weight[None, :, None, None] * input_normalized + self.bias[None, :, None, None]\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a random input tensor\n",
        "batch_size, channels, height, width = 4, 3, 2, 2\n",
        "x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "# Create custom BatchNorm2d layer\n",
        "custom_bn = CustomBatchNorm2d(channels, eps=1e-5, momentum=0.1)\n",
        "\n",
        "# Create PyTorch's BatchNorm2d layer\n",
        "torch_bn = nn.BatchNorm2d(channels, eps=1e-5, momentum=0.1)\n",
        "\n",
        "# Ensure both are in training mode\n",
        "custom_bn.train()\n",
        "torch_bn.train()\n",
        "\n",
        "# Forward pass\n",
        "custom_output = custom_bn(x)\n",
        "torch_output = torch_bn(x)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"\\nInput:\")\n",
        "print(x)\n",
        "\n",
        "print(\"\\nCustom BatchNorm output:\")\n",
        "print(custom_output)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm output:\")\n",
        "print(torch_output)\n",
        "\n",
        "print(\"\\nDifference between custom and PyTorch implementations:\")\n",
        "print(torch.abs(custom_output - torch_output).max().item())\n",
        "\n",
        "print(\"\\nCustom BatchNorm running mean:\")\n",
        "print(custom_bn.running_mean)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm running mean:\")\n",
        "print(torch_bn.running_mean)\n",
        "\n",
        "print(\"\\nCustom BatchNorm running variance:\")\n",
        "print(custom_bn.running_var)\n",
        "\n",
        "print(\"\\nPyTorch BatchNorm running variance:\")\n",
        "print(torch_bn.running_var)\n",
        "\n",
        "# Test in eval mode\n",
        "custom_bn.eval()\n",
        "torch_bn.eval()\n",
        "\n",
        "custom_eval_output = custom_bn(x)\n",
        "torch_eval_output = torch_bn(x)\n",
        "\n",
        "print(\"\\nDifference between custom and PyTorch implementations (eval mode):\")\n",
        "print(torch.abs(custom_eval_output - torch_eval_output).max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Normalization\n",
        "\n",
        "- seems to be the same as BN, except we are splitting them into subgroups, and normalising from there. read: https://claude.ai/chat/445c1357-378a-4261-8d49-ae077ef0e190"
      ],
      "metadata": {
        "id": "Um_1qCuGM8DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create a random tensor\n",
        "batch_size = 4\n",
        "channels = 8\n",
        "height = 4\n",
        "width = 4\n",
        "x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "print(\"Input tensor shape:\", x.shape)\n",
        "\n",
        "# Define normalization layers\n",
        "num_groups = 4  # for Group Normalization\n",
        "batch_norm = nn.BatchNorm2d(channels)\n",
        "group_norm = nn.GroupNorm(num_groups, channels)\n",
        "\n",
        "# Apply normalizations\n",
        "batch_norm_output = batch_norm(x)\n",
        "group_norm_output = group_norm(x)\n",
        "\n",
        "print(\"\\nBatch Normalization output shape:\", batch_norm_output.shape)\n",
        "print(\"Group Normalization output shape:\", group_norm_output.shape)\n",
        "\n",
        "# Manual calculation for Group Normalization\n",
        "def manual_group_norm(x, num_groups, eps=1e-5):\n",
        "    batch_size, channels, height, width = x.shape\n",
        "    x = x.view(batch_size, num_groups, -1)\n",
        "\n",
        "    mean = x.mean(dim=2, keepdim=True)\n",
        "    var = x.var(dim=2, keepdim=True)\n",
        "    x = (x - mean) / torch.sqrt(var + eps)\n",
        "\n",
        "    return x.view(batch_size, channels, height, width)\n",
        "\n",
        "manual_group_norm_output = manual_group_norm(x, num_groups)\n",
        "\n",
        "print(\"\\nManual Group Normalization output shape:\", manual_group_norm_output.shape)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nMax difference between PyTorch and manual Group Norm:\")\n",
        "print(torch.max(torch.abs(group_norm_output - manual_group_norm_output)))\n",
        "\n",
        "# Visualize a slice of the data\n",
        "slice_idx = 0\n",
        "print(f\"\\nOriginal data (first channel, first sample):\\n{x[slice_idx, 0]}\")\n",
        "print(f\"\\nBatch Normalized data (first channel, first sample):\\n{batch_norm_output[slice_idx, 0]}\")\n",
        "print(f\"\\nGroup Normalized data (first channel, first sample):\\n{group_norm_output[slice_idx, 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0c6qoJqAwmh",
        "outputId": "fd83753a-db09-4bbe-f488-37ea70ee91ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([4, 8, 4, 4])\n",
            "\n",
            "Batch Normalization output shape: torch.Size([4, 8, 4, 4])\n",
            "Group Normalization output shape: torch.Size([4, 8, 4, 4])\n",
            "\n",
            "Manual Group Normalization output shape: torch.Size([4, 8, 4, 4])\n",
            "\n",
            "Max difference between PyTorch and manual Group Norm:\n",
            "tensor(0.0471, grad_fn=<MaxBackward1>)\n",
            "\n",
            "Original data (first channel, first sample):\n",
            "tensor([[-0.9138, -0.6581,  0.0780,  0.5258],\n",
            "        [-0.4880,  1.1914, -0.8140, -0.7360],\n",
            "        [-1.4032,  0.0360, -0.0635,  0.6756],\n",
            "        [-0.0978,  1.8446, -1.1845,  1.3835]])\n",
            "\n",
            "Batch Normalized data (first channel, first sample):\n",
            "tensor([[-0.7922, -0.5238,  0.2491,  0.7192],\n",
            "        [-0.3452,  1.4179, -0.6874, -0.6055],\n",
            "        [-1.3061,  0.2050,  0.1005,  0.8765],\n",
            "        [ 0.0645,  2.1037, -1.0764,  1.6197]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Group Normalized data (first channel, first sample):\n",
            "tensor([[-1.0120, -0.7520, -0.0033,  0.4520],\n",
            "        [-0.5789,  1.1288, -0.9105, -0.8311],\n",
            "        [-1.5097, -0.0461, -0.1472,  0.6043],\n",
            "        [-0.1821,  1.7931, -1.2873,  1.3243]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Power Norm\n",
        "\n",
        "- difference in mathematics vs batch norm\n",
        "\n",
        "overall, very simillar. both have learnable parameters like weights and biases. But slight differences in their varience and usage of means and moments etc\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kGwDy3kyNbzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're right to ask for a clear comparison of the mathematics. Let's break down the key equations for BatchNorm, LayerNorm, and PowerNorm (as implemented in the MaskPowerNorm class you've shared):\n",
        "\n",
        "1. Batch Normalization:\n",
        "\n",
        "y = γ * ((x - μ) / √(σ² + ε)) + β\n",
        "\n",
        "Where:\n",
        "- μ is the mean of x over the batch dimension\n",
        "- σ² is the variance of x over the batch dimension\n",
        "- γ and β are learnable parameters\n",
        "- ε is a small constant for numerical stability\n",
        "\n",
        "2. Layer Normalization:\n",
        "\n",
        "y = γ * ((x - μ) / √(σ² + ε)) + β\n",
        "\n",
        "Where:\n",
        "- μ and σ² are calculated across the feature dimension for each sample\n",
        "- γ and β are learnable parameters\n",
        "- ε is a small constant for numerical stability\n",
        "\n",
        "3. Power Normalization (from the provided MaskPowerNorm implementation):\n",
        "\n",
        "y = γ * (x / √(E[x²] + ε)) + β\n",
        "\n",
        "Where:\n",
        "- E[x²] is the second moment (mean of squared values) of x\n",
        "- γ (weight) and β (bias) are learnable parameters\n",
        "- ε is a small constant for numerical stability\n",
        "\n",
        "Key differences:\n",
        "\n",
        "1. Moment calculation:\n",
        "   - BatchNorm: Uses first (mean) and second (variance) moments across the batch dimension\n",
        "   - LayerNorm: Uses first (mean) and second (variance) moments across the feature dimension\n",
        "   - PowerNorm: Uses only the second moment (E[x²]) across a specified dimension (often feature dimension)\n",
        "\n",
        "2. Centering:\n",
        "   - BatchNorm and LayerNorm: Center the data by subtracting the mean (x - μ)\n",
        "   - PowerNorm: Does not center the data\n",
        "\n",
        "3. Scaling factor:\n",
        "   - BatchNorm and LayerNorm: Use standard deviation √(σ² + ε)\n",
        "   - PowerNorm: Uses root mean square √(E[x²] + ε)\n",
        "\n",
        "4. Running statistics:\n",
        "   - BatchNorm: Typically maintains running mean and variance\n",
        "   - LayerNorm: Does not maintain running statistics\n",
        "   - PowerNorm: Maintains a running second moment (running_phi in the code)\n",
        "\n",
        "5. Additional features in PowerNorm:\n",
        "   - Uses an exponential moving average of gradients (ema_gz)\n",
        "   - Has separate forward and backward momentum parameters (alpha_fwd and alpha_bkw)\n",
        "   - Implements a warm-up period\n",
        "\n",
        "The key innovation in PowerNorm is its use of only the second moment for normalization, which can make it more robust to varying input distributions, especially in NLP tasks. By not centering the data, it preserves some information about the absolute magnitude of the inputs, which can be beneficial in certain scenarios.\n",
        "\n",
        "The additional features like the gradient EMA and separate forward/backward momenta are designed to stabilize training and potentially improve performance, especially in scenarios with highly variable input distributions."
      ],
      "metadata": {
        "id": "w5NQ3MrsQq_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Masking\n",
        "\n",
        "- https://claude.ai/chat/2c32cab8-0c1d-449e-be72-da7a79deaa2a"
      ],
      "metadata": {
        "id": "dVGjvs1IVI9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimplifiedMaskPowerNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(num_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
        "        self.running_phi = torch.ones(1, num_features, 1, 1)\n",
        "\n",
        "    def forward(self, input, pad_mask=None):\n",
        "        shaped_input = (len(input.shape) == 2)\n",
        "        if shaped_input:\n",
        "            input = input.unsqueeze(0)\n",
        "        T, B, C = input.shape\n",
        "\n",
        "        if pad_mask is None:\n",
        "            mask_input = input.clone()\n",
        "        else:\n",
        "            bn_mask = ~pad_mask\n",
        "            bn_mask = bn_mask.transpose(0, 1)\n",
        "\n",
        "        if pad_mask is not None:\n",
        "            pad_size = (~bn_mask).sum()\n",
        "            mask_input = input[bn_mask, :]\n",
        "        else:\n",
        "            mask_input = input.clone()\n",
        "\n",
        "        mask_input = mask_input.reshape(-1, self.num_features)\n",
        "\n",
        "        input = input.permute(1, 2, 0).contiguous()\n",
        "        input_shape = input.size()\n",
        "        input = input.reshape(input.size(0), self.num_features, -1)\n",
        "        input = input.unsqueeze(-1)\n",
        "\n",
        "        # Simplified PowerFunction logic\n",
        "        N, C, H, W = input.size()\n",
        "        x2 = (mask_input * mask_input).mean(dim=0)\n",
        "        var = x2.reshape(1, C, 1, 1)\n",
        "\n",
        "        output = input / (var + self.eps).sqrt()\n",
        "        output = self.weight.reshape(1,C,1,1) * output + self.bias.reshape(1,C,1,1)\n",
        "\n",
        "        output = output.reshape(input_shape)\n",
        "        output = output.permute(2, 0, 1).contiguous()\n",
        "\n",
        "        if shaped_input:\n",
        "            output = output.squeeze(0)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Create sample input and mask\n",
        "T, B, C = 5, 2, 8\n",
        "input_tensor = torch.randn(T, B, C)\n",
        "pad_mask = torch.tensor([[False, False, False, True, True],\n",
        "                         [False, False, True, True, True]])\n",
        "\n",
        "# Initialize and apply SimplifiedMaskPowerNorm\n",
        "power_norm = SimplifiedMaskPowerNorm(num_features=C)\n",
        "output = power_norm(input_tensor, pad_mask)\n",
        "\n",
        "# Print shapes and results\n",
        "print(\"Input shape:\", input_tensor.shape)\n",
        "print(\"Pad mask shape:\", pad_mask.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "# Print a slice of input and output for comparison\n",
        "print(\"\\nInput slice (first sequence, first batch):\")\n",
        "print(input_tensor[:, 0, :3])  # First 3 features\n",
        "print(\"\\nOutput slice (first sequence, first batch):\")\n",
        "print(output[:, 0, :3])  # First 3 features\n",
        "\n",
        "# Verify that padding is handled correctly\n",
        "print(\"\\nInput at padded position (3, 0, 0):\", input_tensor[3, 0, 0].item())\n",
        "print(\"Output at padded position (3, 0, 0):\", output[3, 0, 0].item())\n",
        "print(\"Input at padded position (4, 1, 0):\", input_tensor[4, 1, 0].item())\n",
        "print(\"Output at padded position (4, 1, 0):\", output[4, 1, 0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnQWyGzDVU9Z",
        "outputId": "e8bdf2db-9648-44ab-cbfb-ba4ee8a89861"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([5, 2, 8])\n",
            "Pad mask shape: torch.Size([2, 5])\n",
            "Output shape: torch.Size([5, 2, 8])\n",
            "\n",
            "Input slice (first sequence, first batch):\n",
            "tensor([[ 1.8595,  2.6221,  0.3691],\n",
            "        [-0.8032, -1.1209,  0.1956],\n",
            "        [ 0.0613,  0.0853,  0.7481],\n",
            "        [-1.8737,  2.3259, -0.9204],\n",
            "        [-0.4138,  0.5184, -0.7015]])\n",
            "\n",
            "Output slice (first sequence, first batch):\n",
            "tensor([[ 1.9468,  1.8341,  0.5079],\n",
            "        [-0.8409, -0.7841,  0.2692],\n",
            "        [ 0.0642,  0.0596,  1.0295],\n",
            "        [-1.9616,  1.6269, -1.2666],\n",
            "        [-0.4332,  0.3626, -0.9654]], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Input at padded position (3, 0, 0): -1.8736529350280762\n",
            "Output at padded position (3, 0, 0): -1.9615864753723145\n",
            "Input at padded position (4, 1, 0): -1.083811640739441\n",
            "Output at padded position (4, 1, 0): -1.1346766948699951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "- unsure whats masking doing, the pad_mask, GPT have kinda given me a wrong answer instead, as it doesn't understand how does masking work too.\n",
        "\n",
        "- idk, it looks like that, pad_mask isn't used for LN, GN, but is used by BN, PN.\n",
        "\n",
        "- https://claude.ai/chat/2c32cab8-0c1d-449e-be72-da7a79deaa2a"
      ],
      "metadata": {
        "id": "bH8XAI99VWND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're correct in your observation. Let's break this down to understand why:\n",
        "\n",
        "1. Layer Normalization (LN):\n",
        "   - LN normalizes across the feature dimension for each sample independently.\n",
        "   - It doesn't need a pad_mask because it operates on each sample separately, so padding in one sample doesn't affect the normalization of another.\n",
        "\n",
        "2. Group Normalization (GN):\n",
        "   - Similar to LN, GN operates independently on each sample, just dividing features into groups.\n",
        "   - Again, no pad_mask is needed as it doesn't compute statistics across the batch dimension.\n",
        "\n",
        "3. Batch Normalization (BN) and Power Normalization (PN):\n",
        "   - These compute statistics across the batch dimension.\n",
        "   - In sequences of varying lengths (common in NLP tasks), padding is often used to make all sequences in a batch the same length.\n",
        "   - Without a pad_mask, BN and PN would include padding values in their statistics computation, potentially skewing the results.\n",
        "   - The pad_mask allows these methods to compute accurate statistics by excluding padding from the calculations.\n",
        "\n",
        "The key difference is that BN and PN compute statistics across samples in a batch, while LN and GN compute statistics within each sample independently. This is why BN and PN need to account for padding in variable-length sequences, but LN and GN do not.\n",
        "\n",
        "In the context of transformers or other models dealing with variable-length sequences:\n",
        "- LN and GN can be applied as-is without worrying about padding.\n",
        "- BN and PN need the additional pad_mask to ensure they're not affected by padding values when computing batch statistics.\n",
        "\n",
        "This is one reason why LN is often preferred in NLP tasks with transformers - it naturally handles variable-length sequences without the need for explicit padding masks in the normalization layer."
      ],
      "metadata": {
        "id": "Q83pVdJuXbIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pad-mask testing"
      ],
      "metadata": {
        "id": "jpHdSiGrZych"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Let's say we have a batch size of 3 and sequence length of 5\n",
        "B, T = 3, 5\n",
        "pad_token = 0\n",
        "\n",
        "# Example of x after padding (0 is our pad token)\n",
        "x = torch.tensor([\n",
        "    [1, 2, 3, 4, 5],    # Full sequence\n",
        "    [6, 7, 8, 0, 0],    # Padded sequence\n",
        "    [9, 0, 0, 0, 0]     # Heavily padded sequence\n",
        "])\n",
        "\n",
        "print(\"x:\")\n",
        "print(x)\n",
        "\n",
        "# Create pad_mask\n",
        "pad_mask = (x != pad_token).float()\n",
        "\n",
        "print(\"\\npad_mask:\")\n",
        "print(pad_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLora2ulZz-v",
        "outputId": "40b5f7dd-1153-49ac-8026-9f886e68ab2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:\n",
            "tensor([[1, 2, 3, 4, 5],\n",
            "        [6, 7, 8, 0, 0],\n",
            "        [9, 0, 0, 0, 0]])\n",
            "\n",
            "pad_mask:\n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oCD2vCVZ0h3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}