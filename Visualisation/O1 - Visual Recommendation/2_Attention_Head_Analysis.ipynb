{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch matplotlib seaborn"
      ],
      "metadata": {
        "id": "jgfPx90kM7ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLrHrMM8LXtV",
        "outputId": "03cdff7e-0db8-40e3-c405-683f10333252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define the list of model names\n",
        "model_names = [\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD'\n",
        "]\n",
        "\n",
        "# Sample input text\n",
        "sample_text = \"Once upon a time in a land far, far away, there lived a wise old owl.\"\n",
        "\n",
        "# Directory to save attention plots\n",
        "output_dir = \"attention_plots\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize the tokenizer once since all models share the same tokenizer settings\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.cls_token = \"[CLS]\"\n",
        "tokenizer.add_special_tokens({'cls_token': '[CLS]'})  # Ensure special tokens are added\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot a single attention heatmap\n",
        "def plot_attention(ax, attention, layer, head, model_name):\n",
        "    sns.heatmap(attention, cmap='viridis', ax=ax, cbar=False)\n",
        "    ax.set_title(f'{model_name}\\nLayer {layer+1} Head {head+1}', fontsize=8)\n",
        "    ax.set_xlabel('Key Positions', fontsize=6)\n",
        "    ax.set_ylabel('Query Positions', fontsize=6)\n",
        "    ax.tick_params(labelsize=6)\n",
        "\n",
        "# Initialize the figure\n",
        "num_models = len(model_names)\n",
        "num_layers = 12  # Assuming GPT-2 base with 12 layers\n",
        "num_heads = 12   # Assuming GPT-2 base with 12 heads per layer\n",
        "\n",
        "# Define subplot grid: models vertically, layers within models, heads within layers\n",
        "fig_height = num_models * num_layers * 0.5  # Adjust the height per layer\n",
        "fig_width = num_heads * 1.0  # Adjust the width per head\n",
        "fig, axes = plt.subplots(num_models * num_layers, num_heads, figsize=(num_heads * 1.0, num_models * num_layers * 0.5))\n",
        "\n",
        "# Ensure axes is a 2D array even if num_heads=1\n",
        "if num_models * num_layers == 1:\n",
        "    axes = np.array([axes])\n",
        "elif num_heads == 1:\n",
        "    axes = axes.reshape(num_models * num_layers, 1)\n",
        "\n",
        "# Iterate over each model\n",
        "for model_idx, model_name in enumerate(model_names):\n",
        "    print(f\"Processing model: {model_name}\")\n",
        "\n",
        "    # Load the model with attention outputs\n",
        "    model = GPT2Model.from_pretrained(model_name, output_attentions=True)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(sample_text, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    # Get the attention outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    attentions = outputs.attentions  # List of tensors: one for each layer\n",
        "\n",
        "    # Iterate over each layer\n",
        "    for layer in range(num_layers):\n",
        "        # Iterate over each head in the layer\n",
        "        for head in range(num_heads):\n",
        "            # Calculate the absolute position in the axes grid\n",
        "            plot_idx = model_idx * num_layers + layer\n",
        "            ax = axes[plot_idx, head] if num_heads > 1 else axes[plot_idx]\n",
        "\n",
        "            # Extract attention weights for this layer and head\n",
        "            # Shape: (batch_size, num_heads, seq_length, seq_length)\n",
        "            attention = attentions[layer][0, head].cpu().numpy()\n",
        "\n",
        "            # Plot the attention heatmap\n",
        "            plot_attention(ax, attention, layer, head, model_name.split('/')[-1])\n",
        "\n",
        "        if (layer + 1) % 4 == 0:\n",
        "            print(f\"  Processed Layer {layer+1}/{num_layers}\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Add a main title\n",
        "plt.suptitle('Attention Head Comparisons Across Models', fontsize=16, y=1.02)\n",
        "\n",
        "# Save the figure\n",
        "fig_file = os.path.join(output_dir, 'all_models_attention_comparison.png')\n",
        "plt.savefig(fig_file, bbox_inches='tight', dpi=300)\n",
        "print(f\"All attention heatmaps have been plotted and saved to {fig_file}\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-52MGVLe1n",
        "outputId": "3e419e5f-89ab-4cda-b4ad-6681a16cc147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD\n",
            "  Processed Layer 4/12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Averaging Heads across each Layer"
      ],
      "metadata": {
        "id": "ZdMgHf2bOdcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "# Define the list of model names\n",
        "model_names = [\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD'\n",
        "]\n",
        "\n",
        "# Sample input text\n",
        "sample_text = \"Once upon a time in a land far, far away, there lived a wise old owl.\"\n",
        "\n",
        "# Directory to save attention plots (optional)\n",
        "output_dir = \"attention_plots_layer_avg\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize the tokenizer once since all models share the same tokenizer settings\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.cls_token = \"[CLS]\"\n",
        "# Add special tokens only once\n",
        "if '[CLS]' not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
        "\n",
        "# Function to plot a single attention heatmap\n",
        "def plot_attention(ax, attention, layer, model_name):\n",
        "    sns.heatmap(attention, cmap='viridis', ax=ax, cbar=False)\n",
        "    ax.set_title(f'{model_name}\\nLayer {layer+1}', fontsize=8)\n",
        "    ax.set_xlabel('Key Positions', fontsize=6)\n",
        "    ax.set_ylabel('Query Positions', fontsize=6)\n",
        "    ax.tick_params(labelsize=6)\n",
        "\n",
        "# Iterate over each model\n",
        "for model_name in model_names:\n",
        "    print(f\"Processing model: {model_name}\")\n",
        "\n",
        "    # Load the model with attention outputs\n",
        "    model = GPT2Model.from_pretrained(model_name, output_attentions=True)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(sample_text, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    # Get the attention outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    attentions = outputs.attentions  # List of tensors: one for each layer\n",
        "\n",
        "    num_layers = len(attentions)\n",
        "    num_heads = attentions[0].shape[1]  # Assuming all layers have the same number of heads\n",
        "    seq_length = attentions[0].shape[-1]\n",
        "\n",
        "    print(f\"Number of layers: {num_layers}, Number of heads per layer: {num_heads}, Sequence length: {seq_length}\")\n",
        "\n",
        "    # Prepare a grid for plotting: e.g., 4 rows (layers) x 3 cols (heads) for 12 layers\n",
        "    # Adjust grid size based on number of layers\n",
        "    cols = 6  # Number of columns in the grid\n",
        "    rows = int(np.ceil(num_layers / cols))\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
        "    fig.suptitle(f'Layer-wise Averaged Attention - {model_name.split(\"/\")[-1]}', fontsize=12)\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        # Average attention across heads\n",
        "        attention_avg = attentions[layer].mean(dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Determine subplot position\n",
        "        row = layer // cols\n",
        "        col = layer % cols\n",
        "\n",
        "        ax = axs[row, col] if rows > 1 else axs[col]\n",
        "\n",
        "        # Plot the averaged attention\n",
        "        plot_attention(ax, attention_avg, layer, model_name.split('/')[-1])\n",
        "\n",
        "    # Remove any empty subplots\n",
        "    total_subplots = rows * cols\n",
        "    if num_layers < total_subplots:\n",
        "        for empty in range(num_layers, total_subplots):\n",
        "            row = empty // cols\n",
        "            col = empty % cols\n",
        "            ax = axs[row, col] if rows > 1 else axs[col]\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    # Save the figure\n",
        "    save_path = os.path.join(output_dir, f'{model_name.split(\"/\")[-1]}_layer_avg_attention.png')\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved averaged attention plots for model: {model_name}\\n\")\n",
        "\n",
        "print(\"All models have been processed and layer-averaged attention heatmaps have been saved.\")"
      ],
      "metadata": {
        "id": "M6WgdvT3OdNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Averaging Heads across each Layer AND THEN Averaging each Layer"
      ],
      "metadata": {
        "id": "XA8qwHNAOglh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define the list of model names\n",
        "model_names = [\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD',\n",
        "    'shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD'\n",
        "]\n",
        "\n",
        "# Sample input text\n",
        "sample_text = \"Once upon a time in a land far, far away, there lived a wise old owl.\"\n",
        "\n",
        "# Directory to save overall attention plots\n",
        "output_dir = \"attention_plots_overall_avg\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize the tokenizer once since all models share the same tokenizer settings\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.cls_token = \"[CLS]\"\n",
        "# Add special tokens only once\n",
        "if '[CLS]' not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({'cls_token': '[CLS]'})\n",
        "\n",
        "# Function to plot a single overall attention heatmap\n",
        "def plot_overall_attention(ax, attention, model_name):\n",
        "    sns.heatmap(attention, cmap='viridis', ax=ax, cbar=True)\n",
        "    ax.set_title(f'Overall Averaged Attention\\n{model_name}', fontsize=10)\n",
        "    ax.set_xlabel('Key Positions', fontsize=8)\n",
        "    ax.set_ylabel('Query Positions', fontsize=8)\n",
        "    ax.tick_params(labelsize=6)\n",
        "\n",
        "# Initialize a single figure with subplots for all models\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid for four models\n",
        "fig.suptitle('Overall Averaged Attention Across Models', fontsize=16)\n",
        "\n",
        "# Iterate over each model\n",
        "for idx, model_name in enumerate(model_names):\n",
        "    print(f\"Processing model: {model_name}\")\n",
        "\n",
        "    # Load the model with attention outputs\n",
        "    model = GPT2Model.from_pretrained(model_name, output_attentions=True)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(sample_text, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    # Get the attention outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    attentions = outputs.attentions  # List of tensors: one for each layer\n",
        "\n",
        "    num_layers = len(attentions)\n",
        "    num_heads = attentions[0].shape[1]  # Assuming all layers have the same number of heads\n",
        "    seq_length = attentions[0].shape[-1]\n",
        "\n",
        "    print(f\"Number of layers: {num_layers}, Number of heads per layer: {num_heads}, Sequence length: {seq_length}\")\n",
        "\n",
        "    # Average attention across heads for each layer\n",
        "    layer_avg_attentions = [attn.mean(dim=1).squeeze(0).cpu().numpy() for attn in attentions]\n",
        "\n",
        "    # Further average across layers\n",
        "    overall_avg_attention = np.mean(layer_avg_attentions, axis=0)\n",
        "\n",
        "    # Determine subplot position\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    ax = axs[row, col]\n",
        "\n",
        "    # Plot the overall averaged attention\n",
        "    plot_overall_attention(ax, overall_avg_attention, model_name.split('/')[-1])\n",
        "\n",
        "    print(f\"Completed processing for model: {model_name}\\n\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "# Save the figure\n",
        "save_path = os.path.join(output_dir, 'overall_averaged_attention.png')\n",
        "plt.savefig(save_path, dpi=300)\n",
        "plt.close()\n",
        "print(f\"Overall averaged attention heatmaps have been saved to {save_path}\")\n",
        "\n",
        "# Display the plot (optional)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b9ybFp9qLr2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cT8ttq62Onr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# https://chatgpt.com/c/66effe31-bd64-8003-baf2-f8977d9a3a10?model=o1-mini"
      ],
      "metadata": {
        "id": "COOCFpyNPqmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your approach to analyzing ablated transformer models by examining gradient magnitudes is insightful and provides valuable information about the optimization landscape of each model. To further enrich your analysis and gain a more comprehensive understanding of how different normalization strategies impact transformer architectures, you can incorporate several additional methods. Below are some recommended techniques, along with explanations and implementation guidance, tailored to fit within the scope of your IB Extended Essay.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Attention Head Analysis**\n",
        "\n",
        "### **a. Description**\n",
        "Transformer models rely heavily on attention mechanisms. Each layer contains multiple attention heads that focus on different parts of the input. Analyzing these attention patterns can reveal how normalization strategies influence the model's focus and information flow.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Understanding Focus:** Determines whether certain heads become more specialized or diffuse based on normalization.\n",
        "- **Identifying Redundancy:** Helps identify if some heads become redundant or overly concentrated.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Extract Attention Weights:**\n",
        "   During inference, extract the attention weights from each head in each layer.\n",
        "   ```python\n",
        "   from transformers import GPT2Model, GPT2Tokenizer\n",
        "   import torch\n",
        "\n",
        "   tokenizer = GPT2Tokenizer.from_pretrained('your_model')\n",
        "   model = GPT2Model.from_pretrained('your_model', output_attentions=True)\n",
        "   inputs = tokenizer(\"Your input text here\", return_tensors='pt')\n",
        "   outputs = model(**inputs)\n",
        "   attentions = outputs.attentions  # List of attention weights for each layer\n",
        "   ```\n",
        "\n",
        "2. **Visualize Attention Maps:**\n",
        "   Use visualization libraries like Matplotlib or Seaborn to plot attention distributions.\n",
        "   ```python\n",
        "   import matplotlib.pyplot as plt\n",
        "   import seaborn as sns\n",
        "\n",
        "   def plot_attention(attention, layer, head):\n",
        "       sns.heatmap(attention[layer][0][head].detach().numpy(), cmap='viridis')\n",
        "       plt.title(f'Layer {layer+1} Head {head+1} Attention')\n",
        "       plt.xlabel('Key Positions')\n",
        "       plt.ylabel('Query Positions')\n",
        "       plt.show()\n",
        "\n",
        "   # Example: Plotting attention for layer 0, head 0\n",
        "   plot_attention(attentions, layer=0, head=0)\n",
        "   ```\n",
        "\n",
        "3. **Analyze Patterns:**\n",
        "   - **Focused vs. Diffuse Attention:** Determine if normalization leads to more focused attention (peaky distributions) or diffuse attention (more spread out).\n",
        "   - **Cross-Head Specialization:** Check if different heads specialize in different types of attention (e.g., syntactic vs. semantic).\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Normalization Impact:** How normalization affects the distribution and focus of attention across heads.\n",
        "- **Model Efficiency:** Whether normalization leads to more efficient use of attention heads.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Activation Distribution Comparison**\n",
        "\n",
        "### **a. Description**\n",
        "Analyzing the distribution of activations across different layers and components can provide insights into how normalization strategies influence the internal representations of the model.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Distribution Shifts:** Identifies how normalization affects the scale and spread of activations.\n",
        "- **Dynamic Range:** Assesses whether normalization leads to more stable activation ranges across layers.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Extract Activations:**\n",
        "   Modify the model to return activations from each layer.\n",
        "   ```python\n",
        "   from transformers import GPT2Model, GPT2Tokenizer\n",
        "   import torch\n",
        "\n",
        "   tokenizer = GPT2Tokenizer.from_pretrained('your_model')\n",
        "   model = GPT2Model.from_pretrained('your_model', output_hidden_states=True)\n",
        "   inputs = tokenizer(\"Your input text here\", return_tensors='pt')\n",
        "   outputs = model(**inputs)\n",
        "   hidden_states = outputs.hidden_states  # List of hidden states for each layer\n",
        "   ```\n",
        "\n",
        "2. **Compute Statistics:**\n",
        "   Calculate mean, variance, skewness, and kurtosis for activations in each layer.\n",
        "   ```python\n",
        "   import numpy as np\n",
        "\n",
        "   def compute_activation_stats(hidden_states):\n",
        "       stats = []\n",
        "       for layer in hidden_states:\n",
        "           layer_np = layer.detach().numpy()\n",
        "           mean = np.mean(layer_np)\n",
        "           var = np.var(layer_np)\n",
        "           skew = np.mean((layer_np - mean)**3) / (np.var(layer_np)**1.5)\n",
        "           kurt = np.mean((layer_np - mean)**4) / (np.var(layer_np)**2)\n",
        "           stats.append({'mean': mean, 'variance': var, 'skewness': skew, 'kurtosis': kurt})\n",
        "       return stats\n",
        "\n",
        "   activation_stats = compute_activation_stats(hidden_states)\n",
        "   ```\n",
        "\n",
        "3. **Visualize Distributions:**\n",
        "   Use box plots or histograms to compare statistics across models.\n",
        "   ```python\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   def plot_activation_stats(stats, metric):\n",
        "       values = [layer[metric] for layer in stats]\n",
        "       plt.plot(values, label=f'{metric.capitalize()}')\n",
        "       plt.xlabel('Layer')\n",
        "       plt.ylabel(metric.capitalize())\n",
        "       plt.title(f'Activation {metric.capitalize()} Across Layers')\n",
        "       plt.legend()\n",
        "       plt.show()\n",
        "\n",
        "   # Example: Plotting mean activations\n",
        "   plot_activation_stats(activation_stats, 'mean')\n",
        "   ```\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Stability:** Whether normalization leads to more stable activations across layers.\n",
        "- **Scaling Effects:** How normalization affects the scale of representations, potentially impacting learning dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Representational Similarity Analysis (RSA) or Centered Kernel Alignment (CKA)**\n",
        "\n",
        "### **a. Description**\n",
        "RSA and CKA are techniques used to compare the internal representations of different models or different layers within a model. They measure the similarity between activation patterns, providing a quantitative way to assess how different normalization strategies affect learned representations.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Comparative Insights:** Quantifies how similar or different the representations are across models.\n",
        "- **Layer-Wise Analysis:** Allows comparison of specific layers, identifying where normalization has the most impact.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Extract Hidden States:**\n",
        "   Similar to the activation extraction step above.\n",
        "\n",
        "2. **Compute CKA:**\n",
        "   Use libraries like `keras-rl` or implement CKA from scratch. Here’s a simplified example using CKA:\n",
        "   ```python\n",
        "   import numpy as np\n",
        "   from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "   def center_matrix(X):\n",
        "       return X - X.mean(axis=0)\n",
        "\n",
        "   def linear_cka(X, Y):\n",
        "       X_centered = center_matrix(X)\n",
        "       Y_centered = center_matrix(Y)\n",
        "       K = linear_kernel(X_centered, X_centered)\n",
        "       L = linear_kernel(Y_centered, Y_centered)\n",
        "       return np.sum(K * L) / (np.sqrt(np.sum(K * K)) * np.sqrt(np.sum(L * L)))\n",
        "\n",
        "   # Example: Comparing layer 0 of two models\n",
        "   cka_score = linear_cka(hidden_states_model1[0].reshape(-1, hidden_states_model1[0].shape[-1]),\n",
        "                          hidden_states_model2[0].reshape(-1, hidden_states_model2[0].shape[-1]))\n",
        "   print(f'CKA Score for Layer 0: {cka_score}')\n",
        "   ```\n",
        "\n",
        "3. **Interpret Results:**\n",
        "   - **High Similarity:** Indicates that the normalization strategies lead to similar representations.\n",
        "   - **Low Similarity:** Suggests that normalization strategies result in different internal representations.\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Representation Divergence:** How different normalization approaches cause models to learn distinct representations.\n",
        "- **Layer-Specific Effects:** Identifies which layers are most affected by normalization strategies.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Weight Distribution and Norm Comparison**\n",
        "\n",
        "### **a. Description**\n",
        "Analyzing the distribution and norms of model weights can provide insights into how normalization affects parameter scaling and distribution, potentially impacting learning dynamics and generalization.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Weight Scaling:** Normalization can influence the scale of weights, affecting training stability and convergence.\n",
        "- **Distribution Shifts:** Changes in weight distributions can indicate how normalization strategies guide the optimization process.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Extract Model Weights:**\n",
        "   ```python\n",
        "   model = GPT2Model.from_pretrained('your_model')\n",
        "   weights = [param.detach().numpy() for param in model.parameters()]\n",
        "   ```\n",
        "\n",
        "2. **Compute Statistics:**\n",
        "   Calculate mean, variance, skewness, and kurtosis for weights in each layer.\n",
        "   ```python\n",
        "   def compute_weight_stats(weights):\n",
        "       stats = []\n",
        "       for layer_weights in weights:\n",
        "           mean = np.mean(layer_weights)\n",
        "           var = np.var(layer_weights)\n",
        "           skew = np.mean((layer_weights - mean)**3) / (np.var(layer_weights)**1.5)\n",
        "           kurt = np.mean((layer_weights - mean)**4) / (np.var(layer_weights)**2)\n",
        "           stats.append({'mean': mean, 'variance': var, 'skewness': skew, 'kurtosis': kurt})\n",
        "       return stats\n",
        "\n",
        "   weight_stats = compute_weight_stats(weights)\n",
        "   ```\n",
        "\n",
        "3. **Compare Norms:**\n",
        "   Calculate norms (e.g., L2 norm) of weights across layers and models.\n",
        "   ```python\n",
        "   def compute_weight_norms(weights):\n",
        "       norms = []\n",
        "       for layer_weights in weights:\n",
        "           norm = np.linalg.norm(layer_weights)\n",
        "           norms.append(norm)\n",
        "       return norms\n",
        "\n",
        "   weight_norms = compute_weight_norms(weights)\n",
        "   ```\n",
        "\n",
        "4. **Visualize and Compare:**\n",
        "   Use line plots or box plots to compare weight statistics across models.\n",
        "   ```python\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   def plot_weight_stats(stats, metric, model_name):\n",
        "       values = [layer[metric] for layer in stats]\n",
        "       plt.plot(values, label=model_name)\n",
        "       plt.xlabel('Layer')\n",
        "       plt.ylabel(metric.capitalize())\n",
        "       plt.title(f'Weight {metric.capitalize()} Across Layers')\n",
        "       plt.legend()\n",
        "       plt.show()\n",
        "\n",
        "   # Example: Plotting weight variances\n",
        "   plot_weight_stats(weight_stats_model1, 'variance', 'Model 1')\n",
        "   plot_weight_stats(weight_stats_model2, 'variance', 'Model 2')\n",
        "   ```\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Normalization Effects:** Understanding how different normalization strategies impact weight scaling and distribution.\n",
        "- **Training Dynamics:** Insights into how normalization influences the optimization landscape through weight behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Probing Tasks**\n",
        "\n",
        "### **a. Description**\n",
        "Probing tasks involve training simple classifiers on the model’s internal representations to assess what linguistic or semantic information is encoded at different layers. This method helps in understanding the qualitative aspects of what the model learns.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Semantic Understanding:** Determines how different normalization strategies affect the encoding of semantic and syntactic information.\n",
        "- **Layer-Specific Insights:** Identifies at which layers specific types of information are most strongly represented.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Select Probing Tasks:**\n",
        "   Choose tasks like part-of-speech tagging, syntactic parsing, or semantic role labeling.\n",
        "\n",
        "2. **Extract Representations:**\n",
        "   Obtain hidden states from each layer for a set of labeled data.\n",
        "   ```python\n",
        "   from transformers import GPT2Model, GPT2Tokenizer\n",
        "   import torch\n",
        "\n",
        "   tokenizer = GPT2Tokenizer.from_pretrained('your_model')\n",
        "   model = GPT2Model.from_pretrained('your_model', output_hidden_states=True)\n",
        "   inputs = tokenizer(\"Your input text here\", return_tensors='pt')\n",
        "   outputs = model(**inputs)\n",
        "   hidden_states = outputs.hidden_states  # List of hidden states for each layer\n",
        "   ```\n",
        "\n",
        "3. **Train Probing Classifiers:**\n",
        "   For each layer, train a simple classifier (e.g., logistic regression) to predict the task labels.\n",
        "   ```python\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "   from sklearn.metrics import accuracy_score\n",
        "\n",
        "   # Example: Probing for Part-of-Speech Tagging\n",
        "   def probe_layer(hidden_state, labels):\n",
        "       # Flatten the hidden state\n",
        "       X = hidden_state.detach().numpy().reshape(-1, hidden_state.shape[-1])\n",
        "       y = labels.flatten()\n",
        "       # Train a logistic regression classifier\n",
        "       clf = LogisticRegression(max_iter=1000)\n",
        "       clf.fit(X, y)\n",
        "       y_pred = clf.predict(X)\n",
        "       return accuracy_score(y, y_pred)\n",
        "\n",
        "   # Assume `labels` is a numpy array of POS tags corresponding to the input\n",
        "   pos_accuracy = probe_layer(hidden_states[0], labels)\n",
        "   print(f'POS Tagging Accuracy for Layer 0: {pos_accuracy}')\n",
        "   ```\n",
        "\n",
        "4. **Compare Across Models:**\n",
        "   Evaluate and compare probing classifier performance across different models and layers.\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Information Encoding:** How normalization strategies influence the encoding of different types of linguistic information.\n",
        "- **Layer Specialization:** Identifies which layers specialize in encoding specific information based on normalization.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Weight Distribution and Norm Comparison**\n",
        "\n",
        "### **a. Description**\n",
        "Examining the distribution and norms of model weights can reveal how normalization impacts parameter scaling and distribution, which in turn affects training dynamics and generalization.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Parameter Scaling:** Normalization can influence the scale of weights, affecting stability and convergence.\n",
        "- **Distribution Shifts:** Changes in weight distributions can indicate how normalization strategies guide the optimization process.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Extract Model Weights:**\n",
        "   ```python\n",
        "   model = GPT2Model.from_pretrained('your_model')\n",
        "   weights = [param.detach().numpy() for param in model.parameters()]\n",
        "   ```\n",
        "\n",
        "2. **Compute Statistics:**\n",
        "   Calculate mean, variance, skewness, and kurtosis for weights in each layer.\n",
        "   ```python\n",
        "   def compute_weight_stats(weights):\n",
        "       stats = []\n",
        "       for layer_weights in weights:\n",
        "           mean = np.mean(layer_weights)\n",
        "           var = np.var(layer_weights)\n",
        "           skew = np.mean((layer_weights - mean)**3) / (np.var(layer_weights)**1.5)\n",
        "           kurt = np.mean((layer_weights - mean)**4) / (np.var(layer_weights)**2)\n",
        "           stats.append({'mean': mean, 'variance': var, 'skewness': skew, 'kurtosis': kurt})\n",
        "       return stats\n",
        "\n",
        "   weight_stats = compute_weight_stats(weights)\n",
        "   ```\n",
        "\n",
        "3. **Compare Norms:**\n",
        "   Calculate norms (e.g., L2 norm) of weights across layers and models.\n",
        "   ```python\n",
        "   def compute_weight_norms(weights):\n",
        "       norms = []\n",
        "       for layer_weights in weights:\n",
        "           norm = np.linalg.norm(layer_weights)\n",
        "           norms.append(norm)\n",
        "       return norms\n",
        "\n",
        "   weight_norms = compute_weight_norms(weights)\n",
        "   ```\n",
        "\n",
        "4. **Visualize and Compare:**\n",
        "   Use line plots or box plots to compare weight statistics across models.\n",
        "   ```python\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   def plot_weight_stats(stats, metric, model_name):\n",
        "       values = [layer[metric] for layer in stats]\n",
        "       plt.plot(values, label=model_name)\n",
        "       plt.xlabel('Layer')\n",
        "       plt.ylabel(metric.capitalize())\n",
        "       plt.title(f'Weight {metric.capitalize()} Across Layers')\n",
        "       plt.legend()\n",
        "       plt.show()\n",
        "\n",
        "   # Example: Plotting weight variances\n",
        "   plot_weight_stats(weight_stats_model1, 'variance', 'Model 1')\n",
        "   plot_weight_stats(weight_stats_model2, 'variance', 'Model 2')\n",
        "   ```\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Normalization Effects:** Understanding how different normalization strategies impact weight scaling and distribution.\n",
        "- **Training Dynamics:** Insights into how normalization influences the optimization landscape through weight behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Loss Landscape Analysis**\n",
        "\n",
        "### **a. Description**\n",
        "Loss landscape analysis involves visualizing how the loss function behaves around the current parameters. This can help understand the optimization landscape shaped by different normalization strategies.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Optimization Insights:** Reveals whether normalization leads to smoother or more rugged loss landscapes.\n",
        "- **Convergence Behavior:** Helps understand the ease with which models can reach lower loss regions.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Generate Perturbations:**\n",
        "   Apply small perturbations to the model’s parameters in various directions.\n",
        "\n",
        "2. **Compute Loss:**\n",
        "   Evaluate the loss for each perturbed parameter set.\n",
        "   ```python\n",
        "   import torch\n",
        "   import numpy as np\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   def plot_loss_landscape(model, input_ids, target_ids, perturbation=0.1):\n",
        "       # Define perturbation directions\n",
        "       directions = [torch.randn_like(param) for param in model.parameters()]\n",
        "       losses = []\n",
        "       for direction in directions:\n",
        "           perturbed_model = copy.deepcopy(model)\n",
        "           for param, dir in zip(perturbed_model.parameters(), directions):\n",
        "               param.data += perturbation * dir\n",
        "           outputs = perturbed_model(input_ids)\n",
        "           loss = compute_loss(outputs, target_ids)\n",
        "           losses.append(loss.item())\n",
        "       # Plot the loss landscape\n",
        "       plt.scatter(range(len(losses)), losses)\n",
        "       plt.xlabel('Perturbation Direction')\n",
        "       plt.ylabel('Loss')\n",
        "       plt.title('Loss Landscape')\n",
        "       plt.show()\n",
        "\n",
        "   # Example usage\n",
        "   plot_loss_landscape(model, input_ids, target_ids)\n",
        "   ```\n",
        "\n",
        "3. **Visualize Loss Surface:**\n",
        "   Create 2D or 3D plots to visualize how loss varies with parameter perturbations.\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Smoothness:** Smoother loss landscapes suggest better optimization properties influenced by normalization.\n",
        "- **Local Minima:** Understanding the proximity to local minima and how normalization affects their accessibility.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Robustness and Stability Analysis**\n",
        "\n",
        "### **a. Description**\n",
        "Assessing how robust and stable each model is under various input perturbations or noisy conditions can provide insights into the effects of normalization strategies on model generalization and resilience.\n",
        "\n",
        "### **b. Why It’s Useful**\n",
        "- **Generalization:** Robust models are better at handling unseen or noisy data.\n",
        "- **Stability:** Stable training dynamics indicate effective normalization.\n",
        "\n",
        "### **c. How to Implement**\n",
        "1. **Input Perturbations:**\n",
        "   Apply noise, adversarial attacks, or other perturbations to input data.\n",
        "\n",
        "2. **Evaluate Performance:**\n",
        "   Measure how performance metrics degrade under perturbations.\n",
        "   ```python\n",
        "   from transformers import GPT2Tokenizer, GPT2Model\n",
        "   import torch\n",
        "\n",
        "   tokenizer = GPT2Tokenizer.from_pretrained('your_model')\n",
        "   model = GPT2Model.from_pretrained('your_model')\n",
        "\n",
        "   def add_noise(text, noise_level=0.1):\n",
        "       words = text.split()\n",
        "       num_noisy = int(len(words) * noise_level)\n",
        "       noisy_indices = np.random.choice(len(words), num_noisy, replace=False)\n",
        "       for idx in noisy_indices:\n",
        "           words[idx] = ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=3))\n",
        "       return ' '.join(words)\n",
        "\n",
        "   original_text = \"This is a sample input for robustness testing.\"\n",
        "   noisy_text = add_noise(original_text, noise_level=0.2)\n",
        "\n",
        "   inputs_original = tokenizer(original_text, return_tensors='pt')\n",
        "   inputs_noisy = tokenizer(noisy_text, return_tensors='pt')\n",
        "\n",
        "   outputs_original = model(**inputs_original)\n",
        "   outputs_noisy = model(**inputs_noisy)\n",
        "\n",
        "   # Compare outputs or downstream task performance\n",
        "   ```\n",
        "\n",
        "3. **Measure Performance Degradation:**\n",
        "   Compare metrics like accuracy, F1 score, or perplexity between clean and perturbed inputs.\n",
        "\n",
        "### **d. Insights Gained**\n",
        "- **Resilience:** How normalization affects the model’s ability to handle noisy or adversarial inputs.\n",
        "- **Generalization:** Indicates the robustness of learned representations under different conditions.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Summary of Recommended Methods**\n",
        "\n",
        "Given the constraints of your **IB Extended Essay** (4,000 words) and the focus on **Text Generation**, **QA**, and **Summarization**, it's essential to prioritize methods that offer significant insights without overcomplicating your analysis. Here’s a summary of recommended methods to complement your gradient visualization:\n",
        "\n",
        "1. **Attention Head Analysis:**\n",
        "   - **Why:** Reveals how normalization impacts the focus and specialization of attention heads.\n",
        "   - **Implementation:** Extract and visualize attention weights; analyze patterns.\n",
        "\n",
        "2. **Activation Distribution Comparison:**\n",
        "   - **Why:** Shows how normalization affects internal activation scales and distributions.\n",
        "   - **Implementation:** Compute and visualize activation statistics across layers.\n",
        "\n",
        "3. **Representational Similarity Analysis (CKA):**\n",
        "   - **Why:** Quantifies similarity of internal representations between models.\n",
        "   - **Implementation:** Use CKA to compare hidden states across models and layers.\n",
        "\n",
        "4. **Probing Tasks:**\n",
        "   - **Why:** Assesses what linguistic or semantic information is encoded in representations.\n",
        "   - **Implementation:** Train simple classifiers on hidden states for diagnostic tasks.\n",
        "\n",
        "5. **Weight Distribution and Norm Comparison:**\n",
        "   - **Why:** Understands how normalization affects weight scaling and distribution.\n",
        "   - **Implementation:** Analyze and visualize weight statistics and norms.\n",
        "\n",
        "6. **Robustness and Stability Analysis:**\n",
        "   - **Why:** Evaluates model resilience and generalization capabilities.\n",
        "   - **Implementation:** Apply input perturbations and measure performance changes.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Integrating These Methods into Your Research**\n",
        "\n",
        "### **a. Structuring Your Analysis Section**\n",
        "Organize your analysis section to systematically present each method, its implementation, and the insights gained. For example:\n",
        "\n",
        "1. **Gradient Analysis:**\n",
        "   - Present your existing gradient visualization results.\n",
        "   - Interpret the implications of higher gradient magnitudes in ablated models.\n",
        "\n",
        "2. **Attention Head Analysis:**\n",
        "   - Show visualizations of attention maps.\n",
        "   - Discuss differences in attention patterns across models.\n",
        "\n",
        "3. **Activation Distribution Comparison:**\n",
        "   - Present statistics and visualizations of activations.\n",
        "   - Analyze how normalization influences activation scales.\n",
        "\n",
        "4. **Representational Similarity (CKA):**\n",
        "   - Provide CKA scores comparing models.\n",
        "   - Interpret similarities or divergences in internal representations.\n",
        "\n",
        "5. **Probing Tasks:**\n",
        "   - Describe the probing tasks and their outcomes.\n",
        "   - Discuss how normalization affects the encoding of linguistic features.\n",
        "\n",
        "6. **Weight Distribution Comparison:**\n",
        "   - Present weight statistics and norm comparisons.\n",
        "   - Analyze the impact of normalization on weight scaling.\n",
        "\n",
        "7. **Robustness Analysis:**\n",
        "   - Show performance metrics under perturbations.\n",
        "   - Discuss the resilience of each model.\n",
        "\n",
        "### **b. Ensuring Clarity and Conciseness**\n",
        "Given the word limit, focus on the most impactful findings from each method. Use visual aids like tables, graphs, and heatmaps to succinctly present complex data.\n",
        "\n",
        "### **c. Linking to Research Question**\n",
        "Ensure that each method and its findings directly contribute to answering your research question about the impact of different layer normalization strategies. Highlight how each analysis provides unique insights into model behavior and performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **11. Example Integration of Additional Methods**\n",
        "\n",
        "Here’s an example of how you might integrate one additional method into your analysis section:\n",
        "\n",
        "---\n",
        "\n",
        "### **Attention Head Analysis**\n",
        "\n",
        "To further understand the impact of different normalization strategies on the model's internal mechanisms, we conducted an attention head analysis. By visualizing the attention weights of each head across layers, we observed distinct patterns in how models with different normalization strategies focus on input tokens.\n",
        "\n",
        "**Findings:**\n",
        "- **Base Model:** Exhibits a balanced distribution of attention across heads, with some heads consistently focusing on syntactic elements like subjects and objects.\n",
        "- **noNorm Model:** Attention maps are more diffuse, indicating less specialization among heads. This aligns with the higher gradient magnitudes observed, suggesting that without normalization, the model requires more adjustments to achieve optimal focus.\n",
        "- **FFNonly and AttnOnly Models:** Show intermediate patterns, with specific heads demonstrating increased focus on certain token relationships, reflecting the selective application of normalization.\n",
        "\n",
        "**Implications:**\n",
        "These differences suggest that normalization strategies influence the specialization and efficiency of attention heads, potentially affecting how information is processed and integrated within the model.\n",
        "\n",
        "---\n",
        "\n",
        "## **12. Final Recommendations**\n",
        "\n",
        "1. **Prioritize Impactful Methods:**\n",
        "   - Choose methods that provide the most significant and relevant insights into your research question.\n",
        "   - Focus on **Attention Head Analysis** and **Activation Distribution Comparison** as primary additional methods, given their direct relevance and manageability.\n",
        "\n",
        "2. **Use Visual Aids Effectively:**\n",
        "   - Employ heatmaps, line plots, and box plots to present complex data succinctly.\n",
        "   - Ensure all visualizations are clearly labeled and referenced within your text.\n",
        "\n",
        "3. **Maintain Focus and Coherence:**\n",
        "   - Avoid overloading your essay with too many methods. Instead, select a few complementary techniques that together provide a comprehensive view.\n",
        "   - Link each method’s findings back to your central research question to maintain narrative coherence.\n",
        "\n",
        "4. **Document Methodology Clearly:**\n",
        "   - Provide detailed explanations of how each analysis was conducted, ensuring reproducibility.\n",
        "   - Include any scripts or tools used in an appendix if necessary, referencing them in your main text.\n",
        "\n",
        "5. **Reflect on Findings Holistically:**\n",
        "   - Synthesize insights from all analysis methods to present a unified understanding of how normalization strategies impact transformer models.\n",
        "   - Discuss any conflicting findings and provide plausible explanations based on your observations.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "By incorporating additional analytical methods such as **Attention Head Analysis**, **Activation Distribution Comparison**, **Representational Similarity Analysis (CKA)**, **Probing Tasks**, **Weight Distribution Comparison**, and **Robustness Analysis**, you can deepen your understanding of how different layer normalization strategies influence transformer models. These methods complement your existing gradient visualization approach, providing a multifaceted evaluation that aligns with your research objectives while remaining manageable within the constraints of an IB Extended Essay.\n",
        "\n",
        "Focusing on a select few impactful methods ensures that your analysis is both thorough and coherent, enabling you to present a compelling and insightful study. If you need further assistance with implementing any specific method or interpreting results, feel free to ask!"
      ],
      "metadata": {
        "id": "nGpIhtlIPtDu"
      }
    }
  ]
}