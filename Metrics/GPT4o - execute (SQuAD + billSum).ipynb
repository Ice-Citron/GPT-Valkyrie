{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["x9FMmEohbyBE"],"machine_shape":"hm","authorship_tag":"ABX9TyMwapZxnvtQaB0Ma/R+rT3d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install openai==0.28"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7Z0b4S0emX5","executionInfo":{"status":"ok","timestamp":1727593305580,"user_tz":-480,"elapsed":2819,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"5d5916d7-d78d-402b-aab2-83c4e224993b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n"]}]},{"cell_type":"markdown","source":["# QA"],"metadata":{"id":"VtUhu6I8fQGz"}},{"cell_type":"code","source":["import pandas as pd\n","import openai\n","import os\n","import time\n","from tqdm import tqdm\n","from google.colab import userdata\n","import re\n","\n","# Set your OpenAI API key\n","openai.api_key = userdata.get('OPENAI_API_KEY')\n","\n","\n","def get_gpt4_evaluation_qa(model_name, norm_type, variant, question, context, reference_answers, generated_answer):\n","    # Construct the prompt with instructions for commenting on each column\n","    prompt = f\"\"\"\n","As an expert evaluator, your task is to assess the quality of an answer generated by a question-answering system based on the provided context. Please focus on the following criteria:\n","\n","1. **Correctness**: Is the answer correct based on the context?\n","2. **Completeness**: Does the answer fully address the question?\n","3. **Relevance**: Is the answer relevant to the question and context?\n","4. **Fluency**: Is the answer well-written with proper grammar and style?\n","5. **Conciseness**: Is the answer concise and to the point?\n","\n","For each criterion, provide:\n","- **Score**: A number from 1 to 5 (where 1 is poor and 5 is excellent).\n","- **Explanation**: A brief justification for the score.\n","\n","After evaluating each criterion, provide:\n","- **Overall Score**: The average of the five scores.\n","- **Overall Feedback**: A short overall feedback.\n","\n","**Additionally**, provide a short paragraph commenting on the following columns:\n","- **Model Name**: {model_name}\n","- **Norm Type**: {norm_type}\n","- **Variant**: {variant}\n","- **Question**: {question}\n","- **Context**: {context}\n","- **Reference Answers**: {reference_answers}\n","- **Generated Answer**: {generated_answer}\n","\n","**Please present your evaluation in the following structured format:**\n","\n","```\n","Model Name: {model_name}\n","\n","Correctness Score: [1-5]\n","Correctness Explanation: [Your explanation]\n","\n","Completeness Score: [1-5]\n","Completeness Explanation: [Your explanation]\n","\n","Relevance Score: [1-5]\n","Relevance Explanation: [Your explanation]\n","\n","Fluency Score: [1-5]\n","Fluency Explanation: [Your explanation]\n","\n","Conciseness Score: [1-5]\n","Conciseness Explanation: [Your explanation]\n","\n","Overall Score: [Average score]\n","Overall Feedback: [Your feedback]\n","\n","Comments on Columns:\n","[Your short paragraph commenting on each column]\n","```\n","\n","---\n","**Model Name:**\n","{model_name}\n","\n","---\n","**Question:**\n","{question}\n","\n","---\n","**Context:**\n","{context}\n","\n","---\n","**Reference Answers:**\n","{reference_answers}\n","\n","---\n","**Generated Answer:**\n","{generated_answer}\n","\n","---\n","\"\"\"\n","    # Call the OpenAI API using the ChatCompletion endpoint\n","    try:\n","        response = openai.ChatCompletion.create(\n","            model=\"gpt-4o\",\n","            messages=[\n","                {\"role\": \"user\", \"content\": prompt}\n","            ],\n","            max_tokens=700,  # Adjusted to accommodate additional comments\n","            temperature=0.0,  # For deterministic output\n","        )\n","        evaluation = response['choices'][0]['message']['content']\n","        return evaluation\n","    except openai.error.RateLimitError as e:\n","        print(f\"Rate limit error: {e}\")\n","        retry_after = int(e.headers.get(\"Retry-After\", 5))\n","        print(f\"Retrying after {retry_after} seconds...\")\n","        time.sleep(retry_after)\n","        # Retry the request\n","        return get_gpt4_evaluation_qa(model_name, norm_type, variant, question, context, reference_answers, generated_answer)\n","    except Exception as e:\n","        print(f\"Error during OpenAI API call: {e}\")\n","        return None\n","\n","\n","def parse_evaluation_qa(evaluation_text):\n","    \"\"\"\n","    Parses GPT-4's evaluation text and extracts scores, explanations, and comments.\n","    \"\"\"\n","    patterns = {\n","        'Correctness Score': r'Correctness Score:\\s*(\\d)',\n","        'Correctness Explanation': r'Correctness Explanation:\\s*(.*?)\\n\\n',\n","        'Completeness Score': r'Completeness Score:\\s*(\\d)',\n","        'Completeness Explanation': r'Completeness Explanation:\\s*(.*?)\\n\\n',\n","        'Relevance Score': r'Relevance Score:\\s*(\\d)',\n","        'Relevance Explanation': r'Relevance Explanation:\\s*(.*?)\\n\\n',\n","        'Fluency Score': r'Fluency Score:\\s*(\\d)',\n","        'Fluency Explanation': r'Fluency Explanation:\\s*(.*?)\\n\\n',\n","        'Conciseness Score': r'Conciseness Score:\\s*(\\d)',\n","        'Conciseness Explanation': r'Conciseness Explanation:\\s*(.*?)\\n\\n',\n","        'Overall Score': r'Overall Score:\\s*([\\d\\.]+)',\n","        'Overall Feedback': r'Overall Feedback:\\s*(.*?)\\n\\n',\n","        'Comments on Columns': r'Comments on Columns:\\s*(.*)',  # Captures the paragraph\n","    }\n","\n","    result = {}\n","    for key, pattern in patterns.items():\n","        match = re.search(pattern, evaluation_text, re.DOTALL)\n","        if match:\n","            result[key] = match.group(1).strip()\n","        else:\n","            result[key] = None\n","    return result\n","\n","\n","def evaluate_qa():\n","    variants = [\"baseModel\", \"noNorm\", \"AttnOnly\", \"FFNonly\"]\n","    norm_types = [\"LN\", \"RMSN\"]\n","    results = []\n","\n","    for norm_type in norm_types:\n","        for variant in variants:\n","            filename = f\"./modified_QA-source/{norm_type}_{variant}_gpt4_evaluation_data_modified.csv\"\n","            print(f\"Processing file: {filename}\")\n","\n","            if not os.path.exists(filename):\n","                print(f\"File {filename} does not exist. Skipping.\")\n","                continue\n","\n","            # Read the CSV file\n","            df = pd.read_csv(filename)\n","            # Limit to the first 25 rows to manage costs\n","            df_limited = df.head(25) # <-- no limits placed on QA eval\n","            evaluations = []\n","\n","            for idx, row in tqdm(df_limited.iterrows(), total=df_limited.shape[0], desc=f\"Evaluating QA for {filename}\"):\n","                model_name = row['model_name']\n","                norm_type = row['norm_type']\n","                variant = row['variant']\n","                question = row['question']\n","                context = row['context']\n","                reference_answers = row['reference_answers']\n","                generated_answer = row['generated_answer']\n","\n","                # Convert reference_answers from string representation to list if necessary\n","                if isinstance(reference_answers, str):\n","                    try:\n","                        reference_answers = eval(reference_answers)\n","                        if not isinstance(reference_answers, list):\n","                            reference_answers = [reference_answers]\n","                    except:\n","                        reference_answers = [reference_answers]\n","                elif not isinstance(reference_answers, list):\n","                    reference_answers = [reference_answers]\n","\n","                # Optionally truncate context if too long\n","                max_context_length = 2000  # Adjust as needed\n","                if len(context.split()) > max_context_length:\n","                    context = ' '.join(context.split()[:max_context_length]) + \"...\"\n","\n","                # Get GPT-4 evaluation\n","                evaluation_text = get_gpt4_evaluation_qa(model_name, norm_type, variant, question, context, reference_answers, generated_answer)\n","\n","                if evaluation_text:\n","                    parsed = parse_evaluation_qa(evaluation_text)\n","                    parsed['model_name'] = model_name\n","                    parsed['norm_type'] = norm_type\n","                    parsed['variant'] = variant\n","                    evaluations.append(parsed)\n","                    time.sleep(1)  # To respect API rate limits\n","                else:\n","                    evaluations.append({\n","                        'model_name': model_name,\n","                        'norm_type': norm_type,\n","                        'variant': variant,\n","                        'Correctness Score': None,\n","                        'Correctness Explanation': None,\n","                        'Completeness Score': None,\n","                        'Completeness Explanation': None,\n","                        'Relevance Score': None,\n","                        'Relevance Explanation': None,\n","                        'Fluency Score': None,\n","                        'Fluency Explanation': None,\n","                        'Conciseness Score': None,\n","                        'Conciseness Explanation': None,\n","                        'Overall Score': None,\n","                        'Overall Feedback': None,\n","                        'Comments on Columns': 'Error or Empty Response'\n","                    })\n","\n","            # Save evaluations to a new CSV file\n","            eval_df = pd.DataFrame(evaluations)\n","            parsed_eval_filename = f\"{norm_type}_{variant}_gpt4_qa_parsed_evaluations.csv\"\n","            eval_df.to_csv(parsed_eval_filename, index=False)\n","            print(f\"Saved parsed evaluations to {parsed_eval_filename}\")\n","\n","if __name__ == \"__main__\":\n","    evaluate_qa()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDooZn2sfS0Q","executionInfo":{"status":"ok","timestamp":1727595318828,"user_tz":-480,"elapsed":1135873,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"9f22bb18-2d07-49d0-a252-fa921b5fe071"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: ./modified_QA-source/LN_baseModel_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/LN_baseModel_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:16<00:00,  5.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to LN_baseModel_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/LN_noNorm_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/LN_noNorm_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:17<00:00,  5.49s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to LN_noNorm_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/LN_AttnOnly_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/LN_AttnOnly_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:24<00:00,  5.80s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to LN_AttnOnly_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/LN_FFNonly_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/LN_FFNonly_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:20<00:00,  5.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to LN_FFNonly_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/RMSN_baseModel_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/RMSN_baseModel_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:19<00:00,  5.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to RMSN_baseModel_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/RMSN_noNorm_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/RMSN_noNorm_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:20<00:00,  5.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to RMSN_noNorm_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/RMSN_AttnOnly_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/RMSN_AttnOnly_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:29<00:00,  6.00s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to RMSN_AttnOnly_gpt4_qa_parsed_evaluations.csv\n","Processing file: ./modified_QA-source/RMSN_FFNonly_gpt4_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating QA for ./modified_QA-source/RMSN_FFNonly_gpt4_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:24<00:00,  5.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to RMSN_FFNonly_gpt4_qa_parsed_evaluations.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# Summary BillSum"],"metadata":{"id":"zl4-icbBfSF7"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z016fTX05yWx","executionInfo":{"status":"ok","timestamp":1727597956415,"user_tz":-480,"elapsed":1255726,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"3bb206e1-8a7b-4109-9b62-241bf2733564"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: ./Summarization/Original/LN_baseModel_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/LN_baseModel_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:35<00:00,  6.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/LN_baseModel_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/LN_noNorm_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/LN_noNorm_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:32<00:00,  6.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/LN_noNorm_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/LN_AttnOnly_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/LN_AttnOnly_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:33<00:00,  6.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/LN_AttnOnly_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/LN_FFNonly_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/LN_FFNonly_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:33<00:00,  6.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/LN_FFNonly_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/RMSN_baseModel_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/RMSN_baseModel_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:36<00:00,  6.27s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/RMSN_baseModel_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/RMSN_noNorm_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/RMSN_noNorm_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:39<00:00,  6.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/RMSN_noNorm_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/RMSN_AttnOnly_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/RMSN_AttnOnly_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:41<00:00,  6.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/RMSN_AttnOnly_gpt4_summary_parsed_evaluations.csv\n","Processing file: ./Summarization/Original/RMSN_FFNonly_evaluation_data_modified.csv\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating Summaries for ./Summarization/Original/RMSN_FFNonly_evaluation_data_modified.csv: 100%|██████████| 25/25 [02:40<00:00,  6.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Saved parsed evaluations to ./Summarization/RMSN_FFNonly_gpt4_summary_parsed_evaluations.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import pandas as pd\n","import openai\n","import os\n","import time\n","from tqdm import tqdm\n","from google.colab import userdata\n","\n","# Set your OpenAI API key\n","openai.api_key = userdata.get('OPENAI_API_KEY')\n","\n","\n","def get_gpt4_evaluation_summary(model_name, norm_type, variant, truncated_input, generated_summary):\n","    # Construct the prompt for GPT-4 evaluation\n","    prompt = f\"\"\"\n","As an expert evaluator, your task is to assess the quality of a generated summary based on the provided truncated input text. Please focus on the following criteria:\n","\n","1. **Relevance**: Does the summary capture the main points of the truncated input?\n","2. **Conciseness**: Is the summary succinct without unnecessary details?\n","3. **Fluency**: Is the summary well-written with proper grammar and style?\n","4. **Accuracy**: Does the summary accurately represent the content of the truncated input without errors?\n","5. **Coherence**: Is the summary logically organized and easy to understand?\n","\n","For each criterion, provide:\n","- **Score**: A number from 1 to 5 (where 1 is poor and 5 is excellent).\n","- **Explanation**: A brief justification for the score.\n","\n","After evaluating each criterion, provide:\n","- **Overall Score**: The average of the five scores.\n","- **Overall Feedback**: A short overall feedback.\n","\n","**Additionally**, provide a short paragraph commenting on the following columns:\n","- **Model Name**: {model_name}\n","- **Norm Type**: {norm_type}\n","- **Variant**: {variant}\n","- **Truncated Input**: [Truncated input is provided above.]\n","- **Generated Summary**: [Generated summary is provided above.]\n","\n","**Please present your evaluation in the following structured format:**\n","\n","```\n","Model Name: {model_name}\n","\n","Relevance Score: [1-5]\n","Relevance Explanation: [Your explanation]\n","\n","Conciseness Score: [1-5]\n","Conciseness Explanation: [Your explanation]\n","\n","Fluency Score: [1-5]\n","Fluency Explanation: [Your explanation]\n","\n","Accuracy Score: [1-5]\n","Accuracy Explanation: [Your explanation]\n","\n","Coherence Score: [1-5]\n","Coherence Explanation: [Your explanation]\n","\n","Overall Score: [Average score]\n","Overall Feedback: [Your feedback]\n","\n","Comments on Columns:\n","[Your short paragraph commenting on each column]\n","```\n","\n","---\n","**Truncated Input:**\n","\n","{truncated_input}\n","\n","---\n","**Generated Summary:**\n","\n","{generated_summary}\n","\n","---\n","\"\"\"\n","    # Call the OpenAI API using the ChatCompletion endpoint\n","    try:\n","        response = openai.ChatCompletion.create(\n","            model=\"gpt-4o\",\n","            messages=[\n","                {\"role\": \"user\", \"content\": prompt}\n","            ],\n","            max_tokens=700,  # As per your requirement\n","            temperature=0.0,  # For deterministic output\n","        )\n","        evaluation = response['choices'][0]['message']['content']\n","        return evaluation\n","    except openai.error.RateLimitError as e:\n","        print(f\"Rate limit error: {e}\")\n","        retry_after = int(e.headers.get(\"Retry-After\", 5))\n","        print(f\"Retrying after {retry_after} seconds...\")\n","        time.sleep(retry_after)\n","        # Retry the request\n","        return get_gpt4_evaluation_summary(model_name, norm_type, variant, truncated_input, generated_summary)\n","    except Exception as e:\n","        print(f\"Error during OpenAI API call: {e}\")\n","        return None\n","\n","def parse_evaluation_summary(evaluation_text):\n","    \"\"\"\n","    Parses GPT-4's evaluation text and extracts scores, explanations, and comments.\n","    \"\"\"\n","    patterns = {\n","        'Relevance Score': r'Relevance Score:\\s*(\\d)',\n","        'Relevance Explanation': r'Relevance Explanation:\\s*(.*?)\\n\\n',\n","        'Conciseness Score': r'Conciseness Score:\\s*(\\d)',\n","        'Conciseness Explanation': r'Conciseness Explanation:\\s*(.*?)\\n\\n',\n","        'Fluency Score': r'Fluency Score:\\s*(\\d)',\n","        'Fluency Explanation': r'Fluency Explanation:\\s*(.*?)\\n\\n',\n","        'Accuracy Score': r'Accuracy Score:\\s*(\\d)',\n","        'Accuracy Explanation': r'Accuracy Explanation:\\s*(.*?)\\n\\n',\n","        'Coherence Score': r'Coherence Score:\\s*(\\d)',\n","        'Coherence Explanation': r'Coherence Explanation:\\s*(.*?)\\n\\n',\n","        'Overall Score': r'Overall Score:\\s*([\\d\\.]+)',\n","        'Overall Feedback': r'Overall Feedback:\\s*(.*?)\\n\\n',\n","        'Comments on Columns': r'Comments on Columns:\\s*(.*)',  # Captures the paragraph\n","    }\n","\n","    result = {}\n","    for key, pattern in patterns.items():\n","        match = re.search(pattern, evaluation_text, re.DOTALL)\n","        if match:\n","            result[key] = match.group(1).strip()\n","        else:\n","            result[key] = None\n","    return result\n","\n","def evaluate_summaries():\n","    variants = [\"baseModel\", \"noNorm\", \"AttnOnly\", \"FFNonly\"]\n","    norm_types = [\"LN\", \"RMSN\"]\n","\n","    for norm_type in norm_types:\n","        for variant in variants:\n","            filename = f\"./Summarization/Original/{norm_type}_{variant}_evaluation_data_modified.csv\"\n","            print(f\"Processing file: {filename}\")\n","\n","            if not os.path.exists(filename):\n","                print(f\"File {filename} does not exist. Skipping.\")\n","                continue\n","\n","            # Read the CSV file\n","            df = pd.read_csv(filename)\n","            # Limit to the first 25 rows to manage costs\n","            df_limited = df.head(25)\n","            evaluations = []\n","\n","            for idx, row in tqdm(df_limited.iterrows(), total=df_limited.shape[0], desc=f\"Evaluating Summaries for {filename}\"):\n","                model_name = row['model_name']\n","                norm_type = row['norm_type']\n","                variant = row['variant']\n","                truncated_input = row['truncated_input']\n","                generated_summary = row['generated_summary']\n","\n","                # Optionally truncate input if too long\n","                max_input_length = 1000  # Adjust as needed\n","                if len(truncated_input.split()) > max_input_length:\n","                    truncated_input = ' '.join(truncated_input.split()[:max_input_length]) + \"...\"\n","\n","                # Get GPT-4 evaluation\n","                evaluation_text = get_gpt4_evaluation_summary(\n","                    model_name,\n","                    norm_type,\n","                    variant,\n","                    truncated_input,\n","                    generated_summary\n","                )\n","\n","                if evaluation_text:\n","                    parsed = parse_evaluation_summary(evaluation_text)\n","                    parsed['model_name'] = model_name\n","                    parsed['norm_type'] = norm_type\n","                    parsed['variant'] = variant\n","                    evaluations.append(parsed)\n","                    time.sleep(1)  # To respect API rate limits\n","                else:\n","                    evaluations.append({\n","                        'model_name': model_name,\n","                        'norm_type': norm_type,\n","                        'variant': variant,\n","                        'Relevance Score': None,\n","                        'Relevance Explanation': None,\n","                        'Conciseness Score': None,\n","                        'Conciseness Explanation': None,\n","                        'Fluency Score': None,\n","                        'Fluency Explanation': None,\n","                        'Accuracy Score': None,\n","                        'Accuracy Explanation': None,\n","                        'Coherence Score': None,\n","                        'Coherence Explanation': None,\n","                        'Overall Score': None,\n","                        'Overall Feedback': None,\n","                        'Comments on Columns': 'Error or Empty Response'\n","                    })\n","\n","            # Save evaluations to a new CSV file\n","            eval_df = pd.DataFrame(evaluations)\n","            parsed_eval_filename = f\"./Summarization/{norm_type}_{variant}_gpt4_summary_parsed_evaluations.csv\"\n","            eval_df.to_csv(parsed_eval_filename, index=False)\n","            print(f\"Saved parsed evaluations to {parsed_eval_filename}\")\n","\n","if __name__ == \"__main__\":\n","    evaluate_summaries()"]},{"cell_type":"markdown","source":["### Truncation, removing unneccesary columns for summarisation csv"],"metadata":{"id":"x9FMmEohbyBE"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def modify_csv_files():\n","    variants = [\"baseModel\", \"noNorm\", \"AttnOnly\", \"FFNonly\"]\n","    norm_types = [\"LN\", \"RMSN\"]\n","\n","    for norm_type in norm_types:\n","        for variant in variants:\n","            original_filename = f\"{norm_type}_{variant}_evaluation_data.csv\"\n","            modified_filename = f\"./modified/{norm_type}_{variant}_evaluation_data_modified.csv\"\n","            print(f\"Processing file: {original_filename}\")\n","\n","            if not os.path.exists(original_filename):\n","                print(f\"File {original_filename} does not exist. Skipping.\")\n","                continue\n","\n","            # Read the original CSV file\n","            df = pd.read_csv(original_filename)\n","\n","            # Keep only the required columns\n","            columns_to_keep = [\n","                'model_name',\n","                'norm_type',\n","                'variant',\n","                'truncated_input',\n","                'generated_summary'\n","            ]\n","\n","            # Optionally include 'truncated_reference_summary' if you decide to keep it\n","            # columns_to_keep.append('truncated_reference_summary')\n","\n","            df_modified = df[columns_to_keep]\n","\n","            # Save the modified DataFrame to a new CSV file\n","            df_modified.to_csv(modified_filename, index=False)\n","            print(f\"Modified file saved as: {modified_filename}\")\n","\n","if __name__ == \"__main__\":\n","    modify_csv_files()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRdvbDwMb7yD","executionInfo":{"status":"ok","timestamp":1727590923301,"user_tz":-480,"elapsed":580,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"53393f1f-9f79-49e9-ffbf-31602384eb7b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing file: LN_baseModel_evaluation_data.csv\n","Modified file saved as: ./modified/LN_baseModel_evaluation_data_modified.csv\n","Processing file: LN_noNorm_evaluation_data.csv\n","Modified file saved as: ./modified/LN_noNorm_evaluation_data_modified.csv\n","Processing file: LN_AttnOnly_evaluation_data.csv\n","Modified file saved as: ./modified/LN_AttnOnly_evaluation_data_modified.csv\n","Processing file: LN_FFNonly_evaluation_data.csv\n","Modified file saved as: ./modified/LN_FFNonly_evaluation_data_modified.csv\n","Processing file: RMSN_baseModel_evaluation_data.csv\n","Modified file saved as: ./modified/RMSN_baseModel_evaluation_data_modified.csv\n","Processing file: RMSN_noNorm_evaluation_data.csv\n","Modified file saved as: ./modified/RMSN_noNorm_evaluation_data_modified.csv\n","Processing file: RMSN_AttnOnly_evaluation_data.csv\n","Modified file saved as: ./modified/RMSN_AttnOnly_evaluation_data_modified.csv\n","Processing file: RMSN_FFNonly_evaluation_data.csv\n","Modified file saved as: ./modified/RMSN_FFNonly_evaluation_data_modified.csv\n"]}]},{"cell_type":"markdown","source":["### modifying format of SQuAD QA csv file"],"metadata":{"id":"2fjoI8AijG8u"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import re\n","from tqdm import tqdm\n","\n","def extract_norm_type_variant(model_name):\n","    \"\"\"\n","    Extracts norm_type and variant from the model_name string.\n","\n","    Assumes the model_name format is:\n","    shng2025/GPT-Valkyrie_<norm_type>-124m__<variant>__SQuAD\n","\n","    Example:\n","    shng2025/GPT-Valkyrie_LN-124m__AttnOnly__SQuAD\n","    => norm_type: LN\n","    => variant: AttnOnly\n","    \"\"\"\n","    try:\n","        # Split the model_name by '__' to separate components\n","        parts = model_name.split('_')\n","        if len(parts) < 3:\n","            raise ValueError(\"Model name does not have enough parts separated by '__'.\")\n","\n","        # Extract norm_type from the second part (e.g., 'LN-124m')\n","        config_part = parts[1]  # 'LN-124m'\n","        norm_type = config_part.split('-')[0]  # 'LN'\n","\n","        # Extract variant from the third part (e.g., 'AttnOnly')\n","        variant = parts[3]  # 'AttnOnly'\n","\n","        return norm_type, variant\n","    except Exception as e:\n","        print(f\"Error extracting norm_type and variant from model_name '{model_name}': {e}\")\n","        return \"Unknown\", \"Unknown\"\n","\n","def process_qa_csv(input_filepath, output_filepath, max_rows=25):\n","    \"\"\"\n","    Processes a single QA CSV file:\n","    - Extracts norm_type and variant from model_name\n","    - Adds them as separate columns\n","    - Removes unnecessary columns\n","    - Limits to the first `max_rows` rows\n","    - Saves the modified DataFrame to a new CSV file\n","    \"\"\"\n","    try:\n","        # Read the CSV file\n","        df = pd.read_csv(input_filepath)\n","        print(f\"Processing '{input_filepath}' with {len(df)} rows.\")\n","\n","        # Extract norm_type and variant from model_name\n","        df['norm_type'], df['variant'] = zip(*df['model_name'].apply(extract_norm_type_variant))\n","\n","        # Define columns to keep\n","        columns_to_keep = ['model_name', 'norm_type', 'variant', 'question', 'context', 'reference_answers', 'generated_answer']\n","\n","        # Check if these columns exist in the DataFrame\n","        existing_columns = [col for col in columns_to_keep if col in df.columns]\n","        missing_columns = set(columns_to_keep) - set(existing_columns)\n","        if missing_columns:\n","            print(f\"Warning: The following expected columns are missing and will be skipped: {missing_columns}\")\n","\n","        # Create a new DataFrame with only the desired columns\n","        df_modified = df[existing_columns]\n","\n","        # Limit to the first `max_rows` rows\n","        df_modified = df_modified.head(max_rows)\n","\n","        # Save the modified DataFrame to a new CSV file\n","        df_modified.to_csv(output_filepath, index=False)\n","        print(f\"Saved modified CSV to '{output_filepath}' with {len(df_modified)} rows.\\n\")\n","\n","    except Exception as e:\n","        print(f\"Error processing '{input_filepath}': {e}\\n\")\n","\n","def modify_all_qa_csvs(input_dir, output_dir, max_rows=25):\n","    \"\"\"\n","    Processes all QA CSV files in the input directory and saves the modified versions to the output directory.\n","\n","    Parameters:\n","    - input_dir: Directory containing the original QA CSV files.\n","    - output_dir: Directory where modified CSV files will be saved.\n","    - max_rows: Maximum number of rows to keep per CSV.\n","    \"\"\"\n","    # Ensure the output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # List all CSV files in the input directory\n","    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n","\n","    if not csv_files:\n","        print(f\"No CSV files found in '{input_dir}'.\")\n","        return\n","\n","    # Process each CSV file with a progress bar\n","    for csv_file in tqdm(csv_files, desc=\"Processing QA CSV files\"):\n","        input_filepath = os.path.join(input_dir, csv_file)\n","        filename, ext = os.path.splitext(csv_file)\n","        output_filename = f\"{filename}_modified{ext}\"\n","        output_filepath = os.path.join(output_dir, output_filename)\n","\n","        process_qa_csv(input_filepath, output_filepath, max_rows=max_rows)\n","\n","if __name__ == \"__main__\":\n","    # Define input and output directories\n","    input_directory = \"./QA-source\"      # Replace with your actual input directory\n","    output_directory = \"./modified_QA-source\"  # Replace with your desired output directory\n","\n","    # Define maximum number of rows per CSV to keep\n","    max_rows_per_csv = 100\n","\n","    # Run the modification process\n","    modify_all_qa_csvs(input_directory, output_directory, max_rows=max_rows_per_csv)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dL96NnW7cIGT","executionInfo":{"status":"ok","timestamp":1727593064901,"user_tz":-480,"elapsed":582,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"e4cb13aa-d237-4b66-e24d-47451e8cd19b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing QA CSV files: 100%|██████████| 8/8 [00:00<00:00, 159.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Processing './QA-source/LN_noNorm_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/LN_noNorm_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/LN_AttnOnly_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/LN_AttnOnly_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/RMSN_FFNonly_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/RMSN_FFNonly_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/LN_FFNonly_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/LN_FFNonly_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/RMSN_baseModel_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/RMSN_baseModel_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/LN_baseModel_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/LN_baseModel_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/RMSN_noNorm_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/RMSN_noNorm_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n","Processing './QA-source/RMSN_AttnOnly_gpt4_evaluation_data.csv' with 100 rows.\n","Saved modified CSV to './modified_QA-source/RMSN_AttnOnly_gpt4_evaluation_data_modified.csv' with 100 rows.\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["p"],"metadata":{"id":"dReZgwx6jNPw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# starting out before QA\n","\n","0.72 USD spent - 3.37 USD final"],"metadata":{"id":"3MuBW9OEosd2"}},{"cell_type":"markdown","source":["# starting out before Summarization\n","\n","3.39 USD spent -"],"metadata":{"id":"oIbFXfl0yzmk"}}]}