{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNYXjElaXg4CHyx0LxVgxp5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWASq5I0ksDp","executionInfo":{"status":"ok","timestamp":1727543932501,"user_tz":-480,"elapsed":8654,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"e54fb4d1-60a9-4901-e127-2f9af877a380"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Collecting datasets\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"]}],"source":["!pip install torch transformers datasets tqdm pandas evaluate"]},{"cell_type":"code","source":["from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n","from datasets import load_dataset\n","from evaluate import load as load_metric\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","def load_model_and_tokenizer(model_name):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","    return model, tokenizer\n","\n","def evaluate_model(model_name, dataset, norm_type, max_samples=100):\n","    model, tokenizer = load_model_and_tokenizer(model_name)\n","    model.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    all_predictions = []\n","    all_references = []\n","    data_for_gpt4 = []\n","\n","    for i, example in enumerate(tqdm(dataset, desc=f\"Evaluating {model_name}\")):\n","        if i >= max_samples:\n","            break\n","\n","        question = example[\"question\"]\n","        context = example[\"context\"]\n","        reference_answers = example[\"answers\"][\"text\"]\n","\n","        inputs = tokenizer(\n","            question,\n","            context,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=512,\n","            padding=True\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        start_logits = outputs.start_logits[0].cpu().numpy()\n","        end_logits = outputs.end_logits[0].cpu().numpy()\n","\n","        # Find the tokens with the highest `start` and `end` scores\n","        start_index = np.argmax(start_logits)\n","        end_index = np.argmax(end_logits)\n","\n","        # Compute the score of the \"no answer\" option\n","        no_answer_score = start_logits[0] + end_logits[0]\n","        best_answer_score = start_logits[start_index] + end_logits[end_index]\n","\n","        if no_answer_score > best_answer_score or end_index < start_index:\n","            answer = \"\"\n","        else:\n","            answer_tokens = inputs[\"input_ids\"][0][start_index:end_index+1]\n","            answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n","\n","        # Collect predictions and references\n","        prediction = {\"id\": example[\"id\"], \"prediction_text\": answer, \"no_answer_probability\": float(1 / (1 + np.exp(best_answer_score - no_answer_score)))}\n","        reference = {\"id\": example[\"id\"], \"answers\": example[\"answers\"]}\n","\n","        all_predictions.append(prediction)\n","        all_references.append(reference)\n","\n","        # Collect data for GPT-4 evaluation\n","        data_for_gpt4.append({\n","            \"model_name\": model_name,  # Add the model_name here\n","            \"question\": question,\n","            \"context\": context,\n","            \"reference_answers\": reference_answers,\n","            \"generated_answer\": answer\n","        })\n","\n","    # Save data for GPT-4 evaluation\n","    df = pd.DataFrame(data_for_gpt4)\n","    # Create a filename based on norm_type and variant\n","    # norm_type = model_name.split(\"_\")[2].split(\"-\")[0]\n","    variant = model_name.split(\"__\")[1]\n","    filename = f\"{norm_type}_{variant}_gpt4_evaluation_data.csv\"\n","    df.to_csv(filename, index=False)\n","    print(f\"Data for GPT-4 evaluation saved to {filename}\")\n","\n","    # Compute evaluation metrics\n","    squad_metric = load_metric(\"squad_v2\")\n","    results = squad_metric.compute(predictions=all_predictions, references=all_references)\n","    return results\n","\n","def main():\n","    variants = [\"baseModel\", \"noNorm\", \"AttnOnly\", \"FFNonly\"]\n","    norm_types = [\"LN\", \"RMSN\"]\n","    results = []\n","    # Load dataset\n","    dataset = load_dataset(\"squad_v2\", split=\"validation\")\n","    # Limit to first 100 examples\n","    dataset = dataset.select(range(100))\n","    for norm_type in norm_types:\n","        for variant in variants:\n","            model_name = f\"shng2025/GPT-Valkyrie_{norm_type}-124m__{variant}__SQuAD\"\n","            print(f\"\\nEvaluating model {model_name}\")\n","            try:\n","                metrics = evaluate_model(model_name, dataset, norm_type, max_samples=100)\n","                metrics['model_name'] = model_name\n","                metrics['norm_type'] = norm_type\n","                metrics['variant'] = variant\n","                results.append(metrics)\n","            except Exception as e:\n","                print(f\"Error evaluating model {model_name}: {e}\")\n","                continue\n","    # Save results to CSV\n","    df = pd.DataFrame(results)\n","    df.to_csv('squad_evaluation_results.csv', index=False)\n","    print(\"\\nEvaluation results saved to squad_evaluation_results.csv\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNRXbtsAkxMG","executionInfo":{"status":"ok","timestamp":1727544296993,"user_tz":-480,"elapsed":33566,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"d8a910bd-f93e-4d19-8438-81607f90874c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating model shng2025/GPT-Valkyrie_LN-124m__baseModel__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_LN-124m__baseModel__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 74.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to LN_baseModel_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_LN-124m__noNorm__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_LN-124m__noNorm__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 77.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to LN_noNorm_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_LN-124m__AttnOnly__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_LN-124m__AttnOnly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 77.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to LN_AttnOnly_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_LN-124m__FFNonly__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_LN-124m__FFNonly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 77.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to LN_FFNonly_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 76.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to RMSN_baseModel_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 76.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to RMSN_noNorm_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 77.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to RMSN_AttnOnly_gpt4_evaluation_data.csv\n","\n","Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 75.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Data for GPT-4 evaluation saved to RMSN_FFNonly_gpt4_evaluation_data.csv\n","\n","Evaluation results saved to squad_evaluation_results.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XSW-f2d7k_HL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GPT-4o SQuAD prompt"],"metadata":{"id":"BWZM7pvYmw4V"}},{"cell_type":"markdown","source":["As an expert evaluator, your task is to assess the quality of an answer generated by a question-answering system based on the provided context. Please focus on the following criteria:\n","\n","1. **Correctness**: Is the answer correct based on the context?\n","2. **Completeness**: Does the answer fully address the question?\n","3. **Relevance**: Is the answer relevant to the question and context?\n","4. **Fluency**: Is the answer well-written with proper grammar and style?\n","5. **Conciseness**: Is the answer concise and to the point?\n","\n","For each criterion, provide:\n","\n","- **Score**: A number from 1 to 5 (where 1 is poor and 5 is excellent).\n","- **Explanation**: A brief justification for the score.\n","\n","After evaluating each criterion, provide an **overall score** and a **short overall feedback**.\n","\n","---\n","\n","**Model Name:**\n","\n","*Insert the model name here*\n","\n","---\n","\n","**Question:**\n","\n","*Insert the question here*\n","\n","---\n","\n","**Context:**\n","\n","*Insert the context here*\n","\n","---\n","\n","**Reference Answers:**\n","\n","*Insert the reference answers here*\n","\n","---\n","\n","**Generated Answer:**\n","\n","*Insert the generated answer here*\n","\n","---\n","\n","**Your Evaluation:**\n","\n","*GPT-4 will fill this part*\n"],"metadata":{"id":"E7ximiSJm0wv"}}]}