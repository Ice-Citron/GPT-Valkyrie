{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IQGvEeef_DT0"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets==2.21.0\n",
        "# !pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TfGhtPskBU_H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2ForQuestionAnswering, GPT2TokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import wandb\n",
        "from transformers.integrations import WandbCallback\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_metric # used in compute_metrics\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "def freeze_layers(model, variant_type):\n",
        "    if variant_type == \"noNorm\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln\" in name:\n",
        "                param.requires_grad = False\n",
        "    elif variant_type == \"AttnOnly\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln_2\" in name:  # Freeze FFN layer norm\n",
        "                param.requires_grad = False\n",
        "    elif variant_type == \"FFOnly\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln_1\" in name:  # Freeze attention layer norm\n",
        "                param.requires_grad = False\n",
        "    # For baseModel, we don't freeze any layers\n",
        "\n",
        "def prepare_squad_dataset(tokenizer):\n",
        "    dataset = load_dataset(\"squad\")\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        contexts = examples[\"context\"]\n",
        "        answers = examples[\"answers\"]\n",
        "        example_ids = examples[\"id\"]\n",
        "\n",
        "        # Use a unique separator between question and context\n",
        "        separator = tokenizer.eos_token  # GPT-2's eos_token is '<|endoftext|>'\n",
        "        separator_length = len(separator)\n",
        "\n",
        "        # Concatenate question and context with the separator\n",
        "        inputs = [question + separator + context for question, context in zip(questions, contexts)]\n",
        "        \n",
        "        # Keep track of question lengths to adjust character positions later\n",
        "        question_lengths = [len(question) for question in questions]\n",
        "\n",
        "        # Tokenize concatenated inputs\n",
        "        tokenized_examples = tokenizer(\n",
        "            inputs,\n",
        "            max_length=384,\n",
        "            truncation=True,\n",
        "            stride=128,\n",
        "            return_overflowing_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            return_offsets_mapping=True,  # Ensure offsets are returned\n",
        "        )\n",
        "\n",
        "        # Map from features to examples\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")  # We will add this back to tokenized_examples\n",
        "\n",
        "        # Add example_id to the tokenized examples\n",
        "        tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "        # Initialize lists\n",
        "        tokenized_examples[\"offset_mapping\"] = []\n",
        "        tokenized_examples[\"example_id\"] = []\n",
        "        tokenized_examples[\"start_positions\"] = []\n",
        "        tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "            # Map feature to its example index\n",
        "            sample_index = sample_mapping[i]\n",
        "            tokenized_examples[\"example_id\"].append(example_ids[sample_index])\n",
        "\n",
        "            # Get the offsets for the current feature\n",
        "            offsets = offset_mapping[i]\n",
        "            tokenized_examples[\"offset_mapping\"].append(offsets)  # Add offsets to features\n",
        "\n",
        "            # Get the answer text and its start position\n",
        "            answer = answers[sample_index]\n",
        "            if len(answer[\"answer_start\"]) == 0 or len(answer[\"text\"][0]) == 0:\n",
        "                # If there's no answer, set positions to 0\n",
        "                tokenized_examples[\"start_positions\"].append(0)\n",
        "                tokenized_examples[\"end_positions\"].append(0)\n",
        "                # Do not use 'continue' here; allow the loop to proceed\n",
        "                continue\n",
        "\n",
        "            # Compute the start and end character positions of the answer in the concatenated input\n",
        "            answer_start_char = answer[\"answer_start\"][0]\n",
        "            answer_end_char = answer_start_char + len(answer[\"text\"][0])\n",
        "\n",
        "            # Adjust the answer positions to account for the question and separator\n",
        "            context_start_char = question_lengths[sample_index] + separator_length\n",
        "            adjusted_answer_start = answer_start_char + context_start_char\n",
        "            adjusted_answer_end = answer_end_char + context_start_char\n",
        "\n",
        "            # Find the start and end token indices in the tokenized input\n",
        "            start_position = None\n",
        "            end_position = None\n",
        "            for idx, (offset_start, offset_end) in enumerate(offsets):\n",
        "                if offset_start is None or offset_end is None:\n",
        "                    continue\n",
        "                if offset_start <= adjusted_answer_start < offset_end:\n",
        "                    start_position = idx\n",
        "                if offset_start < adjusted_answer_end <= offset_end:\n",
        "                    end_position = idx\n",
        "                    break\n",
        "            if start_position is not None and end_position is not None:\n",
        "                tokenized_examples[\"start_positions\"].append(start_position)\n",
        "                tokenized_examples[\"end_positions\"].append(end_position)\n",
        "            else:\n",
        "                # If the answer is not found in the tokenized input, set positions to 0\n",
        "                tokenized_examples[\"start_positions\"].append(0)\n",
        "                tokenized_examples[\"end_positions\"].append(0)\n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    tokenized_train = dataset['train'].map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['train'].column_names,\n",
        "        load_from_cache_file=False,\n",
        "    )\n",
        "\n",
        "    # Prepare the validation dataset and collect features\n",
        "    validation_features = []\n",
        "\n",
        "    def preprocess_validation_function(examples):\n",
        "        # Use the same preprocessing function\n",
        "        tokenized_examples = preprocess_function(examples)\n",
        "\n",
        "        # Collect features with example_id\n",
        "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "            feature = {key: tokenized_examples[key][i] for key in tokenized_examples.keys()}\n",
        "            validation_features.append(feature)\n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    tokenized_validation = dataset['validation'].map(\n",
        "        preprocess_validation_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset['validation'].column_names,\n",
        "        load_from_cache_file=False,\n",
        "    )\n",
        "\n",
        "    return tokenized_train, tokenized_validation, validation_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "def postprocess_qa_predictions_single_logits(\n",
        "    examples, features, raw_predictions, tokenizer, n_best_size=20, max_answer_length=30\n",
        "):\n",
        "    import numpy as np\n",
        "    import collections\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    all_logits = raw_predictions  # raw_predictions: (num_features, sequence_length)\n",
        "\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[feature[\"example_id\"]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Loop over all the examples.\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        example_id = example[\"id\"]\n",
        "        # Indices of the features associated with the current example.\n",
        "        feature_indices = features_per_example[example_id]\n",
        "\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Loop through all features associated with the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # Grab the predictions of the model for this feature.\n",
        "            logits = all_logits[feature_index]\n",
        "            # Map positions in logits to spans of text in the original context.\n",
        "            offsets = features[feature_index][\"offset_mapping\"]\n",
        "            input_ids = features[feature_index][\"input_ids\"]\n",
        "\n",
        "            # Find the index of the pad_token (used as separator)\n",
        "            try:\n",
        "                sep_index = input_ids.index(pad_token_id)\n",
        "            except ValueError:\n",
        "                # Separator token not found; skip this feature\n",
        "                continue\n",
        "\n",
        "            # The context starts after the separator token\n",
        "            context_start = sep_index + 1\n",
        "\n",
        "            # Only consider context tokens\n",
        "            context_offsets = offsets[context_start:]\n",
        "            context_logits = logits[context_start:]\n",
        "\n",
        "            # Get indices of the top logits\n",
        "            top_indices = np.argsort(context_logits)[-n_best_size:]\n",
        "\n",
        "            # Generate possible answer spans based on top scoring tokens\n",
        "            for idx in top_indices:\n",
        "                start_index = context_start + idx\n",
        "                for end_index in range(start_index, min(start_index + max_answer_length, len(logits))):\n",
        "                    if end_index >= len(offsets):\n",
        "                        break\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Compute the score for the span (sum of logits)\n",
        "                    span_score = logits[start_index] + logits[end_index]\n",
        "                    start_char = offsets[start_index][0]\n",
        "                    end_char = offsets[end_index][1]\n",
        "                    answer_text = context[start_char:end_char]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": span_score,\n",
        "                            \"text\": answer_text\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        # Select the best answer (with the highest score)\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In case no valid answer is found\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        predictions[example_id] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "class PredictionTrainer(Trainer):\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        prediction_loss_only,\n",
        "        ignore_keys=None,\n",
        "    ):\n",
        "        if not prediction_loss_only:\n",
        "            has_labels = all(inputs.get(k) is not None for k in self.label_names)\n",
        "            inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits\n",
        "\n",
        "            if has_labels:\n",
        "                labels = tuple(inputs.get(name) for name in self.label_names)\n",
        "            else:\n",
        "                labels = None\n",
        "\n",
        "            # Also return the inputs\n",
        "            return (None, logits, labels, inputs)\n",
        "        else:\n",
        "            return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fine_tune_model(model, tokenizer, train_dataset, validation_dataset, output_dir, variant, num_train_epochs=3):\n",
        "    # Initialize wandb run\n",
        "    wandb.init(project=f\"GPT-Valkyrie_LN-124m__{variant}__SQuAD\", reinit=True)\n",
        "    run_name = wandb.run.name\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=80,\n",
        "        per_device_eval_batch_size=80,\n",
        "        warmup_steps=300,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_steps=200,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=run_name,\n",
        "    )\n",
        "\n",
        "    # from transformers import DefaultDataCollator\n",
        "    from transformers import DataCollatorWithPadding\n",
        "\n",
        "    def custom_data_collator(features):\n",
        "        labels = ['start_positions', 'end_positions']\n",
        "        for f in features:\n",
        "            for k in list(f.keys()):\n",
        "                if k not in ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'] + labels:\n",
        "                    del f[k]\n",
        "\n",
        "        # Check if all features have 'offset_mapping'\n",
        "        for i, f in enumerate(features):\n",
        "            if 'offset_mapping' not in f:\n",
        "                # Handle the missing 'offset_mapping'\n",
        "                f['offset_mapping'] = [(0, 0)] * len(f['input_ids'])  # Assign dummy offsets\n",
        "\n",
        "        batch = DataCollatorWithPadding(tokenizer)(features)\n",
        "\n",
        "        batch['offset_mapping'] = [f['offset_mapping'] for f in features]\n",
        "        batch['example_id'] = [f['example_id'] for f in features]\n",
        "\n",
        "        return batch\n",
        "\n",
        "    # Use the custom PredictionTrainer\n",
        "    trainer = PredictionTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=validation_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=custom_data_collator,\n",
        "        compute_metrics=build_compute_metrics_fn(examples),\n",
        "        callbacks=[WandbCallback()],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    wandb.finish()\n",
        "    return trainer.model, run_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEST CODE\n",
        "small_dataset = load_dataset(\"squad\")['train'].select(range(10))\n",
        "tokenized_small_dataset = small_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=small_dataset.column_names,\n",
        ")\n",
        "\n",
        "# Check if all features have 'offset_mapping'\n",
        "for feature in tokenized_small_dataset:\n",
        "    assert 'offset_mapping' in feature, \"offset_mapping missing in feature\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH5wuIcA_QaF",
        "outputId": "b5e03a6a-e4df-4bca-adab-b1df8ae01615"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb84784ef06e445ea927610fdb0e6687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea73c6c24ea7479e89698877831751af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# MAIN LOOP\n",
        "wandb.login()\n",
        "\n",
        "variants = [\"noNorm\", \"AttnOnly\", \"FFNonly\", \"baseModel\"]\n",
        "base_model_path = \"shng2025/GPT-Valkyrie_LN-124m__baseModel__\"  # Changed to LN model\n",
        "\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Prepare the dataset and collect validation features\n",
        "tokenized_train, tokenized_validation, validation_features = prepare_squad_dataset(tokenizer)\n",
        "examples = load_dataset(\"squad\")[\"validation\"]  # Original validation examples\n",
        "\n",
        "\n",
        "def build_compute_metrics_fn(examples):\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels, inputs = eval_pred\n",
        "        logits = logits[0] if isinstance(logits, tuple) else logits  # Ensure logits is an array\n",
        "\n",
        "        # Get the features from inputs\n",
        "        features = {\n",
        "            \"input_ids\": inputs[\"input_ids\"],\n",
        "            \"offset_mapping\": inputs[\"offset_mapping\"],\n",
        "            \"example_id\": inputs[\"example_id\"],\n",
        "        }\n",
        "\n",
        "        # Convert features to a list of dictionaries\n",
        "        features = [\n",
        "            {\n",
        "                \"input_ids\": features[\"input_ids\"][i],\n",
        "                \"offset_mapping\": features[\"offset_mapping\"][i],\n",
        "                \"example_id\": features[\"example_id\"][i],\n",
        "            }\n",
        "            for i in range(len(features[\"input_ids\"]))\n",
        "        ]\n",
        "\n",
        "        # Get final predictions\n",
        "        final_predictions = postprocess_qa_predictions_single_logits(\n",
        "            examples, features, logits, tokenizer\n",
        "        )\n",
        "\n",
        "        # Prepare references\n",
        "        references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "\n",
        "        # Load the SQuAD metric\n",
        "        metric = load_metric(\"squad\")\n",
        "\n",
        "        # Compute the metric\n",
        "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "        results = metric.compute(predictions=formatted_predictions, references=references)\n",
        "\n",
        "        return {\n",
        "            \"exact_match\": results[\"exact_match\"],\n",
        "            \"f1\": results[\"f1\"]\n",
        "        }\n",
        "    return compute_metrics\n",
        "\n",
        "compute_metrics_fn = build_compute_metrics_fn(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "95bcf996da3e44edab5ce5429029c5eb",
            "d02592292dfd4e06b7eea7d833e8732c",
            "c9c3184e6c984dc69a98a1cde5f0e230",
            "ff1cef8f5efa474fb1f81a54eb21aff8",
            "18a2588bb5584d6cbe25dfd6e77b6cbf",
            "9a2812638f9c4c438f4b334653f7686a",
            "cdfff40ed0d44bc48b768c22244b9336",
            "5e010758c86c4c7ba63c164990dae7c1"
          ]
        },
        "id": "6uNhEQNWBk5P",
        "outputId": "e0f84913-67f3-4e39-f3c3-afed78ef52b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing noNorm model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at shng2025/GPT-Valkyrie_LN-124m__noNorm__ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.18.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/user/Main_dir/QA metrics/wandb/run-20240921_182109-8dyo5nv1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/8dyo5nv1' target=\"_blank\">fluent-bush-55</a></strong> to <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/8dyo5nv1' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/8dyo5nv1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "WandbCallback\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'offset_mapping'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m freeze_layers(model, variant)\n\u001b[1;32m     10\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m fine_tuned_model, run_name \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the model locally\u001b[39;00m\n\u001b[1;32m     14\u001b[0m local_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./local_models/GPT-Valkyrie_LN-124m__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m__SQuAD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[7], line 52\u001b[0m, in \u001b[0;36mfine_tune_model\u001b[0;34m(model, tokenizer, train_dataset, validation_dataset, output_dir, variant, num_train_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Use the custom PredictionTrainer\u001b[39;00m\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PredictionTrainer(\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[WandbCallback()],\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mmodel, run_name\n",
            "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/transformers/trainer.py:2178\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2178\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
            "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mfine_tune_model.<locals>.custom_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m f[k]\n\u001b[1;32m     33\u001b[0m batch \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer)(features)\n\u001b[0;32m---> 35\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffset_mapping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
            "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m f[k]\n\u001b[1;32m     33\u001b[0m batch \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer)(features)\n\u001b[0;32m---> 35\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffset_mapping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m     36\u001b[0m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
            "\u001b[0;31mKeyError\u001b[0m: 'offset_mapping'"
          ]
        }
      ],
      "source": [
        "for variant in variants:\n",
        "    print(f\"Processing {variant} model...\")\n",
        "\n",
        "    # Use the correct base model for each variant\n",
        "    model_path = f\"shng2025/GPT-Valkyrie_LN-124m__{variant}__\"\n",
        "    model = GPT2ForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "    freeze_layers(model, variant)\n",
        "\n",
        "    output_dir = f\"./results/{variant}\"\n",
        "    fine_tuned_model, run_name = fine_tune_model(model, tokenizer, tokenized_train, tokenized_validation, output_dir, variant)\n",
        "\n",
        "    # Save the model locally\n",
        "    local_save_dir = f\"./local_models/GPT-Valkyrie_LN-124m__{variant}__SQuAD\"\n",
        "    fine_tuned_model.save_pretrained(local_save_dir)\n",
        "    tokenizer.save_pretrained(local_save_dir)\n",
        "    print(f\"Model saved locally to {local_save_dir}\")\n",
        "\n",
        "    # Push the model to your HuggingFace Hub repository\n",
        "    new_repo_name = f\"shng2025/GPT-Valkyrie_LN-124m__{variant}__SQuAD\"\n",
        "    fine_tuned_model.push_to_hub(new_repo_name, branch=run_name)\n",
        "    tokenizer.push_to_hub(new_repo_name, branch=run_name)\n",
        "    print(f\"Model pushed to HuggingFace Hub: {new_repo_name}, branch: {run_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKwmxfrbBk2W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXWgYLDABk0E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18a2588bb5584d6cbe25dfd6e77b6cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e010758c86c4c7ba63c164990dae7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95bcf996da3e44edab5ce5429029c5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d02592292dfd4e06b7eea7d833e8732c",
              "IPY_MODEL_c9c3184e6c984dc69a98a1cde5f0e230"
            ],
            "layout": "IPY_MODEL_ff1cef8f5efa474fb1f81a54eb21aff8"
          }
        },
        "9a2812638f9c4c438f4b334653f7686a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c3184e6c984dc69a98a1cde5f0e230": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdfff40ed0d44bc48b768c22244b9336",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e010758c86c4c7ba63c164990dae7c1",
            "value": 1
          }
        },
        "cdfff40ed0d44bc48b768c22244b9336": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d02592292dfd4e06b7eea7d833e8732c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18a2588bb5584d6cbe25dfd6e77b6cbf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a2812638f9c4c438f4b334653f7686a",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "ff1cef8f5efa474fb1f81a54eb21aff8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
