{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IQGvEeef_DT0"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets==2.21.0\n",
        "# !pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2ForQuestionAnswering, GPT2TokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import wandb\n",
        "from transformers.integrations import WandbCallback\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_metric # used in compute_metrics\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "def freeze_layers(model, variant_type):\n",
        "    if variant_type == \"noNorm\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln\" in name:\n",
        "                param.requires_grad = False\n",
        "    elif variant_type == \"AttnOnly\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln_2\" in name:  # Freeze FFN layer norm\n",
        "                param.requires_grad = False\n",
        "    elif variant_type == \"FFOnly\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln_1\" in name:  # Freeze attention layer norm\n",
        "                param.requires_grad = False\n",
        "    # For baseModel, we don't freeze any layers\n",
        "\n",
        "def prepare_squad_dataset(tokenizer):\n",
        "    dataset = load_dataset(\"squad\")\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        questions = [q.strip() for q in examples[\"question\"]]\n",
        "        contexts = [c.strip() for c in examples[\"context\"]]\n",
        "\n",
        "        # Tokenize questions and contexts together\n",
        "        tokenized_examples = tokenizer(\n",
        "            questions,\n",
        "            contexts,\n",
        "            max_length=384,\n",
        "            truncation=\"only_second\",\n",
        "            stride=128,\n",
        "            return_overflowing_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            return_offsets_mapping=True,\n",
        "        )\n",
        "\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\", None)\n",
        "        if sample_mapping is None:\n",
        "            sample_mapping = list(range(len(questions)))\n",
        "\n",
        "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "        # help us compute the start_positions and end_positions.\n",
        "        offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "        # Let's label those examples!\n",
        "        tokenized_examples[\"start_positions\"] = []\n",
        "        tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "\n",
        "            # For GPT-2, we'll use the first token as our \"impossible answer\" token\n",
        "            impossible_answer_index = 0\n",
        "\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i) if hasattr(tokenized_examples, \"sequence_ids\") else None\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "            # If no answers are given, set the impossible_answer_index as answer.\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                tokenized_examples[\"start_positions\"].append(impossible_answer_index)\n",
        "                tokenized_examples[\"end_positions\"].append(impossible_answer_index)\n",
        "            else:\n",
        "                # Start/end character index of the answer in the text.\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                # Start token index of the current span in the text.\n",
        "                token_start_index = 0\n",
        "                while sequence_ids is not None and token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n",
        "                    token_start_index += 1\n",
        "\n",
        "                # End token index of the current span in the text.\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids is not None and token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                # Detect if the answer is out of the span (in which case this feature is labeled with the impossible_answer_index).\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    tokenized_examples[\"start_positions\"].append(impossible_answer_index)\n",
        "                    tokenized_examples[\"end_positions\"].append(impossible_answer_index)\n",
        "                else:\n",
        "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                    while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    tokenized_datasets = dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names,\n",
        "    )\n",
        "\n",
        "    return tokenized_datasets\n",
        "\n",
        "\n",
        "from datasets import load_metric\n",
        "from transformers import EvalPrediction\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    metric = load_metric(\"squad\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    start_logits, end_logits = logits\n",
        "\n",
        "    # Convert start and end logits to predictions\n",
        "    start_predictions = np.argmax(start_logits, axis=-1)\n",
        "    end_predictions = np.argmax(end_logits, axis=-1)\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Check if labels is empty or None\n",
        "    if labels is None or len(labels) == 0:\n",
        "        print(\"Warning: Labels are empty or None. Using dummy labels.\")\n",
        "        labels = [{\"id\": str(i), \"answers\": {\"text\": [\"\"], \"answer_start\": [0]}} for i in range(len(start_predictions))]\n",
        "\n",
        "    for i, (start_pred, end_pred) in enumerate(zip(start_predictions, end_predictions)):\n",
        "        # Get the predicted answer\n",
        "        pred_tokens = tokenizer.convert_ids_to_tokens(\n",
        "            range(start_pred, end_pred + 1)\n",
        "        )\n",
        "        pred_text = tokenizer.convert_tokens_to_string(pred_tokens)\n",
        "\n",
        "        # Get the actual answer\n",
        "        if isinstance(labels, (list, np.ndarray)) and i < len(labels):\n",
        "            example_labels = labels[i]\n",
        "            if isinstance(example_labels, dict):\n",
        "                true_text = example_labels.get('answers', {}).get('text', [''])[0]\n",
        "                true_start = example_labels.get('answers', {}).get('answer_start', [0])[0]\n",
        "            else:\n",
        "                true_text = \"\"\n",
        "                true_start = 0\n",
        "        else:\n",
        "            true_text = \"\"\n",
        "            true_start = 0\n",
        "\n",
        "        predictions.append({\n",
        "            \"id\": str(i),\n",
        "            \"prediction_text\": pred_text\n",
        "        })\n",
        "\n",
        "        references.append({\n",
        "            \"id\": str(i),\n",
        "            \"answers\": {\n",
        "                \"text\": [true_text],\n",
        "                \"answer_start\": [true_start]\n",
        "            }\n",
        "        })\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": results[\"exact_match\"],\n",
        "        \"f1\": results[\"f1\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def fine_tune_model(model, tokenizer, dataset, output_dir, variant, num_train_epochs=3):\n",
        "    # Initialize wandb run\n",
        "    wandb.init(project=f\"GPT-Valkyrie_LN-124m__{variant}__SQuAD\", reinit=True)\n",
        "    run_name = wandb.run.name\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=300,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=run_name,\n",
        "    )\n",
        "\n",
        "    from transformers import DefaultDataCollator\n",
        "\n",
        "    # Use default_data_collator instead of a custom one\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DefaultDataCollator(return_tensors=\"pt\"),\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[WandbCallback()],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    wandb.finish()\n",
        "    return trainer.model, run_name"
      ],
      "metadata": {
        "id": "TfGhtPskBU_H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rH5wuIcA_QaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4c0c23-02bb-4d98-bfc5-6daf06e80962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshng2025\u001b[0m (\u001b[33mmarlborough-college-malaysia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# MAIN LOOP\n",
        "wandb.login()\n",
        "\n",
        "variants = [\"noNorm\", \"AttnOnly\", \"FFNonly\", \"baseModel\"]\n",
        "base_model_path = \"shng2025/GPT-Valkyrie_LN-124m__baseModel__\"  # Changed to LN model\n",
        "\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = prepare_squad_dataset(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for variant in variants:\n",
        "    print(f\"Processing {variant} model...\")\n",
        "\n",
        "    # Use the correct base model for each variant\n",
        "    model_path = f\"shng2025/GPT-Valkyrie_LN-124m__{variant}__\"\n",
        "    model = GPT2ForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "    freeze_layers(model, variant)\n",
        "\n",
        "    output_dir = f\"./results/{variant}\"\n",
        "    fine_tuned_model, run_name = fine_tune_model(model, tokenizer, dataset, output_dir, variant)\n",
        "\n",
        "    # Save the model locally\n",
        "    local_save_dir = f\"./local_models/GPT-Valkyrie_LN-124m__{variant}__SQuAD\"\n",
        "    fine_tuned_model.save_pretrained(local_save_dir)\n",
        "    tokenizer.save_pretrained(local_save_dir)\n",
        "    print(f\"Model saved locally to {local_save_dir}\")\n",
        "\n",
        "    # Push the model to your HuggingFace Hub repository\n",
        "    new_repo_name = f\"shng2025/GPT-Valkyrie_LN-124m__{variant}__SQuAD\"\n",
        "    fine_tuned_model.push_to_hub(new_repo_name, branch=run_name)\n",
        "    tokenizer.push_to_hub(new_repo_name, branch=run_name)\n",
        "    print(f\"Model pushed to HuggingFace Hub: {new_repo_name}, branch: {run_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "6uNhEQNWBk5P",
        "outputId": "49ae958e-1297-4f80-c9be-6aa9b8d26276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing noNorm model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at shng2025/GPT-Valkyrie_LN-124m__noNorm__ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240921_145836-nj8n349k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/nj8n349k' target=\"_blank\">clear-star-34</a></strong> to <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/nj8n349k' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/nj8n349k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "WandbCallback\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='119' max='33189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  119/33189 02:42 < 12:46:22, 0.72 it/s, Epoch 0.01/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.082000</td>\n",
              "      <td>4.612017</td>\n",
              "      <td>59.256168</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-9ecccde81bf1>:125: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"squad\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKwmxfrbBk2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXWgYLDABk0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}