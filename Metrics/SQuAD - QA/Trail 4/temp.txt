You're welcome! Let's focus on evaluating your summarization models using the OpenAI API with `gpt-4o` and a maximum token limit of 700. Given your data format:

- **Columns**: `model_name`, `norm_type`, `variant`, `truncated_input`, `generated_summary`

We'll create a script that:

1. Reads your CSV file containing summarization data.
2. Constructs prompts for `gpt-4o` to evaluate each generated summary.
3. Parses the responses to extract evaluation scores and explanations.
4. Handles any potential rate limits or errors gracefully.
5. Saves the evaluations to a new CSV file.

---

## **Summarization Evaluation Script**

Below is the complete Python script that performs the evaluation:

```python
import openai
import os
import pandas as pd
import time
import re
from tqdm import tqdm

# Set your OpenAI API key
openai.api_key = os.getenv('OPENAI_API_KEY')

def get_gpt4_evaluation_summary(model_name, norm_type, variant, truncated_input, generated_summary):
    # Construct the prompt for GPT-4 evaluation
    prompt = f"""
As an expert evaluator, your task is to assess the quality of a generated summary based on the provided truncated input text. Please focus on the following criteria:

1. **Relevance**: Does the summary capture the main points of the truncated input?
2. **Conciseness**: Is the summary succinct without unnecessary details?
3. **Fluency**: Is the summary well-written with proper grammar and style?
4. **Accuracy**: Does the summary accurately represent the content of the truncated input without errors?
5. **Coherence**: Is the summary logically organized and easy to understand?

For each criterion, provide:
- **Score**: A number from 1 to 5 (where 1 is poor and 5 is excellent).
- **Explanation**: A brief justification for the score.

After evaluating each criterion, provide:
- **Overall Score**: The average of the five scores.
- **Overall Feedback**: A short overall feedback.

**Additionally**, provide a short paragraph commenting on the following columns:
- **Model Name**: {model_name}
- **Norm Type**: {norm_type}
- **Variant**: {variant}
- **Truncated Input**: [Truncated input is provided above.]
- **Generated Summary**: [Generated summary is provided above.]

**Please present your evaluation in the following structured format:**

```
Model Name: {model_name}

Relevance Score: [1-5]
Relevance Explanation: [Your explanation]

Conciseness Score: [1-5]
Conciseness Explanation: [Your explanation]

Fluency Score: [1-5]
Fluency Explanation: [Your explanation]

Accuracy Score: [1-5]
Accuracy Explanation: [Your explanation]

Coherence Score: [1-5]
Coherence Explanation: [Your explanation]

Overall Score: [Average score]
Overall Feedback: [Your feedback]

Comments on Columns:
[Your short paragraph commenting on each column]
```

---
**Truncated Input:**

{truncated_input}

---
**Generated Summary:**

{generated_summary}

---
"""
    # Call the OpenAI API using the ChatCompletion endpoint
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=700,  # As per your requirement
            temperature=0.0,  # For deterministic output
        )
        evaluation = response['choices'][0]['message']['content']
        return evaluation
    except openai.error.RateLimitError as e:
        print(f"Rate limit error: {e}")
        retry_after = int(e.headers.get("Retry-After", 5))
        print(f"Retrying after {retry_after} seconds...")
        time.sleep(retry_after)
        # Retry the request
        return get_gpt4_evaluation_summary(model_name, norm_type, variant, truncated_input, generated_summary)
    except Exception as e:
        print(f"Error during OpenAI API call: {e}")
        return None

def parse_evaluation_summary(evaluation_text):
    """
    Parses GPT-4's evaluation text and extracts scores, explanations, and comments.
    """
    patterns = {
        'Relevance Score': r'Relevance Score:\s*(\d)',
        'Relevance Explanation': r'Relevance Explanation:\s*(.*?)\n\n',
        'Conciseness Score': r'Conciseness Score:\s*(\d)',
        'Conciseness Explanation': r'Conciseness Explanation:\s*(.*?)\n\n',
        'Fluency Score': r'Fluency Score:\s*(\d)',
        'Fluency Explanation': r'Fluency Explanation:\s*(.*?)\n\n',
        'Accuracy Score': r'Accuracy Score:\s*(\d)',
        'Accuracy Explanation': r'Accuracy Explanation:\s*(.*?)\n\n',
        'Coherence Score': r'Coherence Score:\s*(\d)',
        'Coherence Explanation': r'Coherence Explanation:\s*(.*?)\n\n',
        'Overall Score': r'Overall Score:\s*([\d\.]+)',
        'Overall Feedback': r'Overall Feedback:\s*(.*?)\n\n',
        'Comments on Columns': r'Comments on Columns:\s*(.*)',  # Captures the paragraph
    }

    result = {}
    for key, pattern in patterns.items():
        match = re.search(pattern, evaluation_text, re.DOTALL)
        if match:
            result[key] = match.group(1).strip()
        else:
            result[key] = None
    return result

def evaluate_summaries():
    variants = ["baseModel", "noNorm", "AttnOnly", "FFNonly"]
    norm_types = ["LN", "RMSN"]

    for norm_type in norm_types:
        for variant in variants:
            filename = f"./modified_summ-source/{norm_type}_{variant}_evaluation_data_modified.csv"
            print(f"Processing file: {filename}")

            if not os.path.exists(filename):
                print(f"File {filename} does not exist. Skipping.")
                continue

            # Read the CSV file
            df = pd.read_csv(filename)
            # Limit to the first 25 rows to manage costs
            df_limited = df.head(25)
            evaluations = []

            for idx, row in tqdm(df_limited.iterrows(), total=df_limited.shape[0], desc=f"Evaluating Summaries for {filename}"):
                model_name = row['model_name']
                norm_type = row['norm_type']
                variant = row['variant']
                truncated_input = row['truncated_input']
                generated_summary = row['generated_summary']

                # Optionally truncate input if too long
                max_input_length = 1000  # Adjust as needed
                if len(truncated_input.split()) > max_input_length:
                    truncated_input = ' '.join(truncated_input.split()[:max_input_length]) + "..."

                # Get GPT-4 evaluation
                evaluation_text = get_gpt4_evaluation_summary(
                    model_name,
                    norm_type,
                    variant,
                    truncated_input,
                    generated_summary
                )

                if evaluation_text:
                    parsed = parse_evaluation_summary(evaluation_text)
                    parsed['model_name'] = model_name
                    parsed['norm_type'] = norm_type
                    parsed['variant'] = variant
                    evaluations.append(parsed)
                    time.sleep(1)  # To respect API rate limits
                else:
                    evaluations.append({
                        'model_name': model_name,
                        'norm_type': norm_type,
                        'variant': variant,
                        'Relevance Score': None,
                        'Relevance Explanation': None,
                        'Conciseness Score': None,
                        'Conciseness Explanation': None,
                        'Fluency Score': None,
                        'Fluency Explanation': None,
                        'Accuracy Score': None,
                        'Accuracy Explanation': None,
                        'Coherence Score': None,
                        'Coherence Explanation': None,
                        'Overall Score': None,
                        'Overall Feedback': None,
                        'Comments on Columns': 'Error or Empty Response'
                    })

            # Save evaluations to a new CSV file
            eval_df = pd.DataFrame(evaluations)
            parsed_eval_filename = f"{norm_type}_{variant}_gpt4_summary_parsed_evaluations.csv"
            eval_df.to_csv(parsed_eval_filename, index=False)
            print(f"Saved parsed evaluations to {parsed_eval_filename}")

if __name__ == "__main__":
    evaluate_summaries()
