Certainly! Let's proceed to automate the evaluation process using the OpenAI API for both your summarization and QA tasks. We'll write code that:

1. **Reads the data generated by your models.**
2. **Constructs the evaluation prompts as per the templates.**
3. **Uses the OpenAI API to get evaluations from GPT-4.**
4. **Parses the GPT-4 responses to extract scores and explanations.**
5. **Saves the evaluations into CSV files for analysis.**

---

## **Prerequisites**

Before we start, make sure you have:

- **OpenAI API Key:** Ensure you have your API key available. You can set it as an environment variable for security.
- **Python Libraries:** We'll need the `openai` library. Install it using:

  ```bash
  pip install openai
  ```

---

## **1. Setting Up the OpenAI API**

First, set up the OpenAI API in your Python script:

```python
import openai
import os

# Set your OpenAI API key
openai.api_key = os.getenv("OPENAI_API_KEY")
```

Make sure to set the `OPENAI_API_KEY` environment variable in your shell:

```bash
export OPENAI_API_KEY='your-api-key-here'
```

---

## **2. Automating the Evaluation for Summarization**

### **Reading the Generated Summaries**

We'll read the CSV files generated for each model and process them.

```python
import pandas as pd
import openai
import os
import time
from tqdm import tqdm

# Set your OpenAI API key
openai.api_key = os.getenv("OPENAI_API_KEY")
```

### **Function to Get GPT-4 Evaluation**

```python
def get_gpt4_evaluation_summary(model_name, truncated_input, truncated_reference_summary, generated_summary):
    # Construct the prompt
    prompt = f"""
As an expert evaluator, your task is to assess the quality of a generated summary based on the truncated input text and the truncated reference summary. Please focus on the truncated input when evaluating the generated summary. Rate the generated summary according to the following criteria, on a scale from 1 to 5 (where 1 is poor and 5 is excellent):

1. **Relevance**: Does the summary capture the main points of the **truncated input text**?
2. **Conciseness**: Is the summary succinct without unnecessary details?
3. **Fluency**: Is the summary well-written with correct grammar and style?
4. **Accuracy**: Does the summary accurately represent the content of the **truncated input text** without errors?
5. **Coherence**: Is the summary logically organized and easy to understand?

For each criterion, provide:

- **Score**: A number from 1 to 5.
- **Explanation**: A brief justification for the score.

After evaluating each criterion, provide:

- **Overall Score**: The average of the five scores.
- **Overall Feedback**: A short overall feedback.

Please present your evaluation in the following structured format:

```
Model Name: {model_name}

Relevance Score: [1-5]
Relevance Explanation: [Your explanation]

Conciseness Score: [1-5]
Conciseness Explanation: [Your explanation]

Fluency Score: [1-5]
Fluency Explanation: [Your explanation]

Accuracy Score: [1-5]
Accuracy Explanation: [Your explanation]

Coherence Score: [1-5]
Coherence Explanation: [Your explanation]

Overall Score: [Average score]
Overall Feedback: [Your feedback]
```

---

**Truncated Input Text:**

{truncated_input}

---

**Truncated Reference Summary:**

{truncated_reference_summary}

---

**Generated Summary:**

{generated_summary}

---
"""

    # Call the OpenAI API
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.0,
        )
        evaluation = response['choices'][0]['message']['content']
        return evaluation
    except Exception as e:
        print(f"Error during OpenAI API call: {e}")
        return None
```

### **Processing the Data and Getting Evaluations**

```python
def evaluate_summaries():
    variants = ["baseModel", "noNorm", "AttnOnly", "FFNonly"]
    norm_types = ["LN", "RMSN"]

    for norm_type in norm_types:
        for variant in variants:
            filename = f"{norm_type}_{variant}_evaluation_data.csv"
            print(f"Processing file: {filename}")

            if not os.path.exists(filename):
                print(f"File {filename} does not exist. Skipping.")
                continue

            df = pd.read_csv(filename)
            evaluations = []

            for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=f"Evaluating summaries for {filename}"):
                model_name = row['model_name']
                truncated_input = row['truncated_input']
                truncated_reference_summary = row['truncated_reference_summary']
                generated_summary = row['generated_summary']

                evaluation = get_gpt4_evaluation_summary(model_name, truncated_input, truncated_reference_summary, generated_summary)

                if evaluation:
                    evaluations.append({
                        'model_name': model_name,
                        'evaluation': evaluation
                    })
                    time.sleep(1)  # Respect rate limits
                else:
                    evaluations.append({
                        'model_name': model_name,
                        'evaluation': 'Error or Empty Response'
                    })

            # Save evaluations to a CSV file
            eval_df = pd.DataFrame(evaluations)
            eval_filename = f"{norm_type}_{variant}_gpt4_summary_evaluations.csv"
            eval_df.to_csv(eval_filename, index=False)
            print(f"Saved evaluations to {eval_filename}")
```

### **Parsing GPT-4 Responses**

We need to parse the GPT-4 responses to extract the scores and explanations.

```python
import re

def parse_evaluation(evaluation_text):
    # Define regex patterns
    patterns = {
        'Relevance Score': r'Relevance Score:\s*(\d)',
        'Relevance Explanation': r'Relevance Explanation:\s*(.*?)\n\n',
        'Conciseness Score': r'Conciseness Score:\s*(\d)',
        'Conciseness Explanation': r'Conciseness Explanation:\s*(.*?)\n\n',
        'Fluency Score': r'Fluency Score:\s*(\d)',
        'Fluency Explanation': r'Fluency Explanation:\s*(.*?)\n\n',
        'Accuracy Score': r'Accuracy Score:\s*(\d)',
        'Accuracy Explanation': r'Accuracy Explanation:\s*(.*?)\n\n',
        'Coherence Score': r'Coherence Score:\s*(\d)',
        'Coherence Explanation': r'Coherence Explanation:\s*(.*?)\n\n',
        'Overall Score': r'Overall Score:\s*([\d\.]+)',
        'Overall Feedback': r'Overall Feedback:\s*(.*)',
    }

    result = {}
    for key, pattern in patterns.items():
        match = re.search(pattern, evaluation_text, re.DOTALL)
        if match:
            result[key] = match.group(1).strip()
        else:
            result[key] = None
    return result
```

### **Processing and Saving Parsed Evaluations**

```python
def process_evaluations():
    variants = ["baseModel", "noNorm", "AttnOnly", "FFNonly"]
    norm_types = ["LN", "RMSN"]

    for norm_type in norm_types:
        for variant in variants:
            eval_filename = f"{norm_type}_{variant}_gpt4_summary_evaluations.csv"
            print(f"Processing evaluation file: {eval_filename}")

            if not os.path.exists(eval_filename):
                print(f"File {eval_filename} does not exist. Skipping.")
                continue

            eval_df = pd.read_csv(eval_filename)
            parsed_evaluations = []

            for idx, row in eval_df.iterrows():
                model_name = row['model_name']
                evaluation_text = row['evaluation']

                parsed = parse_evaluation(evaluation_text)
                parsed['model_name'] = model_name
                parsed_evaluations.append(parsed)

            # Save parsed evaluations to CSV
            parsed_df = pd.DataFrame(parsed_evaluations)
            parsed_eval_filename = f"{norm_type}_{variant}_gpt4_summary_parsed_evaluations.csv"
            parsed_df.to_csv(parsed_eval_filename, index=False)
            print(f"Saved parsed evaluations to {parsed_eval_filename}")
```

### **Putting It All Together**

Add the following at the end of your script:

```python
if __name__ == "__main__":
    evaluate_summaries()
    process_evaluations()
```

---

## **3. Automating the Evaluation for Question Answering**

### **Function to Get GPT-4 Evaluation for QA**

```python
def get_gpt4_evaluation_qa(model_name, question, context, reference_answers, generated_answer):
    # Construct the prompt
    prompt = f"""
As an expert evaluator, your task is to assess the quality of an answer generated by a question-answering system based on the provided context. Please focus on the following criteria:

1. **Correctness**: Is the answer correct based on the context?
2. **Completeness**: Does the answer fully address the question?
3. **Relevance**: Is the answer relevant to the question and context?
4. **Fluency**: Is the answer well-written with proper grammar and style?
5. **Conciseness**: Is the answer concise and to the point?

For each criterion, provide:

- **Score**: A number from 1 to 5 (where 1 is poor and 5 is excellent).
- **Explanation**: A brief justification for the score.

After evaluating each criterion, provide:

- **Overall Score**: The average of the five scores.
- **Overall Feedback**: A short overall feedback.

Please present your evaluation in the following structured format:

```
Model Name: {model_name}

Correctness Score: [1-5]
Correctness Explanation: [Your explanation]

Completeness Score: [1-5]
Completeness Explanation: [Your explanation]

Relevance Score: [1-5]
Relevance Explanation: [Your explanation]

Fluency Score: [1-5]
Fluency Explanation: [Your explanation]

Conciseness Score: [1-5]
Conciseness Explanation: [Your explanation]

Overall Score: [Average score]
Overall Feedback: [Your feedback]
```

---

**Model Name:**

{model_name}

---

**Question:**

{question}

---

**Context:**

{context}

---

**Reference Answers:**

{reference_answers}

---

**Generated Answer:**

{generated_answer}

---
"""

    # Call the OpenAI API
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.0,
        )
        evaluation = response['choices'][0]['message']['content']
        return evaluation
    except Exception as e:
        print(f"Error during OpenAI API call: {e}")
        return None
```

### **Processing the Data and Getting Evaluations**

```python
def evaluate_qa():
    variants = ["baseModel", "noNorm", "AttnOnly", "FFNonly"]
    norm_types = ["LN", "RMSN"]

    for norm_type in norm_types:
        for variant in variants:
            filename = f"{norm_type}_{variant}_gpt4_evaluation_data.csv"
            print(f"Processing file: {filename}")

            if not os.path.exists(filename):
                print(f"File {filename} does not exist. Skipping.")
                continue

            df = pd.read_csv(filename)
            evaluations = []

            for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=f"Evaluating QA for {filename}"):
                model_name = row['model_name']
                question = row['question']
                context = row['context']
                reference_answers = row['reference_answers']
                generated_answer = row['generated_answer']

                # Convert reference_answers from string representation to list
                if isinstance(reference_answers, str):
                    try:
                        reference_answers = eval(reference_answers)
                    except:
                        reference_answers = [reference_answers]

                # Truncate context if too long (optional)
                if len(context) > 2000:
                    context = context[:2000] + "..."

                evaluation = get_gpt4_evaluation_qa(model_name, question, context, reference_answers, generated_answer)

                if evaluation:
                    evaluations.append({
                        'model_name': model_name,
                        'evaluation': evaluation
                    })
                    time.sleep(1)  # Respect rate limits
                else:
                    evaluations.append({
                        'model_name': model_name,
                        'evaluation': 'Error or Empty Response'
                    })

            # Save evaluations to a CSV file
            eval_df = pd.DataFrame(evaluations)
            eval_filename = f"{norm_type}_{variant}_gpt4_qa_evaluations.csv"
            eval_df.to_csv(eval_filename, index=False)
            print(f"Saved evaluations to {eval_filename}")
```

### **Parsing GPT-4 Responses for QA**

```python
def parse_evaluation_qa(evaluation_text):
    # Define regex patterns
    patterns = {
        'Correctness Score': r'Correctness Score:\s*(\d)',
        'Correctness Explanation': r'Correctness Explanation:\s*(.*?)\n\n',
        'Completeness Score': r'Completeness Score:\s*(\d)',
        'Completeness Explanation': r'Completeness Explanation:\s*(.*?)\n\n',
        'Relevance Score': r'Relevance Score:\s*(\d)',
        'Relevance Explanation': r'Relevance Explanation:\s*(.*?)\n\n',
        'Fluency Score': r'Fluency Score:\s*(\d)',
        'Fluency Explanation': r'Fluency Explanation:\s*(.*?)\n\n',
        'Conciseness Score': r'Conciseness Score:\s*(\d)',
        'Conciseness Explanation': r'Conciseness Explanation:\s*(.*?)\n\n',
        'Overall Score': r'Overall Score:\s*([\d\.]+)',
        'Overall Feedback': r'Overall Feedback:\s*(.*)',
    }

    result = {}
    for key, pattern in patterns.items():
        match = re.search(pattern, evaluation_text, re.DOTALL)
        if match:
            result[key] = match.group(1).strip()
        else:
            result[key] = None
    return result
```

### **Processing and Saving Parsed Evaluations for QA**

```python
def process_evaluations_qa():
    variants = ["baseModel", "noNorm", "AttnOnly", "FFNonly"]
    norm_types = ["LN", "RMSN"]

    for norm_type in norm_types:
        for variant in variants:
            eval_filename = f"{norm_type}_{variant}_gpt4_qa_evaluations.csv"
            print(f"Processing evaluation file: {eval_filename}")

            if not os.path.exists(eval_filename):
                print(f"File {eval_filename} does not exist. Skipping.")
                continue

            eval_df = pd.read_csv(eval_filename)
            parsed_evaluations = []

            for idx, row in eval_df.iterrows():
                model_name = row['model_name']
                evaluation_text = row['evaluation']

                parsed = parse_evaluation_qa(evaluation_text)
                parsed['model_name'] = model_name
                parsed_evaluations.append(parsed)

            # Save parsed evaluations to CSV
            parsed_df = pd.DataFrame(parsed_evaluations)
            parsed_eval_filename = f"{norm_type}_{variant}_gpt4_qa_parsed_evaluations.csv"
            parsed_df.to_csv(parsed_eval_filename, index=False)
            print(f"Saved parsed evaluations to {parsed_eval_filename}")
```

### **Putting It All Together**

Add the following at the end of your script:

```python
if __name__ == "__main__":
    evaluate_qa()
    process_evaluations_qa()
```

---

## **4. Considerations and Best Practices**

### **Rate Limits and API Policies**

- **Rate Limits:** OpenAI's API has rate limits depending on your account. Be sure to respect them. The `time.sleep(1)` in the code is to introduce a delay between requests.
- **Token Usage:** Each API call consumes tokens. Be aware of the token costs to stay within your budget.
- **Error Handling:** The code includes try-except blocks to handle API errors gracefully.
- **Policy Compliance:** Ensure that your use of the API complies with OpenAI's policies.

### **Estimating Costs**

- **Token Counts:** Each request includes the prompt tokens and the response tokens. You can estimate the number of tokens per request to calculate the cost.
- **Calculating Total Cost:** Multiply the total tokens used by the price per 1,000 tokens for GPT-4.

### **Data Privacy**

- **Sensitive Data:** Ensure that the data you send to the API does not contain any sensitive or personal information.

### **Testing**

- **Start Small:** Begin with a small subset of your data (e.g., 5-10 examples) to ensure everything works as expected before scaling up.
- **Monitor Usage:** Keep an eye on your usage and costs as you process more data.

---

## **5. Final Remarks**

By automating the evaluation process using the OpenAI API, you can efficiently obtain detailed assessments of your models' outputs. The structured format allows for easy parsing and analysis.

If you have any questions or need further assistance with the code or any other aspect, feel free to ask!