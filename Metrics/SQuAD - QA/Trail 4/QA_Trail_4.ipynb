{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IQGvEeef_DT0"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets==2.21.0\n",
        "# !pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2ForQuestionAnswering, GPT2TokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "import wandb\n",
        "from transformers.integrations import WandbCallback\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_metric # used in compute_metrics\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "def freeze_layers(model, variant_type):\n",
        "    if variant_type == \"noNorm\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln\" in name:\n",
        "                param.requires_grad = False\n",
        "    elif variant_type == \"AttnOnly\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln_2\" in name:  # Freeze FFN layer norm\n",
        "                param.requires_grad = False\n",
        "    elif variant_type == \"FFOnly\":\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"ln_1\" in name:  # Freeze attention layer norm\n",
        "                param.requires_grad = False\n",
        "    # For baseModel, we don't freeze any layers\n",
        "\n",
        "def prepare_squad_dataset(tokenizer):\n",
        "    dataset = load_dataset(\"squad\")\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        questions = [q.strip() for q in examples[\"question\"]]\n",
        "        contexts = [c.strip() for c in examples[\"context\"]]\n",
        "\n",
        "        # Tokenize questions and contexts together\n",
        "        tokenized_examples = tokenizer(\n",
        "            questions,\n",
        "            contexts,\n",
        "            max_length=384,\n",
        "            truncation=\"only_second\",\n",
        "            stride=128,\n",
        "            return_overflowing_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            return_offsets_mapping=True,\n",
        "        )\n",
        "\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\", None)\n",
        "        if sample_mapping is None:\n",
        "            sample_mapping = list(range(len(questions)))\n",
        "\n",
        "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "        # help us compute the start_positions and end_positions.\n",
        "        offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "        # Let's label those examples!\n",
        "        tokenized_examples[\"start_positions\"] = []\n",
        "        tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "\n",
        "            # For GPT-2, we'll use the first token as our \"impossible answer\" token\n",
        "            impossible_answer_index = 0\n",
        "\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i) if hasattr(tokenized_examples, \"sequence_ids\") else None\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "            # If no answers are given, set the impossible_answer_index as answer.\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                tokenized_examples[\"start_positions\"].append(impossible_answer_index)\n",
        "                tokenized_examples[\"end_positions\"].append(impossible_answer_index)\n",
        "            else:\n",
        "                # Start/end character index of the answer in the text.\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                # Start token index of the current span in the text.\n",
        "                token_start_index = 0\n",
        "                while sequence_ids is not None and token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n",
        "                    token_start_index += 1\n",
        "\n",
        "                # End token index of the current span in the text.\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids is not None and token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                # Detect if the answer is out of the span (in which case this feature is labeled with the impossible_answer_index).\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    tokenized_examples[\"start_positions\"].append(impossible_answer_index)\n",
        "                    tokenized_examples[\"end_positions\"].append(impossible_answer_index)\n",
        "                else:\n",
        "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                    while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    tokenized_datasets = dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names,\n",
        "    )\n",
        "\n",
        "    return tokenized_datasets\n",
        "\n",
        "\n",
        "from datasets import load_metric\n",
        "from transformers import EvalPrediction\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    metric = load_metric(\"squad\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    start_logits, end_logits = logits\n",
        "\n",
        "    # Convert start and end logits to predictions\n",
        "    start_predictions = np.argmax(start_logits, axis=-1)\n",
        "    end_predictions = np.argmax(end_logits, axis=-1)\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Check if labels is empty or None\n",
        "    if labels is None or len(labels) == 0:\n",
        "        print(\"Warning: Labels are empty or None. Using dummy labels.\")\n",
        "        labels = [{\"id\": str(i), \"answers\": {\"text\": [\"\"], \"answer_start\": [0]}} for i in range(len(start_predictions))]\n",
        "\n",
        "    for i, (start_pred, end_pred) in enumerate(zip(start_predictions, end_predictions)):\n",
        "        # Get the predicted answer\n",
        "        pred_tokens = tokenizer.convert_ids_to_tokens(\n",
        "            range(start_pred, end_pred + 1)\n",
        "        )\n",
        "        pred_text = tokenizer.convert_tokens_to_string(pred_tokens)\n",
        "\n",
        "        # Get the actual answer\n",
        "        if isinstance(labels, (list, np.ndarray)) and i < len(labels):\n",
        "            example_labels = labels[i]\n",
        "            if isinstance(example_labels, dict):\n",
        "                true_text = example_labels.get('answers', {}).get('text', [''])[0]\n",
        "                true_start = example_labels.get('answers', {}).get('answer_start', [0])[0]\n",
        "            else:\n",
        "                true_text = \"\"\n",
        "                true_start = 0\n",
        "        else:\n",
        "            true_text = \"\"\n",
        "            true_start = 0\n",
        "\n",
        "        predictions.append({\n",
        "            \"id\": str(i),\n",
        "            \"prediction_text\": pred_text\n",
        "        })\n",
        "\n",
        "        references.append({\n",
        "            \"id\": str(i),\n",
        "            \"answers\": {\n",
        "                \"text\": [true_text],\n",
        "                \"answer_start\": [true_start]\n",
        "            }\n",
        "        })\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    return {\n",
        "        \"f1\": results[\"f1\"],\n",
        "        \"exact_match\": results[\"exact_match\"],\n",
        "    }\n",
        "\n",
        "\n",
        "def fine_tune_model(model, tokenizer, dataset, output_dir, variant, num_train_epochs=3):\n",
        "    # Initialize wandb run\n",
        "    wandb.init(project=f\"GPT-Valkyrie_LN-124m__{variant}__SQuAD\", reinit=True)\n",
        "    run_name = wandb.run.name\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=300,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_steps=200,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=run_name,\n",
        "    )\n",
        "\n",
        "    from transformers import DefaultDataCollator\n",
        "\n",
        "    # Use default_data_collator instead of a custom one\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DefaultDataCollator(return_tensors=\"pt\"),\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[WandbCallback()],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    wandb.finish()\n",
        "    return trainer.model, run_name"
      ],
      "metadata": {
        "id": "TfGhtPskBU_H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rH5wuIcA_QaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e03a6a-e4df-4bca-adab-b1df8ae01615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "# MAIN LOOP\n",
        "wandb.login()\n",
        "\n",
        "variants = [\"noNorm\", \"AttnOnly\", \"FFNonly\", \"baseModel\"]\n",
        "base_model_path = \"shng2025/GPT-Valkyrie_LN-124m__baseModel__\"  # Changed to LN model\n",
        "\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = prepare_squad_dataset(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for variant in variants:\n",
        "    print(f\"Processing {variant} model...\")\n",
        "\n",
        "    # Use the correct base model for each variant\n",
        "    model_path = f\"shng2025/GPT-Valkyrie_LN-124m__{variant}__\"\n",
        "    model = GPT2ForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "    freeze_layers(model, variant)\n",
        "\n",
        "    output_dir = f\"./results/{variant}\"\n",
        "    fine_tuned_model, run_name = fine_tune_model(model, tokenizer, dataset, output_dir, variant)\n",
        "\n",
        "    # Save the model locally\n",
        "    local_save_dir = f\"./local_models/GPT-Valkyrie_LN-124m__{variant}__SQuAD\"\n",
        "    fine_tuned_model.save_pretrained(local_save_dir)\n",
        "    tokenizer.save_pretrained(local_save_dir)\n",
        "    print(f\"Model saved locally to {local_save_dir}\")\n",
        "\n",
        "    # Push the model to your HuggingFace Hub repository\n",
        "    new_repo_name = f\"shng2025/GPT-Valkyrie_LN-124m__{variant}__SQuAD\"\n",
        "    fine_tuned_model.push_to_hub(new_repo_name, branch=run_name)\n",
        "    tokenizer.push_to_hub(new_repo_name, branch=run_name)\n",
        "    print(f\"Model pushed to HuggingFace Hub: {new_repo_name}, branch: {run_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "95bcf996da3e44edab5ce5429029c5eb",
            "d02592292dfd4e06b7eea7d833e8732c",
            "c9c3184e6c984dc69a98a1cde5f0e230",
            "ff1cef8f5efa474fb1f81a54eb21aff8",
            "18a2588bb5584d6cbe25dfd6e77b6cbf",
            "9a2812638f9c4c438f4b334653f7686a",
            "cdfff40ed0d44bc48b768c22244b9336",
            "5e010758c86c4c7ba63c164990dae7c1"
          ]
        },
        "id": "6uNhEQNWBk5P",
        "outputId": "e0f84913-67f3-4e39-f3c3-afed78ef52b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing noNorm model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at shng2025/GPT-Valkyrie_LN-124m__noNorm__ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:sy5tknst) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95bcf996da3e44edab5ce5429029c5eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lemon-sun-35</strong> at: <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/sy5tknst' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/sy5tknst</a><br/> View project at: <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240921_151136-sy5tknst/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:sy5tknst). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240921_151158-t76keguh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/t76keguh' target=\"_blank\">deep-butterfly-36</a></strong> to <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/t76keguh' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/GPT-Valkyrie_LN-124m__noNorm__SQuAD/runs/t76keguh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "WandbCallback\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='401' max='33189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  401/33189 08:30 < 11:39:07, 0.78 it/s, Epoch 0.04/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Exact Match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.021700</td>\n",
              "      <td>4.628865</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43.155259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.711100</td>\n",
              "      <td>4.411581</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>53.153404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.330000</td>\n",
              "      <td>4.343086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.163050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='992' max='1348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 992/1348 01:30 < 00:32, 10.90 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-cc8b5ee2a710>:125: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"squad\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'exact_match': 43.15525876460768, 'f1': 0.0}\n",
            "{'exact_match': 53.15340382118345, 'f1': 0.0}\n",
            "{'exact_match': 31.163049526989425, 'f1': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKwmxfrbBk2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXWgYLDABk0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95bcf996da3e44edab5ce5429029c5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d02592292dfd4e06b7eea7d833e8732c",
              "IPY_MODEL_c9c3184e6c984dc69a98a1cde5f0e230"
            ],
            "layout": "IPY_MODEL_ff1cef8f5efa474fb1f81a54eb21aff8"
          }
        },
        "d02592292dfd4e06b7eea7d833e8732c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18a2588bb5584d6cbe25dfd6e77b6cbf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a2812638f9c4c438f4b334653f7686a",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "c9c3184e6c984dc69a98a1cde5f0e230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdfff40ed0d44bc48b768c22244b9336",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e010758c86c4c7ba63c164990dae7c1",
            "value": 1
          }
        },
        "ff1cef8f5efa474fb1f81a54eb21aff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a2588bb5584d6cbe25dfd6e77b6cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2812638f9c4c438f4b334653f7686a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdfff40ed0d44bc48b768c22244b9336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e010758c86c4c7ba63c164990dae7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}