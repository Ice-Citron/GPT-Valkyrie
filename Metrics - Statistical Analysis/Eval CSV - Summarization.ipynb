{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOVRgo2c5udqClHN+IAHhew"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# BillSum"],"metadata":{"id":"j-NzkII8GmEH"}},{"cell_type":"code","source":["!pip install pingouin\n","!pip install scikit_posthocs\n","!pip install krippendorff"],"metadata":{"id":"zDYrJ98e4w2-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729417802160,"user_tz":-480,"elapsed":8223,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"f63b3691-a5bc-49b6-ebe1-5bebf73e7bef"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pingouin\n","  Downloading pingouin-0.5.5-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pingouin) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pingouin) (1.26.4)\n","Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from pingouin) (2.2.2)\n","Collecting pandas-flavor (from pingouin)\n","  Downloading pandas_flavor-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.10/dist-packages (from pingouin) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pingouin) (1.13.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from pingouin) (0.13.2)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from pingouin) (0.14.4)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from pingouin) (0.9.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->pingouin) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->pingouin) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->pingouin) (2024.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2->pingouin) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2->pingouin) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pingouin) (3.2.0)\n","Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas-flavor->pingouin) (2024.9.0)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->pingouin) (0.5.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->pingouin) (1.16.0)\n","Downloading pingouin-0.5.5-py3-none-any.whl (204 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.4/204.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas_flavor-0.6.0-py3-none-any.whl (7.2 kB)\n","Installing collected packages: pandas-flavor, pingouin\n","Successfully installed pandas-flavor-0.6.0 pingouin-0.5.5\n","Collecting scikit_posthocs\n","  Downloading scikit_posthocs-0.9.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (1.26.4)\n","Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (1.13.1)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (0.14.4)\n","Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (2.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (0.13.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (3.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2024.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (3.2.0)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->scikit_posthocs) (0.5.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->scikit_posthocs) (1.16.0)\n","Downloading scikit_posthocs-0.9.1-py3-none-any.whl (32 kB)\n","Installing collected packages: scikit_posthocs\n","Successfully installed scikit_posthocs-0.9.1\n","Collecting krippendorff\n","  Downloading krippendorff-0.8.0-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from krippendorff) (1.26.4)\n","Downloading krippendorff-0.8.0-py3-none-any.whl (18 kB)\n","Installing collected packages: krippendorff\n","Successfully installed krippendorff-0.8.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","def process_csv(eval_file, gen_file):\n","    eval_df = pd.read_csv(eval_file)\n","    gen_df = pd.read_csv(gen_file)\n","\n","    # Ensure the dataframes have the same number of rows\n","    min_rows = min(len(eval_df), len(gen_df))\n","    eval_df = eval_df.iloc[:min_rows]\n","    gen_df = gen_df.iloc[:min_rows]\n","\n","    # Merge the dataframes\n","    merged_df = pd.merge(eval_df, gen_df[['truncated_input', 'generated_summary']], left_index=True, right_index=True)\n","\n","    # Add model_name, norm_type, and variant columns from gen_df\n","    merged_df['model_name'] = gen_df['model_name']\n","    merged_df['norm_type'] = gen_df['norm_type']\n","    merged_df['variant'] = gen_df['variant']\n","\n","    return merged_df\n","\n","def main():\n","    evaluation_files = [\n","        'LN_AttnOnly_gpt4_summary_parsed_evaluations.csv',\n","        'LN_FFNonly_gpt4_summary_parsed_evaluations.csv',\n","        'LN_baseModel_gpt4_summary_parsed_evaluations.csv',\n","        'LN_noNorm_gpt4_summary_parsed_evaluations.csv',\n","        'RMSN_AttnOnly_gpt4_summary_parsed_evaluations.csv',\n","        'RMSN_FFNonly_gpt4_summary_parsed_evaluations.csv',\n","        'RMSN_baseModel_gpt4_summary_parsed_evaluations.csv',\n","        'RMSN_noNorm_gpt4_summary_parsed_evaluations.csv'\n","    ]\n","\n","    generated_summary_files = [\n","        'LN_AttnOnly_evaluation_data_modified.csv',\n","        'LN_FFNonly_evaluation_data_modified.csv',\n","        'LN_baseModel_evaluation_data_modified.csv',\n","        'LN_noNorm_evaluation_data_modified.csv',\n","        'RMSN_AttnOnly_evaluation_data_modified.csv',\n","        'RMSN_FFNonly_evaluation_data_modified.csv',\n","        'RMSN_baseModel_evaluation_data_modified.csv',\n","        'RMSN_noNorm_evaluation_data_modified.csv'\n","    ]\n","\n","    all_data = pd.concat([process_csv(eval_file, gen_file)\n","                          for eval_file, gen_file in zip(evaluation_files, generated_summary_files)],\n","                         ignore_index=True)\n","\n","    columns_order = ['model_name', 'norm_type', 'variant', 'truncated_input', 'generated_summary',\n","                     'Overall Score', 'Relevance Score', 'Conciseness Score', 'Fluency Score',\n","                     'Accuracy Score', 'Coherence Score', 'Overall Feedback', 'Comments on Columns']\n","\n","    all_data = all_data[columns_order]\n","\n","    # Truncate generated_summary to 1000 characters\n","    all_data['generated_summary'] = all_data['generated_summary'].str[:1000]\n","\n","    # Save the entire dataframe to a single CSV file\n","    output_file = 'combined_data_truncated.csv'\n","    all_data.to_csv(output_file, index=False)\n","    print(f\"All data has been saved to '{output_file}' with generated summary truncated to 1000 characters.\")\n","\n","    print(f\"The file contains {len(all_data)} rows and {len(all_data.columns)} columns.\")\n","    print(\"Columns:\", ', '.join(all_data.columns))\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xi7TKpuFBDw3","executionInfo":{"status":"ok","timestamp":1729417803729,"user_tz":-480,"elapsed":1571,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"f6ea7602-fc12-45b9-ed92-f71a35a8ccbb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["All data has been saved to 'combined_data_truncated.csv' with generated summary truncated to 1000 characters.\n","The file contains 200 rows and 13 columns.\n","Columns: model_name, norm_type, variant, truncated_input, generated_summary, Overall Score, Relevance Score, Conciseness Score, Fluency Score, Accuracy Score, Coherence Score, Overall Feedback, Comments on Columns\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","import os\n","\n","np.random.seed(42)\n","\n","def load_data(file_path):\n","    df = pd.read_csv(file_path)\n","    print(f\"Loaded {len(df)} rows of data\")\n","    print(f\"Columns: {', '.join(df.columns)}\")\n","    return df\n","\n","def calculate_statistics(df):\n","    score_columns = ['Overall Score', 'Relevance Score', 'Conciseness Score',\n","                     'Fluency Score', 'Accuracy Score', 'Coherence Score']\n","    return df.groupby(['model_name', 'norm_type', 'variant'])[score_columns].agg(['mean', 'std', 'min', 'max'])\n","\n","def perform_anova(df, score_column):\n","    models = df['model_name'].unique()\n","    data = [df[df['model_name'] == model][score_column] for model in models]\n","    f_value, p_value = stats.f_oneway(*data)\n","    return f_value, p_value\n","\n","def create_collated_boxplots(df, output_dir):\n","    score_columns = ['Overall Score', 'Relevance Score', 'Conciseness Score',\n","                     'Fluency Score', 'Accuracy Score', 'Coherence Score']\n","\n","    plt.figure(figsize=(20, 15))\n","    for i, score in enumerate(score_columns, 1):\n","        plt.subplot(2, 3, i)\n","        sns.boxplot(x='model_name', y=score, hue='norm_type', data=df)\n","        plt.title(score)\n","        plt.xticks(rotation=45, ha='right')\n","        plt.xlabel('')\n","        if i % 3 != 1:\n","            plt.ylabel('')\n","        if i == 3:\n","            plt.legend(title='Norm Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n","        else:\n","            plt.legend([])\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, 'summarization_collated_boxplots.png'), bbox_inches='tight')\n","    plt.close()\n","\n","def main():\n","    output_dir = 'analysis_results'\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    df = load_data('combined_data_truncated.csv')\n","\n","    stats_df = calculate_statistics(df)\n","    stats_df.to_csv(os.path.join(output_dir, 'descriptive_statistics.csv'))\n","\n","    create_collated_boxplots(df, output_dir)\n","\n","    score_columns = ['Overall Score', 'Relevance Score', 'Conciseness Score',\n","                     'Fluency Score', 'Accuracy Score', 'Coherence Score']\n","    anova_results = []\n","    for score in score_columns:\n","        f_value, p_value = perform_anova(df, score)\n","        anova_results.append({\n","            'Metric': score,\n","            'F-value': f_value,\n","            'p-value': p_value\n","        })\n","\n","    anova_df = pd.DataFrame(anova_results)\n","    anova_df.to_csv(os.path.join(output_dir, 'summarization_anova_results.csv'), index=False)\n","\n","    print(\"Analysis complete. Results saved in 'analysis_results' directory.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Je2IhPlcAd3A","executionInfo":{"status":"ok","timestamp":1729417806709,"user_tz":-480,"elapsed":2983,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"ce56eeab-d17b-4693-aac4-6c825dc56193"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 200 rows of data\n","Columns: model_name, norm_type, variant, truncated_input, generated_summary, Overall Score, Relevance Score, Conciseness Score, Fluency Score, Accuracy Score, Coherence Score, Overall Feedback, Comments on Columns\n","Analysis complete. Results saved in 'analysis_results' directory.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","from statsmodels.stats.multicomp import pairwise_tukeyhsd\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from scipy.stats import kruskal\n","from scikit_posthocs import posthoc_dunn\n","import warnings\n","import os\n","\n","warnings.filterwarnings('ignore')\n","\n","# Create a directory for CSV outputs\n","output_dir = 'billsum_analysis_results'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Load the data\n","df = pd.read_csv('combined_data_truncated.csv')\n","\n","print(\"Columns in the CSV file:\")\n","print(df.columns)\n","\n","# Ensure 'model_name' is treated as a categorical variable\n","df['model_name'] = df['model_name'].astype('category')\n","\n","# Define score columns\n","score_columns = ['Overall Score', 'Relevance Score', 'Conciseness Score', 'Fluency Score', 'Accuracy Score', 'Coherence Score']\n","\n","print(f\"\\nUsing score columns: {score_columns}\")\n","\n","# 1. Effect Size Calculation\n","def calculate_effect_size(df, metric):\n","    try:\n","        f_value, _ = stats.f_oneway(*[group[metric] for name, group in df.groupby('model_name')])\n","        return f_value / (f_value + df.groupby('model_name').size().iloc[0] - 1)\n","    except Exception as e:\n","        print(f\"Error calculating effect size for {metric}: {e}\")\n","        return np.nan\n","\n","effect_sizes = {metric: calculate_effect_size(df, metric) for metric in score_columns}\n","effect_sizes_df = pd.DataFrame.from_dict(effect_sizes, orient='index', columns=['Effect Size'])\n","effect_sizes_df.to_csv(os.path.join(output_dir, 'effect_sizes.csv'))\n","\n","# 2. Post-hoc Tests\n","def perform_tukey_hsd(df, metric):\n","    try:\n","        tukey = pairwise_tukeyhsd(df[metric], df['model_name'])\n","        return pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n","    except Exception as e:\n","        print(f\"Error performing Tukey HSD for {metric}: {e}\")\n","        return pd.DataFrame()\n","\n","tukey_results = {metric: perform_tukey_hsd(df, metric) for metric in score_columns}\n","for metric, result in tukey_results.items():\n","    result.to_csv(os.path.join(output_dir, f'tukey_hsd_{metric.replace(\" \", \"_\")}.csv'), index=False)\n","\n","# 3. Correlation Analysis\n","correlation_matrix = df[score_columns].corr()\n","correlation_matrix.to_csv(os.path.join(output_dir, 'correlation_matrix.csv'))\n","\n","# 4. Principal Component Analysis\n","scaler = StandardScaler()\n","pca = PCA()\n","pca_result = pca.fit_transform(scaler.fit_transform(df[score_columns]))\n","pca_df = pd.DataFrame({\n","    'Principal Component': range(1, len(pca.explained_variance_ratio_) + 1),\n","    'Explained Variance Ratio': pca.explained_variance_ratio_,\n","    'Cumulative Explained Variance Ratio': np.cumsum(pca.explained_variance_ratio_)\n","})\n","pca_df.to_csv(os.path.join(output_dir, 'pca_results.csv'), index=False)\n","\n","# 5. Non-parametric Tests\n","def perform_kruskal_dunn(df, metric):\n","    try:\n","        kruskal_result = kruskal(*[group[metric].values for name, group in df.groupby('model_name')])\n","        dunn_result = posthoc_dunn(df, val_col=metric, group_col='model_name', p_adjust='bonferroni')\n","        kruskal_df = pd.DataFrame({'statistic': [kruskal_result.statistic], 'p-value': [kruskal_result.pvalue]})\n","        return kruskal_df, dunn_result\n","    except Exception as e:\n","        print(f\"Error performing Kruskal-Wallis and Dunn's test for {metric}: {e}\")\n","        return None, None\n","\n","# Use the function and save results\n","kruskal_dunn_results = {metric: perform_kruskal_dunn(df, metric) for metric in score_columns}\n","for metric, (kruskal_result, dunn_result) in kruskal_dunn_results.items():\n","    if kruskal_result is not None:\n","        kruskal_result.to_csv(os.path.join(output_dir, f'kruskal_wallis_{metric.replace(\" \", \"_\")}.csv'), index=False)\n","    if dunn_result is not None:\n","        dunn_result.to_csv(os.path.join(output_dir, f'dunn_test_{metric.replace(\" \", \"_\")}.csv'))\n","\n","# 6. Visualizations (unchanged)\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n","plt.title('Correlation Heatmap of Metrics')\n","plt.tight_layout()\n","plt.savefig(os.path.join(output_dir, 'correlation_heatmap.png'))\n","plt.close()\n","\n","# Violin Plots\n","plt.figure(figsize=(20, 15))\n","for i, metric in enumerate(score_columns, 1):\n","    plt.subplot(2, 3, i)\n","    sns.violinplot(x='model_name', y=metric, data=df)\n","    plt.title(metric)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.xlabel('')\n","    if i % 3 != 1:\n","        plt.ylabel('')\n","plt.tight_layout()\n","plt.savefig(os.path.join(output_dir, 'violin_plots.png'), bbox_inches='tight')\n","plt.close()\n","\n","# Radar Chart\n","def radar_chart(df, metrics):\n","    means = df.groupby('model_name')[metrics].mean()\n","    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n","    means = pd.concat([means, means.iloc[:, :1]], axis=1)\n","    angles = np.concatenate((angles, [angles[0]]))\n","\n","    fig, ax = plt.subplots(figsize=(14, 10), subplot_kw=dict(projection='polar'))\n","    for model in means.index:\n","        values = means.loc[model].values\n","        ax.plot(angles, values, 'o-', linewidth=2, label=model)\n","        ax.fill(angles, values, alpha=0.25)\n","    ax.set_thetagrids(angles[:-1] * 180/np.pi, metrics)\n","    ax.set_ylim(0, 5)  # Assuming scores are between 0 and 5\n","    plt.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n","    plt.title(\"Model Performance Across Metrics\")\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, 'radar_chart.png'), bbox_inches='tight')\n","    plt.close()\n","\n","radar_chart(df, score_columns)\n","\n","# Distribution of Scores\n","plt.figure(figsize=(20, 15))\n","for i, metric in enumerate(score_columns, 1):\n","    plt.subplot(2, 3, i)\n","    for model in df['model_name'].unique():\n","        sns.kdeplot(data=df[df['model_name'] == model], x=metric, label=model)\n","    plt.title(f'Distribution of {metric}')\n","    plt.xlabel('Score')\n","    plt.ylabel('Density')\n","    if i == 3:  # Place legend outside the plots\n","        plt.legend(title='Model Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n","    else:\n","        plt.legend([])  # Remove individual legends\n","plt.tight_layout()\n","plt.savefig(os.path.join(output_dir, 'score_distributions.png'), bbox_inches='tight')\n","plt.close()\n","\n","# Boxplots\n","plt.figure(figsize=(20, 15))\n","for i, metric in enumerate(score_columns, 1):\n","    plt.subplot(2, 3, i)\n","    sns.boxplot(x='model_name', y=metric, data=df)\n","    plt.title(metric)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.xlabel('')\n","    if i % 3 != 1:\n","        plt.ylabel('')\n","plt.tight_layout()\n","plt.savefig(os.path.join(output_dir, 'boxplots.png'), bbox_inches='tight')\n","plt.close()\n","\n","# 7. Basic Descriptive Statistics\n","descriptive_stats = df.groupby('model_name')[score_columns].agg(['mean', 'std', 'min', 'max'])\n","descriptive_stats.to_csv(os.path.join(output_dir, 'descriptive_statistics.csv'))\n","\n","print(f\"\\nAnalysis complete. Results saved in CSV files in the '{output_dir}' directory.\")\n","print(\"Check the generated PNG files for visualizations.\")"],"metadata":{"id":"dBUdFQrAAd00","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729430745657,"user_tz":-480,"elapsed":9419,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"caa1f7fb-171f-43a7-c9c8-80336b6220f5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns in the CSV file:\n","Index(['model_name', 'norm_type', 'variant', 'truncated_input',\n","       'generated_summary', 'Overall Score', 'Relevance Score',\n","       'Conciseness Score', 'Fluency Score', 'Accuracy Score',\n","       'Coherence Score', 'Overall Feedback', 'Comments on Columns'],\n","      dtype='object')\n","\n","Using score columns: ['Overall Score', 'Relevance Score', 'Conciseness Score', 'Fluency Score', 'Accuracy Score', 'Coherence Score']\n","\n","Analysis complete. Results saved in CSV files in the 'billsum_analysis_results' directory.\n","Check the generated PNG files for visualizations.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kewxQHqpMP9P","executionInfo":{"status":"ok","timestamp":1729417817488,"user_tz":-480,"elapsed":3,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Mean Calculation"],"metadata":{"id":"6-4FBI-OVquF"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","def analyze_csv(file_path):\n","    # Read CSV file\n","    df = pd.read_csv(file_path)\n","\n","    # Extract model name from file name\n","    model_name = os.path.basename(file_path).split('_evaluations.csv')[0]\n","\n","    # Calculate statistics\n","    score_columns = ['Relevance Score', 'Conciseness Score', 'Fluency Score', 'Accuracy Score', 'Coherence Score', 'Overall Score']\n","    stats = df[score_columns].agg(['mean', 'median', 'min', 'max'])\n","\n","    # Create distribution plot\n","    plt.figure(figsize=(12, 6))\n","    for column in score_columns:\n","        sns.kdeplot(df[column], shade=True, label=column)\n","\n","    plt.title(f'Score Distribution for {model_name}')\n","    plt.xlabel('Score')\n","    plt.ylabel('Density')\n","    plt.legend()\n","    plt.savefig(f'{model_name}_distribution.png')\n","    plt.close()\n","\n","    # Create bar plot\n","    plt.figure(figsize=(12, 6))\n","    stats.loc['mean'].plot(kind='bar')\n","    plt.title(f'Mean Scores for {model_name}')\n","    plt.xlabel('Criteria')\n","    plt.ylabel('Mean Score')\n","    plt.savefig(f'{model_name}_mean_scores.png')\n","    plt.close()\n","\n","    return stats\n","\n","# List of CSV files\n","csv_files = [\n","    'LN_AttnOnly_gpt4_summary_parsed_evaluations.csv',\n","    'LN_FFNonly_gpt4_summary_parsed_evaluations.csv',\n","    'LN_baseModel_gpt4_summary_parsed_evaluations.csv',\n","    'LN_noNorm_gpt4_summary_parsed_evaluations.csv',\n","    'RMSN_AttnOnly_gpt4_summary_parsed_evaluations.csv',\n","    'RMSN_FFNonly_gpt4_summary_parsed_evaluations.csv',\n","    'RMSN_baseModel_gpt4_summary_parsed_evaluations.csv',\n","    'RMSN_noNorm_gpt4_summary_parsed_evaluations.csv'\n","]\n","\n","# Analyze each CSV file\n","all_stats = {}\n","for file in csv_files:\n","    print(f\"Analyzing {file}...\")\n","    stats = analyze_csv(file)\n","    all_stats[file] = stats\n","    print(f\"Statistics for {file}:\")\n","    print(stats)\n","    print(\"\\n\")\n","\n","# Create a comparative bar plot for mean scores across all models\n","plt.figure(figsize=(15, 8))\n","mean_scores = pd.DataFrame({model: stats.loc['mean'] for model, stats in all_stats.items()})\n","mean_scores.plot(kind='bar', figsize=(15, 8))\n","plt.title('Comparison of Mean Scores Across All Models')\n","plt.xlabel('Criteria')\n","plt.ylabel('Mean Score')\n","plt.legend(title='Models', bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.tight_layout()\n","plt.savefig('all_models_comparison.png')\n","plt.close()\n","\n","print(\"Analysis complete. Check the generated PNG files for visualizations.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"etAf-8zAVqcq","executionInfo":{"status":"ok","timestamp":1729430589524,"user_tz":-480,"elapsed":4729,"user":{"displayName":"Shi Hao Ng","userId":"16424048296553416474"}},"outputId":"f9866db2-d085-4920-96b5-9cbcf678e75e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Analyzing LN_AttnOnly_gpt4_summary_parsed_evaluations.csv...\n","Statistics for LN_AttnOnly_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean               1.28               3.04           3.96            1.08   \n","median             1.00               3.00           4.00            1.00   \n","min                1.00               3.00           3.00            1.00   \n","max                2.00               4.00           4.00            2.00   \n","\n","        Coherence Score  Overall Score  \n","mean               2.96          2.464  \n","median             3.00          2.400  \n","min                2.00          2.000  \n","max                4.00          2.800  \n","\n","\n","Analyzing LN_FFNonly_gpt4_summary_parsed_evaluations.csv...\n","Statistics for LN_FFNonly_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean               1.08               2.96           3.96            1.04   \n","median             1.00               3.00           4.00            1.00   \n","min                1.00               2.00           3.00            1.00   \n","max                2.00               3.00           4.00            2.00   \n","\n","        Coherence Score  Overall Score  \n","mean               3.16           2.44  \n","median             3.00           2.40  \n","min                2.00           1.80  \n","max                4.00           2.80  \n","\n","\n","Analyzing LN_baseModel_gpt4_summary_parsed_evaluations.csv...\n","Statistics for LN_baseModel_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean               1.16               3.04           3.96            1.04   \n","median             1.00               3.00           4.00            1.00   \n","min                1.00               3.00           3.00            1.00   \n","max                2.00               4.00           4.00            2.00   \n","\n","        Coherence Score  Overall Score  \n","mean               3.04          2.448  \n","median             3.00          2.400  \n","min                2.00          2.200  \n","max                4.00          2.800  \n","\n","\n","Analyzing LN_noNorm_gpt4_summary_parsed_evaluations.csv...\n","Statistics for LN_noNorm_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean               1.16               3.04           4.04            1.08   \n","median             1.00               3.00           4.00            1.00   \n","min                1.00               3.00           4.00            1.00   \n","max                2.00               4.00           5.00            2.00   \n","\n","        Coherence Score  Overall Score  \n","mean               3.08           2.48  \n","median             3.00           2.40  \n","min                2.00           2.20  \n","max                4.00           3.40  \n","\n","\n","Analyzing RMSN_AttnOnly_gpt4_summary_parsed_evaluations.csv...\n","Statistics for RMSN_AttnOnly_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean               1.44               3.12           3.76             1.2   \n","median             1.00               3.00           4.00             1.0   \n","min                1.00               3.00           2.00             1.0   \n","max                2.00               4.00           4.00             2.0   \n","\n","        Coherence Score  Overall Score  \n","mean                3.0          2.504  \n","median              3.0          2.600  \n","min                 2.0          1.800  \n","max                 4.0          3.000  \n","\n","\n","Analyzing RMSN_FFNonly_gpt4_summary_parsed_evaluations.csv...\n","Statistics for RMSN_FFNonly_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean                1.2               3.08           3.84            1.08   \n","median              1.0               3.00           4.00            1.00   \n","min                 1.0               3.00           2.00            1.00   \n","max                 3.0               4.00           4.00            2.00   \n","\n","        Coherence Score  Overall Score  \n","mean               2.96          2.432  \n","median             3.00          2.400  \n","min                2.00          1.800  \n","max                4.00          3.000  \n","\n","\n","Analyzing RMSN_baseModel_gpt4_summary_parsed_evaluations.csv...\n","Statistics for RMSN_baseModel_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean               1.28                3.0           3.76            1.04   \n","median             1.00                3.0           4.00            1.00   \n","min                1.00                3.0           2.00            1.00   \n","max                2.00                3.0           4.00            2.00   \n","\n","        Coherence Score  Overall Score  \n","mean               2.92            2.4  \n","median             3.00            2.4  \n","min                2.00            1.8  \n","max                4.00            3.0  \n","\n","\n","Analyzing RMSN_noNorm_gpt4_summary_parsed_evaluations.csv...\n","Statistics for RMSN_noNorm_gpt4_summary_parsed_evaluations.csv:\n","        Relevance Score  Conciseness Score  Fluency Score  Accuracy Score  \\\n","mean                1.4               3.08           3.84             1.2   \n","median              1.0               3.00           4.00             1.0   \n","min                 1.0               3.00           3.00             1.0   \n","max                 3.0               4.00           4.00             2.0   \n","\n","        Coherence Score  Overall Score  \n","mean               2.88           2.48  \n","median             3.00           2.40  \n","min                2.00           2.00  \n","max                3.00           3.20  \n","\n","\n","Analysis complete. Check the generated PNG files for visualizations.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1500x800 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"0srahpBlWm86"},"execution_count":null,"outputs":[]}]}