{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets tqdm pandas\n",
        "!pip install evaluate\n",
        "!pip install scikit_posthocs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjoVnjkSEdsk",
        "outputId": "e167696a-3c89-415c-9397-43ec6ecaad80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Collecting scikit_posthocs\n",
            "  Downloading scikit_posthocs-0.10.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (1.13.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (0.14.4)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from scikit_posthocs) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->scikit_posthocs) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->scikit_posthocs) (3.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->scikit_posthocs) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels->scikit_posthocs) (1.16.0)\n",
            "Downloading scikit_posthocs-0.10.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: scikit_posthocs\n",
            "Successfully installed scikit_posthocs-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAg3xMBZEJMc",
        "outputId": "720e9935-a67b-4a60-cbe9-3b502dc2ae7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_LN-124m__baseModel__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_LN-124m__baseModel__SQuAD: 100%|██████████| 100/100 [00:02<00:00, 42.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_LN-124m__noNorm__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_LN-124m__noNorm__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 65.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_LN-124m__AttnOnly__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_LN-124m__AttnOnly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 78.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_LN-124m__FFNonly__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_LN-124m__FFNonly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 78.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_RMSN-124m__baseModel__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 66.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_RMSN-124m__noNorm__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 73.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_RMSN-124m__AttnOnly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 77.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating shng2025/GPT-Valkyrie_RMSN-124m__FFNonly__SQuAD: 100%|██████████| 100/100 [00:01<00:00, 79.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed evaluation results saved to squad_evaluation_detailed.csv\n",
            "Summary evaluation results saved to squad_evaluation_summary.csv\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as load_metric\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def compute_exact_match(prediction, ground_truth):\n",
        "    \"\"\"Compute exact match for a single prediction.\"\"\"\n",
        "    return int(prediction.lower() == ground_truth.lower())\n",
        "\n",
        "def compute_f1(prediction, ground_truth):\n",
        "    \"\"\"Compute F1 score for a single prediction.\"\"\"\n",
        "    prediction_tokens = prediction.lower().split()\n",
        "    ground_truth_tokens = ground_truth.lower().split()\n",
        "\n",
        "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
        "        return int(prediction_tokens == ground_truth_tokens)\n",
        "\n",
        "    common_tokens = set(prediction_tokens) & set(ground_truth_tokens)\n",
        "    if not common_tokens:\n",
        "        return 0\n",
        "\n",
        "    precision = len(common_tokens) / len(prediction_tokens)\n",
        "    recall = len(common_tokens) / len(ground_truth_tokens)\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def evaluate_model(model_name, dataset, max_samples=100):\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    detailed_results = []\n",
        "\n",
        "    for i, example in enumerate(tqdm(dataset, desc=f\"Evaluating {model_name}\")):\n",
        "        if i >= max_samples:\n",
        "            break\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        start_logits = outputs.start_logits[0].cpu().numpy()\n",
        "        end_logits = outputs.end_logits[0].cpu().numpy()\n",
        "\n",
        "        start_index = np.argmax(start_logits)\n",
        "        end_index = np.argmax(end_logits)\n",
        "\n",
        "        no_answer_score = start_logits[0] + end_logits[0]\n",
        "        best_answer_score = start_logits[start_index] + end_logits[end_index]\n",
        "\n",
        "        if no_answer_score > best_answer_score or end_index < start_index:\n",
        "            answer = \"\"\n",
        "        else:\n",
        "            answer_tokens = inputs[\"input_ids\"][0][start_index:end_index+1]\n",
        "            answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Get the ground truth answer\n",
        "        ground_truth = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"\"\n",
        "\n",
        "        # Compute exact match and F1 scores for this example\n",
        "        exact_match = compute_exact_match(answer, ground_truth)\n",
        "        f1_score = compute_f1(answer, ground_truth)\n",
        "\n",
        "        # Store detailed result for this example\n",
        "        detailed_result = {\n",
        "            \"model_name\": model_name,\n",
        "            \"example_id\": example[\"id\"],\n",
        "            \"question\": example[\"question\"],\n",
        "            \"context\": example[\"context\"],\n",
        "            \"reference_answer\": ground_truth,\n",
        "            \"predicted_answer\": answer,\n",
        "            \"exact_match\": exact_match,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"confidence_score\": float(1 / (1 + np.exp(no_answer_score - best_answer_score))),\n",
        "            \"no_answer_probability\": float(1 / (1 + np.exp(best_answer_score - no_answer_score))),\n",
        "            \"start_logits_max\": float(np.max(start_logits)),\n",
        "            \"end_logits_max\": float(np.max(end_logits)),\n",
        "            \"best_answer_score\": float(best_answer_score),\n",
        "            \"no_answer_score\": float(no_answer_score)\n",
        "        }\n",
        "        detailed_results.append(detailed_result)\n",
        "\n",
        "    # Calculate aggregate metrics\n",
        "    exact_matches = [r[\"exact_match\"] for r in detailed_results]\n",
        "    avg_exact_match = np.mean(exact_matches)\n",
        "    exact_match_count = sum(exact_matches)  # Add this\n",
        "\n",
        "    summary_metrics = {\n",
        "        \"model_name\": model_name,\n",
        "        \"exact_match_ratio\": avg_exact_match,\n",
        "        \"exact_match_count\": exact_match_count,  # Add this\n",
        "        \"total_samples\": len(detailed_results),  # Add this\n",
        "        \"f1_score\": np.mean([r[\"f1_score\"] for r in detailed_results]),\n",
        "        \"avg_confidence\": np.mean([r[\"confidence_score\"] for r in detailed_results]),\n",
        "        \"avg_no_answer_prob\": np.mean([r[\"no_answer_probability\"] for r in detailed_results])\n",
        "    }\n",
        "\n",
        "    return summary_metrics, detailed_results\n",
        "\n",
        "def main():\n",
        "    variants = [\"baseModel\", \"noNorm\", \"AttnOnly\", \"FFNonly\"]\n",
        "    norm_types = [\"LN\", \"RMSN\"]\n",
        "    summary_results = []\n",
        "    all_detailed_results = []\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"squad_v2\", split=\"validation\")\n",
        "    # Limit to first 100 examples\n",
        "    dataset = dataset.select(range(100))\n",
        "\n",
        "    for norm_type in norm_types:\n",
        "        for variant in variants:\n",
        "            model_name = f\"shng2025/GPT-Valkyrie_{norm_type}-124m__{variant}__SQuAD\"\n",
        "            print(f\"\\nEvaluating model {model_name}\")\n",
        "            try:\n",
        "                metrics, detailed_results = evaluate_model(model_name, dataset, max_samples=100)\n",
        "\n",
        "                # Add model info to metrics\n",
        "                metrics['model_name'] = model_name\n",
        "                metrics['norm_type'] = norm_type\n",
        "                metrics['variant'] = variant\n",
        "\n",
        "                summary_results.append(metrics)\n",
        "                all_detailed_results.extend(detailed_results)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating model {model_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Save detailed results\n",
        "    detailed_df = pd.DataFrame(all_detailed_results)\n",
        "    detailed_df.to_csv('squad_evaluation_detailed.csv', index=False)\n",
        "    print(\"\\nDetailed evaluation results saved to squad_evaluation_detailed.csv\")\n",
        "\n",
        "    # Save summary results\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    summary_df.to_csv('squad_evaluation_summary.csv', index=False)\n",
        "    print(\"Summary evaluation results saved to squad_evaluation_summary.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import kruskal\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def perform_kruskal_dunn(df, metric):\n",
        "    \"\"\"Perform Kruskal-Wallis and Dunn's test for a metric.\"\"\"\n",
        "    try:\n",
        "        kruskal_result = kruskal(*[group[metric].values for name, group in df.groupby('model_name')])\n",
        "        if kruskal_result.pvalue < 0.05:\n",
        "            dunn_result = posthoc_dunn(df, val_col=metric, group_col='model_name', p_adjust='bonferroni')\n",
        "            return pd.DataFrame({'statistic': [kruskal_result.statistic], 'p-value': [kruskal_result.pvalue]}), dunn_result\n",
        "        return pd.DataFrame({'statistic': [kruskal_result.statistic], 'p-value': [kruskal_result.pvalue]}), None\n",
        "    except Exception as e:\n",
        "        print(f\"Error performing Kruskal-Wallis and Dunn's test for {metric}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def create_radar_chart(df, metrics, condition_name, output_dir):\n",
        "    \"\"\"Create radar chart with specific formatting.\"\"\"\n",
        "    means = df.groupby('model_name')[metrics].mean()\n",
        "    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
        "    means = pd.concat([means, means.iloc[:, :1]], axis=1)\n",
        "    angles = np.concatenate((angles, [angles[0]]))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10), subplot_kw=dict(projection='polar'))\n",
        "    for model in means.index:\n",
        "        values = means.loc[model].values\n",
        "        ax.plot(angles, values, 'o-', linewidth=2, label=model)\n",
        "        ax.fill(angles, values, alpha=0.25)\n",
        "    ax.set_thetagrids(angles[:-1] * 180/np.pi, metrics)\n",
        "    ax.set_ylim(0, max(means.max().max(), 1.0))\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1.1, 0.5))\n",
        "    plt.title(f\"Model Performance Across Metrics - {condition_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'radar_chart.png'), bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def calculate_anova_summary(df, metrics):\n",
        "    \"\"\"Calculate ANOVA summary for each metric.\"\"\"\n",
        "    anova_results = []\n",
        "\n",
        "    for metric in metrics:\n",
        "        groups = [group[metric].values for name, group in df.groupby('model_name')]\n",
        "        f_value, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "        # Calculate effect size (eta-squared)\n",
        "        eta_squared = f_value / (f_value + df.groupby('model_name').size().iloc[0] - 1)\n",
        "\n",
        "        anova_results.append({\n",
        "            'Metric': metric,\n",
        "            'F_value': f_value,\n",
        "            'p_value': p_value,\n",
        "            'eta_squared': eta_squared\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(anova_results)\n",
        "\n",
        "def create_score_distribution_plots(df, metrics, condition_name, output_dir):\n",
        "    \"\"\"Create detailed score distribution plots.\"\"\"\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(2, 3, i)\n",
        "        for model in df['model_name'].unique():\n",
        "            model_data = df[df['model_name'] == model]\n",
        "            sns.kdeplot(data=model_data, x=metric, label=model)\n",
        "\n",
        "        plt.title(f'Distribution of {metric}')\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Density')\n",
        "        if i == 3:  # Place legend for third plot\n",
        "            plt.legend(title='Model Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        else:\n",
        "            plt.legend([])  # Remove individual legends\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'detailed_score_distributions.png'), bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def create_analysis_for_condition(df, condition_name, output_dir):\n",
        "    \"\"\"Perform statistical analysis for a given condition.\"\"\"\n",
        "    metrics = ['f1_score', 'confidence_score', 'no_answer_probability']\n",
        "\n",
        "    condition_dir = os.path.join(output_dir, condition_name)\n",
        "    os.makedirs(condition_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Basic Descriptive Statistics\n",
        "    desc_stats = df.groupby('model_name')[metrics].agg(['mean', 'std', 'min', 'max', 'count'])\n",
        "    desc_stats.to_csv(os.path.join(condition_dir, 'descriptive_statistics.csv'))\n",
        "\n",
        "    # 2. Effect Size Calculation\n",
        "    effect_sizes = {}\n",
        "    for metric in metrics:\n",
        "        try:\n",
        "            f_value, _ = stats.f_oneway(*[group[metric] for name, group in df.groupby('model_name')])\n",
        "            effect_sizes[metric] = f_value / (f_value + df.groupby('model_name').size().iloc[0] - 1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating effect size for {metric}: {e}\")\n",
        "            effect_sizes[metric] = np.nan\n",
        "\n",
        "    effect_sizes_df = pd.DataFrame.from_dict(effect_sizes, orient='index', columns=['Effect Size'])\n",
        "    effect_sizes_df.to_csv(os.path.join(condition_dir, 'effect_sizes.csv'))\n",
        "\n",
        "    # Add ANOVA summary\n",
        "    anova_summary = calculate_anova_summary(df, metrics)\n",
        "    anova_summary.to_csv(os.path.join(condition_dir, 'anova_summary.csv'), index=False)\n",
        "\n",
        "    # Create detailed score distribution plots\n",
        "    create_score_distribution_plots(df, metrics, condition_name, condition_dir)\n",
        "\n",
        "    score_dist_summary = df[metrics].agg([\n",
        "        'mean', 'std', 'min', 'max',\n",
        "        lambda x: x.quantile(0.25),\n",
        "        lambda x: x.quantile(0.75),\n",
        "        'skew', 'kurt'\n",
        "    ]).round(4)\n",
        "\n",
        "    score_dist_summary.index = ['Mean', 'Std Dev', 'Min', 'Max', 'Q1', 'Q3', 'Skewness', 'Kurtosis']\n",
        "    score_dist_summary.to_csv(os.path.join(condition_dir, 'score_distribution_summary.csv'))\n",
        "\n",
        "    # 3. Post-hoc Tests\n",
        "    for metric in metrics:\n",
        "        tukey = pairwise_tukeyhsd(df[metric], df['model_name'])\n",
        "        pd.DataFrame(data=tukey._results_table.data[1:],\n",
        "                    columns=tukey._results_table.data[0]).to_csv(\n",
        "                    os.path.join(condition_dir, f'tukey_hsd_{metric}.csv'), index=False)\n",
        "\n",
        "    # 4. Correlation Analysis\n",
        "    correlation_matrix = df[metrics].corr()\n",
        "    correlation_matrix.to_csv(os.path.join(condition_dir, 'correlation_matrix.csv'))\n",
        "\n",
        "    # 5. PCA Analysis\n",
        "    scaler = StandardScaler()\n",
        "    pca = PCA()\n",
        "    pca_result = pca.fit_transform(scaler.fit_transform(df[metrics]))\n",
        "    pca_df = pd.DataFrame({\n",
        "        'Principal Component': range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "        'Explained Variance Ratio': pca.explained_variance_ratio_,\n",
        "        'Cumulative Explained Variance Ratio': np.cumsum(pca.explained_variance_ratio_)\n",
        "    })\n",
        "    pca_df.to_csv(os.path.join(condition_dir, 'pca_results.csv'), index=False)\n",
        "\n",
        "    # 6. Kruskal-Wallis and Dunn's Tests\n",
        "    for metric in metrics:\n",
        "        kruskal_result, dunn_result = perform_kruskal_dunn(df, metric)\n",
        "        if kruskal_result is not None:\n",
        "            kruskal_result.to_csv(os.path.join(condition_dir, f'kruskal_{metric}.csv'), index=False)\n",
        "        if dunn_result is not None:\n",
        "            dunn_result.to_csv(os.path.join(condition_dir, f'dunn_test_{metric}.csv'))\n",
        "\n",
        "    # 7. Visualizations\n",
        "\n",
        "    # Correlation Heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title(f'Correlation Heatmap - {condition_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(condition_dir, 'correlation_heatmap.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Violin Plots\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        sns.violinplot(x='model_name', y=metric, data=df)\n",
        "        plt.title(f'{metric} - {condition_name}')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylim(0, 1)  # Set y-axis limits\n",
        "        if i % 3 != 1:\n",
        "            plt.ylabel('')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(condition_dir, 'violin_plots.png'), bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Distribution Plots\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        for model in df['model_name'].unique():\n",
        "            sns.kdeplot(data=df[df['model_name'] == model], x=metric, label=model, clip=(0, 1))  # Add clip parameter\n",
        "        plt.title(f'Distribution of {metric} - {condition_name}')\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Density')\n",
        "        plt.xlim(0, 1)  # Set x-axis limits\n",
        "        if i == 3:  # Place legend outside the plots\n",
        "            plt.legend(title='Model Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        else:\n",
        "            plt.legend([])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(condition_dir, 'score_distributions.png'), bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Box Plots\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        sns.boxplot(x='model_name', y=metric, data=df)\n",
        "        plt.title(metric)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylim(0, 1)  # Set y-axis limits\n",
        "        plt.xlabel('')\n",
        "        if i % 3 != 1:\n",
        "            plt.ylabel('')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(condition_dir, 'boxplots.png'), bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Radar Chart\n",
        "    create_radar_chart(df, metrics, condition_name, condition_dir)\n",
        "\n",
        "    return desc_stats\n",
        "\n",
        "def clean_model_name(name):\n",
        "    \"\"\"Convert full model name to concise format.\"\"\"\n",
        "    # Extract the norm type and variant from the full name\n",
        "    # Example: \"shng2025/GPT-Valkyrie_LN-124m__AttnOnly__SQuAD\" -> \"LN-AttnOnly\"\n",
        "    try:\n",
        "        # Split by underscores and extract relevant parts\n",
        "        parts = name.split('_')\n",
        "        norm_type = parts[1].split('-')[0]  # Get LN or RMSN\n",
        "        variant = parts[3].split('__')[0]    # Get baseModel, noNorm, AttnOnly, or FFNonly\n",
        "        return f\"{norm_type}-{variant}\"\n",
        "    except:\n",
        "        return name  # Return original name if parsing fails\n",
        "\n",
        "# Then modify the main() function:\n",
        "def main():\n",
        "    output_dir = 'qa_analysis_results'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv('squad_evaluation_detailed.csv')\n",
        "\n",
        "    # Clean model names\n",
        "    df['model_name'] = df['model_name'].apply(clean_model_name)\n",
        "    df['model_name'] = df['model_name'].astype('category')\n",
        "\n",
        "    exact_match_df = df[df['exact_match'] == 1]\n",
        "    no_match_df = df[df['exact_match'] == 0]\n",
        "    all_df = df\n",
        "\n",
        "    print(f\"\\nAnalyzing exact matches ({len(exact_match_df)} samples)...\")\n",
        "    exact_stats = create_analysis_for_condition(exact_match_df, 'exact_match', output_dir)\n",
        "\n",
        "    print(f\"\\nAnalyzing non-matches ({len(no_match_df)} samples)...\")\n",
        "    no_match_stats = create_analysis_for_condition(no_match_df, 'no_match', output_dir)\n",
        "\n",
        "    print(f\"\\nAnalyzing all cases ({len(all_df)} samples)...\")\n",
        "    all_stats = create_analysis_for_condition(all_df, 'all_cases', output_dir)\n",
        "\n",
        "    comparison_summary = pd.DataFrame({\n",
        "        'Exact Match Count': len(exact_match_df),\n",
        "        'No Match Count': len(no_match_df),\n",
        "        'Total Count': len(all_df),\n",
        "        'Exact Match Ratio': len(exact_match_df) / len(all_df),\n",
        "        'No Match Ratio': len(no_match_df) / len(all_df)\n",
        "    }, index=['Summary'])\n",
        "\n",
        "    comparison_summary.to_csv(os.path.join(output_dir, 'match_distribution_summary.csv'))\n",
        "\n",
        "    print(f\"\\nAnalysis complete. Results saved in '{output_dir}' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyL2rdAMEONJ",
        "outputId": "ab286d27-4e31-4893-f77a-60e7c2b8b347"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Analyzing exact matches (30 samples)...\n",
            "Error performing Kruskal-Wallis and Dunn's test for f1_score: All numbers are identical in kruskal\n",
            "\n",
            "Analyzing non-matches (770 samples)...\n",
            "\n",
            "Analyzing all cases (800 samples)...\n",
            "\n",
            "Analysis complete. Results saved in 'qa_analysis_results' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vm-nyGHSJTs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}